  %  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================
\documentclass[applsci,article,submit,moreauthors,pdftex]{Definitions/mdpi} 

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2020}
\copyrightyear{2020}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
%\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers 
%\continuouspages{yes}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx,epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption
\usepackage{subfigure}
\usepackage{nth}
%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Sound Source Localization Using Graph Regularized Neural Network}

% Author Orchid ID: enter ID or remove command
%\newcommand{\orcidauthorA}{0000-0000-000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-000-000X} % Add \orcidB{} behind the author's name

% Author homepage: enter homapage URL or remove command
%\newcommand{\homepageauthorA}{https://www.mdpi.com/} % Add \homepageA{} behind the author's name
%\newcommand{\homepageauthorB}{https://www.mdpi.com/} % Add \homepageB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Saulius Sakavičius$^{1,}$*, Andreas Brendel$^{2}$, Artūras Serackis$^{1}$ and Walter Kellermann$^{2}$}

% Authors, for metadata in PDF
\AuthorNames{Saulius Sakavičius, Andreas Brendel, Artūras Serackis and Walter Kellermann}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Vilnius Gediminas Technical University\\
$^{2}$ \quad Friedrich–Alexander University Erlangen–Nürnberg}

% Contact information of the corresponding author
\corres{Correspondence: saulius.sakavicius@vgtu.lt}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{
%	A single paragraph of about 200 words maximum. For research articles, abstracts should give a pertinent overview of the work. We strongly encourage authors to use the following style of structured abstracts, but without headings: (1) Background: Place the question addressed in a broad context and highlight the purpose of the study; (2) Methods: Describe briefly the main methods or treatments applied; (3) Results: Summarize the article's main findings; and (4) Conclusion: Indicate the main conclusions or interpretations. The abstract should be an objective representation of the article, it must not contain results which are not presented and substantiated in the main text and should not exaggerate the main conclusions.
	In this article we present a data-driven approach for a single speech source localization within an acoustic enclosure. 
	Our method consists of high-dimensional acoustic feature extraction, selection based on a fitness criterion, feature manifold learning and low-dimensional embedding, graph dataset construction and an application of a graph-regularized neural network (GRNN) to learn the mapping between the embedded feature coordinates and the Cartesian coordinates of the sound source. Our method relies on the assumption of the feature space spatial smoothness. We present the experimental results of the speech source localization in real acoustic enclosures using two compact circular microphone arrays. We compare the performance of the GRNN for single speech sound source localization to the performance of two baseline algorithms. 
}

% Keywords
\keyword{
%	keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific to the article, yet reasonably common within the subject discipline.)
sound source localization; array signal processing; manifold learning; graph regularization; artificial neural networks
}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{ssdefinitions.tex}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}
%The template details the sections that can be used in a manuscript. Note that the order and names of article sections may differ from the requirements of the journal (e.g., the positioning of the Materials and Methods section). Please check the instructions for authors page of the journal to verify the correct order and names. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX related questions please contact latex@mdpi.com. \cite{stasionis2013new}
%%The order of the section titles is: Introduction, Materials and Methods, Results, Discussion, Conclusions for these journals: aerospace,algorithms,antibodies,antioxidants,atmosphere,axioms,biomedicines,carbon,crystals,designs,diagnostics,environments,fermentation,fluids,forests,fractalfract,informatics,information,inventions,jfmk,jrfm,lubricants,neonatalscreening,neuroglia,particles,pharmaceutics,polymers,processes,technologies,viruses,vision

\section{Introduction}
Sound source localization is an increasingly important component in teleconferencing, autonomous driving, security and human-computer interaction systems \cite{martiSpeakerLocalizationDetection2011,lopatka_detection_2016,valinLocalizationSimultaneousMoving2004}.

\emph{Here could be added a list/few examples of common/typical problems in particular situations/applications, that motivates us to keep investigating this field.}

\emph{Do "camera aiming" is one of the classical tasks that has such name?} Camera aiming can be inaccurate due to the parallax error \emph{citation is needed here} if the camera and the microphone array are not rotating on the same axis. To accurately steer the camera to a certain point in space in such setup, a complete set of coordinates, either Cartesian or polar, of the target point is needed.
Thus, knowing only the direction of arrival (\doa{}) of the sound source would not be sufficient, but at least two \doa{}s are needed to describe the source position on a plane.
There are plenty of source direction of arrival estimation methods utilizing such compact arrays \cite{bruttiComparisonDifferentSound2008,wengThreedimensionalSoundLocalization2001,awad-allaTwostageApproachPassive2020}, but only a few methods are offered for 2D or 3D source localization that can estimate both DoA and the distance or the Cartesian coordinates of the sound source, involving beamforming and particle filtering \cite{valinRobust3DLocalization2006} averaged directivity patterns of blind source separation systems \cite{4959563}.

It might be desired to use compact microphone arrays that have as few microphone elements as possible. By ``compact'' we here imply that the dimensions of the microphone array are much smaller than the dimensions of the source localization space.

%\subsubsection*{2D/3D localization instead of just DoA estimation}
	
%\subsubsection*{Unsupervised and semi-supervised learning methods for SSSL}

The Steered Response Power - Phase Transform (SRP-PHAT) algorithm \cite{dibiaseHighaccuracyLowlatencyTechnique2000} has been shown \cite{silvermanPerformanceRealtimeSourcelocation2005a} to be one of the most robust sound source localization approaches operating in noisy and reverberant environments. However, its practical implementation is usually based on a costly fine grid-search procedure, making the computational cost of the method a real issue \cite{martiSpeakerLocalizationDetection2011}.   \todo{mention SVD-PHAT}

%The performance of SRP-PHAT-based source localization algorithms deteriorate considerable when compact microphone arrays are used \cite{}.
When invoking sound source localization algorithms, usually simplifying free-field and far-field assumptions are made which aren't always accurate, especially inside acoustic enclosures, in which the diffuse field is observed due to acoustic reflections. 
For a particular acoustic enclosure, either of the assumptions might be too coarse and would not reflect the actual situation, which might be a particular combination of either of the acoustic models.
This calls for a data-driven approach which can learn better models.
Learning-based sound source localization methods might be further advantageous in such circumstances. 

There are several learning-based source localization approaches, based on either semi-supervised \cite{laufer-goldshteinSemiSupervisedSoundSource2016} or supervised \cite{heDeepNeuralNetworks2018b,heAdaptationMultipleSound2019,adavanneDirectionArrivalEstimation2017} learning paradigms. In both of these approaches to work, a set of acoustic features from known sound source positions (the labeled dataset) is needed. \todo{cite A. Brendel and colleagues}

Labeled feature acquisition is very costly.
On the other side, it is relatively easy to obtain a large dataset of unlabeled audio features. 
%One might wish to employ an unsupervised or semi-supervised learning strategies. 
Considering our setting, it is relatively easy to collect a large amount of acoustic features without labels, and it is very tedious to provide labels (in our case -- the coordinates of the sound source) for such data. Such a learning-based strategy is promising as users spend most of their time in the same environments (living room, office, etc.), where such training data can be obtained and used.

%\subsection*{Motivation}

In this article we present a method to localize a sound source in two dimensions using a two compact microphone arrays. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
We assume a set of $ M $ sensor nodes, each equipped with two $ N_m $ microphones arranged in a circular array.
The microphone signal $ x_{i,m}(t) $ at microphone $ m $ is modeled as the superposition of the anechoic signal $ s_{i,m}(t) $ and an additional reverberatio/noise signal $ n_{i,m}(t) $
\begin{equation}
    x_{i,m}(t) = s_{i,m}(t) + n_{i,m}(t), 
\end{equation}

\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}

In this section, we provide a theoretical background for our investigation.

\ann{}-based sound source localization can be performed using various acoustic features that depend on the position of the sound source, either relative to the microphone array(s) or the acoustic enclosure.

It can be assumed, that acoustic features are spatially smooth, that is, features obtained at spatially close source positions are also close in feature space. The relative distance or affinity of the features can be simply estimated by calculating the Euclidean distance between feature vectors. 

It can be assumed, that high-dimensional acoustic features lie on a low-dimensional manifold, embedded in a high-dimensional feature space. 
Since the acoustic features are only dependent on the coordinates of the sound source, it is expected that the manifold would represent the spatial relations between the nearby acoustic features. 
We consider that the affinity matrix of the low-dimensional embeddings of the acoustic manifold represent the \emph{graph} of the acoustic features.

While the obtained embeddings of the acoustic manifold might represent the relative spatial relations between the acoustic features, it is not tied to physical properties and it also might be very non-linear. The translation of embedded space to physical space must be done in a separate step using a nonlinear regression method. 

When \ann{}s are utilized to obtain the sound source position via regression using acoustic features, it might be expected that the predictions of an \ann{} would also exhibit spatial smoothness. A feature manifold could be used during the training of the \ann{} to ensure that the \ann{} learns the relation between the acoustic features and the source positions while also retaining the spatial smoothness.
This spatial smoothness as well as the awareness of the relative spatial positions of the acoustic features is especially important when a semi-supervised learning strategy is involved. Using a training dataset that contains both labeled and unlabeled samples, knowledge of the relative distances between unlabeled and labeled samples might help to train the regressor to predict source locations for the unlabeled samples based on their manifold distance to the labeled features.


Our method is comprised of two stages:
\begin{enumerate}
	\item The low-dimensional embedding of an acoustic feature manifold is obtained from a combined dataset of labeled and unlabeled samples. This manifold represents relative distances between acoustic features in a low-dimensional embedded space.
	\item An \ann{} is trained on the combined dataset, using a loss function that consists of a supervised loss (calculated only for labeled samples) and a graph loss (calculated for all samples, considering $ k $ nearest neighbors). The supervised loss ensures that the regressor learns accurate relations between the acoustic features and the source positions. The graph loss ensures that the source position predictions remain spatially smooth.
\end{enumerate}

Considering that the combined dataset consists of a relatively low number of labeled samples and a vast amount of unlabeled samples, the supervised loss acts as a mean to ``straighten'' the manifold while the graph loss is used to infer the labels for the unlabeled samples based on their distance in feature (or embedded) space to the labeled samples.


\subsection{Acoustic Features}
We have considered several types of acoustic features, that are discussed further.

\subsubsection{Time Difference of Arrival}
Time Difference of Arrival (TDoA) is a trivial acoustic feature, that can be estimated using GCC-PHAT. Knowing the TDoA for several non-colinear (or non-parallel?) microphone pairs, it is possible to estimate the position of the sound source using triangulation (trilateration).

While this would be a simple and straightforward method, the accurate TDoA estimation becomes very tricky in reverberant or noisy environments. Moreover, the TDoA contains only very little information about the distance between the sound source and the microphone pair (just one value per pair). For a microphone array with 4 elements, that's only 6 values. TDoA does not explicitly contain any information about the structure of reflections withing the enclosure, nor the geometry or acoustic properties of the enclosure; it only depends on the relative source position with respect to the microphone array(s).

\subsubsection{Room Impulse Response and Room Transfer Function}
It is assumed that high-dimensional acoustic features, such as room impulse response (RIR) or room transfer function (RTF) contain a unique fingerprint of sound source and microphone positions within an enclosure. This is because the structure of room reflections is unique for every source position and every microphone position (theoretically, there might be some cases when same RIR is obtained for more than one combination of microphone and sound source positions, but this is probably possible in ideal room, which exhibit point symmetry around the center of the room; in real rooms this is impossible; also the microphones must be also placed symmetrically in the enclosure for this effect to occur).

While the RIRs and RTFs contain enough information to uniquely determine the position of the source within an enclosure, in practice it is impossible to obtain RIR without knowing the positions of the sound source and the microphone within the room beforehand. RTFs, on the other hand, is a viable option.

\subsubsection{Steered Response Power}
Steered Response Power (SRP) and SRP with Phase Transform (\srpphat{}) vectors can be considered the middle ground between the trivial acoustic features like TDoA and ideal features, like RIR or RTF. 
\srpphat{} features are obtainable in real world, are relatively high-dimensional and contain information about sound reflections within the room.

\subsubsection{Properties of acoustic features}
The most important property of all acoustic features in this investigation is the spatial smoothness of feature space. In other words, acoustic features are similar to each other for sound source positions that are close together.

In our investigation, we use the \srpphat{} spatial spectra as acoustic features \cite{dibiaseHighaccuracyLowlatencyTechnique2000,scheiblerPyroomacousticsPythonPackage2018a}. 

\subsection{Acoustic feature acquisition}
Acoustic features were obtained within an acoustic enclosure using a single sound source, $ z $ coordinate was fixed at height \hsrc{}. \Narr{} circular microphone arrays were used for acoustic signal acquisition, each with \Nmic{} microphone elements and radius \rarr{}.
Planes of the microphone arrays were parallel to the ground.
Both arrays were held at a fixed height \harr{}.
Signals of the microphones are recorded at a fixed sampling frequency \fs{} and a fixed resolution \resolution{}.

\subsubsection{Unlabeled dataset}
The unlabeled dataset may be obtained from an array audio recording where the sound source is slowly moving inside the acoustic enclosure. The maximal speed of the sound source movement $ \speed_{\src\max} $ should be lower than the maximum expected localization error distance $ \error_{\max} $ per frame duration $ \framedur $:
\begin{equation}\label{eq:unlabeled_src_max_speed}
\speed_{\src\max}  = \frac{\error_{\max}}{\framedur}
\end{equation} 
\subsubsection{Labeled dataset}
The labeled dataset may be obtained from an array audio recordings where the sound source is stationed at a known position \sposlabel{}, described by coordinates $ (x,y,z) $ in Cartesian coordinate system within the acoustic enclosure and is producing signal (speech or noise) for a period of \labeledsourceduration{} seconds. A collection of $ \sposidx \in \numel_{\spos} $ recordings at fixed source positions may be obtained.

\subsection{Audio signal framing}
Audio signals obtained from the microphone arrays are split into frames of duration \framedur{} seconds to obtain \Nframes{} frames. 

\subsection{\srpphat{} feature acquisition}
For each audio frame $ \frameidx{} \in \Nframes{} $ and for each microphone array $ \arrayidx{} \in \Narr $, a set of time-frequency representations of the microphone signals is calculated with \NFFT{} FFT points, without frame overlap and no windowing function. 

A \srpphat{} spatial spectrum \SRPspectrum{} is obtained for each frame and for each array. 
\SRPspectrum{} is a vector with $ \numel_{\SRP} $ elements, representing the WHAT\todo{??????} at a particular \doa{} and covering an azimuth angle $ \azimuth_{\arr} \in [\ang{0};\ang{360}] $.
\srpphat{} spectra of all arrays are then concatenated per frame to obtain the acoustic feature \SRPfeature{} of $ \numel_{\arr} \cdot \numel_{\SRP} $ elements.

If the audio recording has an associated location label (known coordinates), a frame is assigned the position label \sposlabel{}.

\subsection{Acoustic features selection (thresholding)}

It is considered that the sound source might not be active a all times, and that the signal is non-stationary (in case of speech signal, it might be considered quasi-stationary for frames that contain only one phoneme or a part of a phoneme\todo{cite}). Thus, in case of an audio frame where the source is not active, the \doa{} of a sound source can not be determined, and the acoustic feature is considered to contain only noise. Such frames are to be discarded. For the selection of the audio frames in which the acoustic feature is usable, a thresholding algorithm was used. 
A metric 
$ \thrmetric{}_{\arrayidx,\frameidx} = f(\SRPspectrum) $ 
is calculated for and compared to the threshold level \thrlevel{} which is the scaled mean \todo{scaled mean? is this a good term?} of the metric of all obtained frames 
$ \thrlevel = \coefficient_\thrmetric \frac{1}{\Nframes}\sum_{\frameidx\in\Nframes} \thrmetric{}_{\frameidx}  $, 
where $ \coefficient_\thrmetric $ is the scaling coefficient used to control the threshold value.
Metric $ \thrmetric{}_{\arrayidx,\frameidx} $ is calculated per array to address a fact that the arrays might be not identical in terms of audio signal gain, the signal-to-noise ratio and frequency response.
The metrics used to evaluate the fitness of the acoustic feature of the particular audio frame are: 
\begin{enumerate}
	\item Root-mean-square \todo{maybe that's energy of a frame?} value of the \srpphat{} spectrum, $ \thrmetric^{\rms,}_{\arrayidx,\frameidx}(\SRPspectrum) = \sqrt{ \mean{\SRP^2} } $. 
	\item Crest factor of the \srpphat{} spectrum, $ \thrmetric^{\cf}_{\arrayidx,\frameidx}(\SRPspectrum) = \frac{\abs{\max(\SRPspectrum)}}{\thrmetric^{\rms,}_{\arrayidx,\frameidx}(\SRPspectrum)} $.
\end{enumerate}

After determining $ \thrmetric{}_{\arrayidx,\frameidx} $ of the \SRPspectrum{} per array per frame, feature vectors \SRPfeature{} are selected of those frames $ j $ for which $ \thrmetric{}_{\arrayidx,\frameidx} > \thrlevel $ for all microphone arrays $ i \in \numel_{\arr} $.


\subsubsection{Training/testing dataset split}
The labeled dataset is split into training and testing subsets by randomly selecting samples from  $ \numel_{\test} $ source positions for training and the rest of the source positions $ \numel_{\train} = \numel_\spos - \numel_\test $ for testing from the entire set of labeled source positions.
Following operations are performed separately for training ant testing labeled datasets.

\subsection{Acoustic manifold embedding learning}

Manifold embedding can be learned using a Nonlinear Dimensionality Reduction (NLDR) algorithm, such as isometric mapping (\isomap{}), t-distributed stochastic neighbor embedding (t-SNE) or localli linear embedding (LLE), among others. 
We have employed \isomap{} NLRD algorithm to obtain the high-dimensional feature embeddings in low-dimensional space, that is, learn the acoustic feature manifold.

\subsubsection{\isomap{} embedding}
One of the earliest approaches to manifold learning is the \isomap{} algorithm. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel Principal Component Analysis (PCA). Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points~\cite{pedregosaScikitlearnMachineLearning2011}.

\srpphat{} features from both labeled and unlabeled training datasets are embedded into \Demb-dimensional embedded space using \isomap{}, with \kemb{} nearest neighbors considered. 
For each \SRPfeature{} feature, an embedding $ \ISOembedding{} = [z_{d_1}, z_{d_2}, \dots, z_{d_{\Demb}}] $. This way, a low-dimensional representations of the high-dimensional acoustic features is obtained. Moreover, the learned manifold corresponds to the spatial structure of the acoustic feature space. Thus, the relative distances in the embedded space of unlabeled features to labeled features is known. 

%\mpar{Is it really the spatial structure of the high-dimensional space?} (see \figurename{}

\subsection{Graph dataset} \label{subsec:graphdataset}

\subsubsection{Dataset preprocessing}
The combined dataset for training the neural network is comprised of two datasets: $ \numel_{\unlabeled} $ acoustic feature samples without source position labels (the unlabeled dataset) and $ \numel_{\labeled} $ acoustic feature samples with source position labels (the labeled training dataset). Each sample feature $ \SRPfeature^{\unlabeled,\labeled} $ in the combined dataset also has a corresponding \isomap{} embedding $ \ISOembedding^{\unlabeled,\labeled} $.
In order to train the \grnn{} with graph regularization, the dataset must be preprocessed: for each sample, regardless of whether it is a labeled or an unlabeled sample, alongside the main feature, neighbor features $ \SRPfeature^{\nbr} $ and their weights $ \nbrweigth^{\nbr} $ where $ \nbr \in \nbrhood $ must be introduced. \nbrhood{} denotes the neighborhood of the sample feature in the embedded space. This is done by first determining the \gnbrs{} nearest neighbors of a particular sample in the embedded feature space and then appending those features as well as their weight coefficients to the training sample.

\subsubsection{Affinity matrix calculation}
In the embedded space, Euclidean distances are calculated between every feature. The distances between each data sample constitute the distance matrix \distancematrix, which is in turn used to calculate the affinity matrix.
Affinity matrix \affinitymatrix{} is calculated by subtracting \distancematrix{} from the identity matrix: $ \affinitymatrix = \mathbf{1} - \distancematrix $. The distance matrix contains the Euclidean distances between each sample in the low dimensional embedded space:

\begin{align}
	\mathbf{D} &= (d_{ij});\\
	d_{ij} &= \lVert \mathbf{p}_{i}-\mathbf{p}_{j} \rVert _{2}^{2}
\end{align}
here $ \mathbf{p}_{i} = (\alpha_i, \beta_i) $ is the point coordinate vector in the embedded space (in case of $ N_{\mathrm{ISO}} = 2 $), $ \alpha $ and $ \beta $ are the Cartesian coordinates in the embedded space. \todo{change here to \Demb{} -dimensional represetation, not constant 2}

Neighbor weights are inversely proportional to the Euclidean distances between the main feature and the neighbor features in the low-dimensional embedded space.

\subsubsection{Neighbor samples and neighbor sample weights}
For the training of the GRNN, each training sample must contain the main \srpphat{} feature and \gnbrs{} neighbor \srpphat{} features (used for calculating the graph loss). Additionally, each neighbor feature is associated with its weight \nbrweigth{}, which is the corresponding element in the affinity matrix.
To obtain the \gnbrs{} neighbors of each sample, each row of the affinity matrix is thresholded so that only the \gnbrs{} highest-valued elements remain their value, while other row elements are set to zero.
The dataset is then expanded so that each sample now has associated neighbor \srpphat{} features (indices of which are the non-zero elements in the rows of the affinity matrix). 


\subsubsection{Labeled/unlabeled sample marking}
For the training dataset, a flag $ m $ denoting wether the sample is labeled or unlabeled is introduced. This flag holds value of either ``True'' of ``False'' (1 or 0). Content of this field is interpreted by the GRNN during the calculation of the loss function. Effectively, the supervised loss component is multiplied by the flag. In case of an unlabeled sample, the supervised loss is ignored, and only the graph loss is considered. In real-world scenario, GRNN expects all fields, including the target feature (the label, the coordinates of the source) to be passed during training. In case of the unlabeled sample (whether during the training phase or during the prediction phase), the supervised loss is not calculated, the label is ignored, and thus it can be set to random values or to zero. 

\subsubsection{Labeled samples repetition}
We wish to train the GRNN using as few as possible labeled samples. It was found that the network is trained more effectively when the labeled samples are introduced more times (more often) than the unlabeled samples. It might be called ``dataset balancing'' \cite{}. 
Labeled samples (those with $ m =  1 $) are repeated $ N_R $ times ($ N_R \in \{1,\dots,199\}$) and appended to the training data subset.

\subsection{Graph-Regularized Neural Network}

In our proposition, a neural network that is trained considering not only the labeled samples, but also neighboring labeled and unlabeled samples.

\subsubsection{Neural network}

Any neural network can be converted to graph-regularized neural network (\grnn) by introducing additional inputs for neighboring features as well as modifying the loss function to accommodate the graph loss.

A general architecture (one of possibilities) of a \grnn model is provided in \figurename{} \ref{fig:networktraining}. In this figure, dotted lines encompasses the input vectors. Dahsed lines inside the GRNN block denote prediction (a forward pass). The loss function is given by $ L = m(\hat{y}_0-y) + \sum_{i \in k_g}a_i(\hat{y}_0 - \hat{y}_1) $. The loss function is discussed further in more detail.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{img/network_training}
	\caption{General architecture of a graph regularized neural network (considering 4 neighbor features). $ x_0 $ is the main input feature, $ x_{1..4} $ are neighbor input features, $ a_{1..4} $ are corresponding neighbor input feature weights, $ y_0 $ is the target feature, $ m $ is the labeled/unlabeled flag, $ \hat{y}_0 $ is the label prediction for main input feature, $ \hat{y}_{1..4} $ are the label predictions for the neighbor input features}
	\label{fig:networktraining}
\end{figure}


\subsubsection{Architecture}
Apart from the introduction of additional inputs (neighbor features, weights and flags), the actual neural network is just a multilayer perceptron. During prediction phase, only the main input contributes to the prediction.

In this experiment, a several multilayer perceptron architectures were used. The summary of the architectures are presented in \tablename \ref{table:nnarch}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
	\caption{Table of neural network architectures used in for the experimentation\label{table:nnarch}}
	\centering
	\small\setlength{\tabcolsep}{2pt}
	\begin{tabular}{cc|cc|cc|cc|cc|cc|cc}
		\toprule
		&   & \multicolumn{12}{c}{Architecture}                                                                                                                                                       \\ \cmidrule{3-14} 
		&   & \multicolumn{2}{c|}{1} & \multicolumn{2}{c|}{2} & \multicolumn{2}{c|}{3} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{5} & \multicolumn{2}{c}{6}                                      \\ \midrule
		Layer                   &   & Activation    & Size   & Activation    & Size   & Activation    & Size   & Activation    & Size   & Activation    & Size   & Activation                                           & Size \\ \midrule
		input                   &   & linear        & 720    & linear        & 720    & linear        & 720    & linear        & 720    & linear        & 720    & linear                                               & 720  \\ \midrule
		\multirow{5}{*}{hidden} & 1 & linear        & 14     & linear        & 4      & linear        & 10     & linear        & 10     & linear        & 10     & \begin{tabular}[c]{@{}c@{}}Leaky\\ ReLU\end{tabular} & 10   \\
		& 2 & sigmoid       & 2      & sigmoid       & 32     & relu          & 31     & ReLU          & 15     & ReLU          & 15     & \begin{tabular}[c]{@{}c@{}}Leaky\\ ReLU\end{tabular} & 15   \\
		& 3 & tanh          & 24     & tanh          & 23     &               &        & ReLU          & 15     & ReLU          & 15     & \begin{tabular}[c]{@{}c@{}}Leaky\\ ReLU\end{tabular} & 15   \\
		& 4 & sigmoid       & 33     & sigmoid       & 54     &               &        &               &        & ReLU          & 15     & \begin{tabular}[c]{@{}c@{}}Leaky\\ ReLU\end{tabular} & 15   \\
		& 5 & linear        & 50     & linear        & 37     &               &        &               &        & ReLU          & 15     & \begin{tabular}[c]{@{}c@{}}Leaky\\ ReLU\end{tabular} & 15   \\ \midrule
		output                  &   &      linear         & 2      &       linear        & 2      &       linear        & 2      &       linear        & 2      &          linear     & 2      &             linear             & 2    \\ \bottomrule
	\end{tabular}
\end{table}
%It contained these layers:
%\begin{enumerate}
%	\item A 1080-dimensional input layer (to accept a concatenated \srpphat{} feature using $ N_M = 3 $ microphone arrays, each covering \ang{360} azimuth with \ang{1} resolution).
%	\item A fully-connected layer with 10 units and linear activations.
%	\item A fully-connected layer with 31 units and ReLU activations.
%	\item A 3-dimensional output layer with linear activations.
%\end{enumerate}

This architecture was the found during previously performed hyperparameter optimization.

%Network architectures that were  used for the experimentation are presented in \tablename{} \ref{table:archs}. The first three architectures were determined during a network architecture optimization process. The last two are the variations of the 3rd architecture.
%
%\begin{table}\footnotesize\setlength{\tabcolsep}{0.3em}
%	\caption{Network achitectures used for the experimentation}\label{table:archs}
%	\begin{tabular}{P{1.4cm}|P{0.6cm}|P{1.4cm}|P{0.6cm}|P{1.4cm}|P{0.6cm}|P{1.4cm}|P{0.6cm}|P{1.4cm}|P{0.6cm}|P{1.4cm}}
%		\toprule
%       & \multicolumn{2}{c}{1st layer} & \multicolumn{2}{c}{2nd layer} & \multicolumn{2}{c}{3rd layer} & \multicolumn{2}{c}{4th layer} & \multicolumn{2}{c}{5th layer} \\
%		\cmidrule(rl){2-3}\cmidrule(rl){4-5}\cmidrule(rl){6-7}\cmidrule(rl){8-9}\cmidrule(rl){10-11}
%		Arch. & N. of units & Activation      & N. of units & Activation      & N. of units & Activation      & N. of units & Activation      & N. of units & Activation      \\ \midrule
%		1                                                                                                    & 14          & linear          & 2           & sigmoid         & 24          & tanh            & 33          & sigmoid         & 50          & linear          \\ \midrule
%		2                                                                                                    & 4           & linear          & 32          & sigmoid         & 23          & tanh            & 54          & sigmoid         & 37          & linear          \\ \midrule
%		3                                                                                                    & 10          & linear          & 31          & ReLU            &             &                 &             &                 &             &                 \\ \midrule
%		4                                                                                                    & 10          & linear          & 15          & ReLU            & 15          & ReLU            &             &                 &             &                 \\ \midrule
%		5                                                                                                    & 10          & linear          & 15          & ReLU            & 15          & ReLU            & 15          & ReLU            & 15          & ReLU            \\ \bottomrule
%	\end{tabular}
%\end{table} 

\subsubsection{Loss function}

Nearby source positions produce similar acoustic features. Therefore, the predicted source positions for the nearby acoustic features should also be similar
If they are similar, the graph loss is small. If they are not similar, we need to penalize the predictor with a large graph loss

The loss function used for the GRNN training is comprised of two parts: the supervised loss (the difference between the ground truth label and the predicted label) and the graph loss (the difference bewteen the main input feature label prediction and the weighted sum of neighbor input features label predictions). It can be expressed as 
\begin{equation}\label{eq:grnn_loss_function}
	L = \mu m \sum_{i \in N_{b}} (\hat{y}_i - y_i)^2  + (1-\mu m) \sum_{i \in N_{b}} \sum_{j \in k_g} a_{ij} (\hat{y}_i-\hat{y}_j)^2
\end{equation}
here $ N_b $ -- number of samples in one training batch, $ k_g $ -- size of the neighborhood, $ a_{ij} $ is the neighbor weight, equal to the corresponding element in the affinity matrix.


\subsection{Experimental setup}
We have evaluated the performance of our proposed method using a \realworld{} microphone array audio dataset with speech signal as the sound source, which was recorded in a particular acoustic enclosure.

\subsubsection{Enclosure}
All audio data used for the experimentation was collected in an irregular shaped room with the side dimensions of \SI{4.902}{\m} in $ x $ axis, \SI{5.361}{\m} in $ y $ axis and \SI{3.75}{\m} in $ z $ axis. The geometry of the enclosure is presented in \figurename{} \ref{fig:room_src_pos}. 

\begin{figure}[h!]
	\centering
	\subfigure[\nth{1} dataset]{\includegraphics[scale=0.45]{img/room_src_pos.pdf}}
	\subfigure[\nth{2} dataset]{\includegraphics[scale=0.45]{img/room_src_pos_2nd_dataset.pdf}}
	\caption{The geometry of the acoustic enclosure used for \realworld{} dataset collection and labeled sound source positions (\nth{1} and \nth{2} datasets); height of the enclosure was \SI{3.75}{\m}}\label{fig:room_src_pos}
\end{figure}

\subsubsection{Sound source}
\paragraph{Source signal}
The signal of the sound source was a \SI{1}{\minute} excerpt from the AMI Corpus \cite{carlettaAMIMeetingCorpus2006}, the mix of close-talking microphone signals, containing male and female speech samples.

Sound source signal was reproduced using a compact battery powered loudspeaker that was mounted on an adjustable height stand.

Two sets of labeled and unlabeled array audio recordings were recorded. \footnote{The sets were recorded on different days and a slight shift in microphone array positions might have had occurred. The first set have more labeled recordings (34), but the unlabeled recording is only around 8.5 minutes long, while the second recording has less labeled recordings (23), and source positions does not cover the entire area of the enclosure, but the unlabeled recording is around 40 minutes long.}

\paragraph{Source positions}

The first set of 34 array audio recordings was obtained with the sound source held stationary for \SI{1}{\minute} in one of 34 positions within the enclosure. 
The second set of 23 array audio recordings was obtained in the same fashion as the first set.

The coordinates of the sound source were measured using a handheld laser distance measurement tool with accuracy of \SI{1}{\mm}. The locations of the known source positions are presented in \figurename{} \ref{fig:room_src_pos} as colored circles. The values of the red, green and blue components of each circle color are proportional to the $ x_{\src} $, $ y_{\src} $ and $ z_{\src} $ coordinates of the sound source position.
The vertical coordinate, $ z_{\src} = \SI{1.9}{\m} $, was constant for all source positions.

\subsubsection{Unlabeled audio dataset}
The unlabeled audio dataset was collected using the same microphone array setup and the same audio source and signal. The loudspeaker was moved manually at a reasonably constant speed of approximately \SI{0.1}{\m\per\s}, scanning the entire floor area of the enclosure \todo[inline]{by moving along $ y $ axis and then along $ y $ axis --how to describe this movement?}. The vertical coordinate of the sound source, $ z = \SI{1.9}{\m} $, was held constant during the entire collection of the unlabeled dataset. 
The total duration of the recording in the first dataset was \SI{511}{\s} \todo{SMF3; SMF4 was 40 minutes long; unused}. In the second dataset, the duration of the unlabeled audio recording was \SI{2420}{\second}.

\subsubsection{Microphone arrays}
In our experimentation we have used $ \numel_{\arr} = 2 $ radial microphone arrays with radius $ \radius_{\arr} = \SI{0.045}{\m} $, each consisting of $ \numel_{\mic} = 4 $ microphone elements spaced at equal angles $ \phi_{\mic} = \ang{90} $. The centers of the microphone arrays were placed at coordinates $ \mathbf{M}_{C,1}^{(x,y,z)} = [2.913, 3.699, 1.313]~\si{\m} $\todo{kaip tinkamai uzrasyti koordinaciu rinkini?} and $ \mathbf{M}_{C,2}^{(x,y,z)} = [2.960, 2.512, 1.309]~\si{\m} $.
%    mac = np.array([[2.913, 3.699, 1.313],
%[2.960, 2.512, 1.309]])

First element of each array was oriented towards the positive $ x $ axis with respect to the microphone array center (the rotation of the elements of the microphone array relative to the $ x $ axis was \ang{0}). 

\subsubsection{Acoustic feature acquisition}

Audio signals obtained from the microphone arrays were split into frames with the duration of \SI{0.05}{\s}. This frame duration was chosen so that every audio frame would contain only one speech phoneme.

For each audio frame and for each array, a \srpphat{} spatial spectrum with 360 elements, covering \doa{} of \ang{360} (1 degree resolution) was calculated using \pra{} \python{} implementation \cite{scheiblerPyroomacousticsPythonPackage2018a} of a method presented in \cite{dibiaseHighaccuracyLowlatencyTechnique2000}. During \srpphat{} spectra calculation, $ \NFFT{} = 512 $ FFT points were used, with \SI{50}{\percent} overlap, for the STFT snapshot calculation. For each frame, \srpphat{} spectra were concatenated to produce a single 720-dimensional acoustic feature.

\subsubsection{Acoustic feature selection}
Acoustic features for further processing were selected based on the \rms{} or \cf{} metrics of the feature vector, thresholded by the scaled mean of the corresponding metrics of the entire dataset.
Thresholding scaling coefficient \thrmeanlvl{} was selected from the range $ \thrmeanlvl \in [1.0, 1.1, \dots, 1.5] $ for both \rms{} and \cf{} based methods.

\subsubsection{Acoustic feature manifold learning}
Acoustic feature manifold embeddings were found using \isomap{} NLDR algorithm, implemented in \cite{pedregosaScikitlearnMachineLearning2011}.
We have chosen to embed the manifold into 2-dimensional embedded space, since in our experimentation, the only the $ x $ and $ y $ coordinate of the sound source was changing, thus, the acoustic feature are expected to rely only on two variables.
The number of nearest neighbors considered for each sample was selected in the range $ \nneighbors = 2^n; n \in [0,1,\dots,6] $.
The same settings were used for both unlabeled and labeled audio dataset. The acoustic manifold was first learned on unlabeled dataset and then the labeled dataset was transformed into low-dimensional embedded space using the already learned manifold.\todo{patikrinti, ar cia logiskai parastya}

%\begin{figure}
%	\centering
%	\includegraphics[width=0.5\linewidth]{example-image-a}
%	\caption{\isomap{} embeddings of acoustic features for unlabeled audio signal, obtained using $ \framedur = \SI{0.05}{\s} $}
%	\label{fig:example-image-a}
%\end{figure}

\subsubsection{Dataset construction}
The training and testing datasets were constructed as described in Section \ref{subsec:graphdataset}. Number of nearest graph neighbors considered for each sample \gnbrs{} was selected from a set $ \gnbrs{} \in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20] $. 

%\subsubsection{\grnn{} architecture}


\subsubsection{\grnn{} training}
We have trained the \grnn{} for $ \numel_{\epochs} = 15 $ epochs with each set of parameters. The sample batch size was $ \numel_{\batch} = 2^n; n \in [6,7,\dots,13] $. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%\todo[inline]{ar dėti iliustracijas, kai klaidos priklausomybe nuo parametro skaičiuojama iš hyperparameter optimization rezultatų, kai keičiami visi parametrai, ar tik iš eksperimentų rinkinių, kai keičiamas vienas parametras, o kiti laikomi fiksuoti?}
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%\begin{quote}
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%\end{quote}

We have investigated the influence of each of the parameters on the sound source localization error.

\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_thr_mean_lvl}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_thr_mean_lvl}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_r_ratio_thr_mean_lvl}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the 
		acoustic feature thresholding level; 
		linear regression model showed in solid line}
	\label{fig:rmsethrmeanlvl}
\end{figure}


\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_n_neighbors}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_n_neighbors}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_r_ratio_n_neighbors}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the number of nearest neighbors considered for \isomap{} embedding; linear regression model showed in solid line}
	\label{fig:rmsenneighbors}
\end{figure}

\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_gnbrs}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_gnbrs}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_r_ratio_gnbrs}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the number of nearest graph neighbors considered during the training dataset construction; $ 2^{\mathrm{nd}} $ order regression model showed in solid line}
	\label{fig:rmsengnbrs}
\end{figure}

\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_k_rep}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_k_rep}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_r_ratio_k_rep}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the labeled samples repetition rate during \grnn{} training; linear regression model showed in solid line}
	\label{fig:rmsekrep}
\end{figure}

\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_mu}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_mu}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_mu}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the ratio between the supervised and unsupervised loses considered during the \grnn{} training; linear regression model showed in solid line}
	\label{fig:rmsemu}
\end{figure}

\begin{figure}[h!]
	\centering
	\subfigure[position error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_spos_batch_size}}
	\subfigure[angular error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_batch_size}}
	\subfigure[radial error]{\includegraphics[width=0.32\linewidth]{img/results/RMSE_err_polar_batch_size}}
	\caption{Dependency between the source position prediction, source \doa{} estimation  and radial estimation RMSE and the \grnn{} training sample batch size; linear regression model showed in solid line}
	\label{fig:rmsebatchsize}
\end{figure}



The results of sound source localization using the geometric source localization approach are presented in \figurename \ref{fig:trirealspeechsrpdoathrrms1p02framedur0p05} and \figurename \ref{fig:trirealspeechsrpdoathrrms1p02framedur0p05err}.

\begin{figure}[h!]
	\centering
	\subfigure[lines represent the \doa{} radii of each array]{
		\includegraphics[width=0.32\linewidth,clip,trim=3cm 0 3cm 0]{img/from_diser/tri_real_speech_srp_doa_thr_rms_1p02_framedur_0p05}
		\label{fig:trirealspeechsrpdoathrrms1p02framedur0p05}	
	}
	\subfigure[lines represent localization error]{
		\includegraphics[width=0.32\linewidth,clip,trim=3cm 0 3cm 0]{img/from_diser/tri_real_speech_srp_doa_thr_rms_1p02_framedur_0p05_err}
		\label{fig:trirealspeechsrpdoathrrms1p02framedur0p05err}	
	}	
	\caption{Results of real-world speech source localization using geometric localization algorithm}
\end{figure}





Using such methods, the positions of the sound source were estimated using real-world speech audio data. The results of the localization are presented in \figurename{} \ref{fig:imprediction}.


\begin{figure}[h!]
	\centering
	\subfigure[source positions is the argument of the intensity map]{\includegraphics[width=0.32\linewidth,clip,trim=3cm 0 3cm 0]{img/from_diser/im_prediction}}
	\subfigure[source positions is the location of the most prominent local peak of the intensity map]{\includegraphics[width=0.32\linewidth,clip,trim=3cm 0 3cm 0]{img/from_diser/im_prediction_peak_local_max}}
	\caption{Predicted source positions using the intensity map approach}
	\label{fig:imprediction}
\end{figure}





After assembling the training dataset with 2 nearest neighbors, and training the neural network for 50 epochs, the predictions of the source position were more accurate than using the baseline methods.



\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{img/from_diser/real_speech_isomap_framedur_0p05}
	\caption{2-dimensional \isomap{} embeddings of the \srpphat{} features of the real-world unlabeled speech signal; point hue represents its}
	\label{fig:realspeechisomapframedur0p05}
\end{figure}

The summary of source location prediction errors for different localization methods are presented in \tablename{} \ref{table:methodsprederr}.
The results of the source location prediction using a GRNN are presented in \figurename{} \ref{fig:grnnpredictionsrealspeecharch350epochsframedur0p05}.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.32\linewidth,clip,trim=3cm 0 3cm 0]{img/from_diser/grnn_predictions_real_speech_arch3_50epochs_framedur_0p05}
	\caption{Prediction of sound source position using a GRNN trained for 50 epochs.}
	\label{fig:grnnpredictionsrealspeecharch350epochsframedur0p05}
\end{figure}

%\begin{table}
%	\centering
%	\caption{Summary of source location prediction errors for different localization methods}\label{table:methodsprederr}
%	\begin{tabular}{lrrr}
%		\toprule
%		Method &       MSE &       MAE &      RMSE \\\toprule
%		Geometric &  8.374483 &  1.925387 &  2.893870 \\\midrule
%		Intensity Map, argmax &  2.501103 &  1.263976 &  1.581488 \\\midrule
%%		Intensity, peak location &  1.891646 &  1.062034 &  1.375371 \\\midrule
%		Intensity, peak location &  0.905498‬ &  1.062034 &  1.375371 \\\midrule
%		GRNN &  \textbf{0.869348} &  \textbf{0.841675} &  \textbf{0.932388} \\\bottomrule
%	\end{tabular}
%\end{table}

\begin{table}
	\centering
	\caption{Summary of source location prediction errors for different localization methods}\label{table:methodsprederr}
	\begin{tabular}{lrrr}
		\toprule
		Method &       MSE, \si{\meter\squared} &       MAE, m &      RMSE, m \\\toprule
		Geometric &  8.37 &  1.92 &  2.89 \\\midrule
		Intensity Map, argmax &  2.50 &  1.26 &  1.58 \\\midrule
		%		Intensity, peak location &  1.891646 &  1.062034 &  1.375371 \\\midrule
		Intensity, peak location &  0.91 &  1.06 &  1.37 \\\midrule
		GRNN &  \textbf{0.86} &  \textbf{0.84} &  \textbf{0.93} \\\bottomrule
	\end{tabular}
\end{table}

The distributions of the prediction errors for different source localization methods are presented in \figurename{} \ref{fig:1prederrdistribution}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{img/from_diser/1pred_err_distribution}
	\caption{The distributions of the prediction errors for different source localization methods}
	\label{fig:1prederrdistribution}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

As can be seen from the experimental results, our method outperform the baseline methods for almost all parameter configurations.
Our method produces position estimation error that is \SI{24.2}{\percent} lower than using a geometrical source localization method, and \SI{19.1}{\percent} lower than using the intensity map method at low feature fitness threshold levels. When the threshold is high, the performance of all methods becomes comparable. It needs to be addressed that it is impractical to use high threshold values because it is possible that the sound source would not be localized at all (all its features are below the threshold).

As seen from the \figurename{} \ref{fig:rmsenneighbors}, parameter $ \numel_{\nneighbors} $ does not affect the performance of the baseline algorithms. This is the expected case, since this parameter is not involved in obtaining the source position estimations using the baseline methods.
As for the \grnn{} approahc, we observe that $ \numel_{\nneighbors} $ has little impact for the position estimation, and has only a marginal impact on the angular and radial source position estimation errors.
Nevertheless, in our method performed on average \SI{20.3}{\percent} better than the baseline methods when considering the source position estimation RMSE.


Considering the number of the nearest neighbors of the samples in the embedded space when constructing the training graph dataset, \figurename{} \ref{fig:rmsengnbrs} shows that the smallest source position estimation error is obtained when only a small number of graph nearest neighbors are selected. This might be due to the nonlinearity of the embedded space. The larger the number of considered neighbors, the further the samples are in the embedded space, and the larger the error.
As can be seen from the radial error plot, the number of the nearest graph neighbors considered is influencing the radial source position prediction error the most. The radial error is smallest when our algorithm considers 10 neighbors. 
Overall smallest source position error is achieved with 3 nearest graph neighbors.

As can be seen from the \figurename{} \ref{fig:rmsekrep}, the labeled sample repetition rate is not influencing the source position estimation error much. The angular error is reduced at high repetition rates, but the radial error is increased. This might be due to the condition where the labeled sample positions are condensed around the center of the enclosure, and the supervised loss function forces the network to predict source positions towards the center. This produces large estimation errors for the sound sources that are further away from the center of the enclosure.

The most suitable ratio of supervised to unsupervised loss, as can be seen from the \figurename{} \ref{fig:rmsemu}, is $ \mu = 0.5 $. 

The training batch size does not significantly affect the source position prediction error (\figurename{} \ref{fig:rmsebatchsize}).
%Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

A novel data-driven sound source localization method was proposed. Compared to baseline sound source localization methods, the geometrical and the intensity map-based method, our method showed an improvement of \SI{4.2}{\percent} of the source position estimation RMSE (\SI{0.86}{\m} compared to \SI{0.91}{\m}), compared to the best performing baseline method (intensity map-based with local peak location). The largest angular error improvement was \SI{20}{\percent} compared to the same baseline algorithm. The radial error improvement was \SI{40}{\percent}. For all experiments, the reverberation time of the enclosure was $ T_{60} = \SI{0.37}{\second}$. To conclude, our method showed to be a viable option when a data-driven approach is needed due to adversity of acoustic conditions within an enclosure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Patents}
%This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% optional
%%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}
%
%% Only for the journal Methods and Protocols:
%% If you wish to submit a video article, please do so with any other supplementary material.
%% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``Conceptualization, X.X. and Y.Y.; methodology, X.X.; software, X.X.; validation, X.X., Y.Y. and Z.Z.; formal analysis, X.X.; investigation, X.X.; resources, X.X.; data curation, X.X.; writing--original draft preparation, X.X.; writing--review and editing, X.X.; visualization, X.X.; supervision, X.X.; project administration, X.X.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.'', please turn to the  \href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work reported.}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\funding{Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript, or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\abbreviations{The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
\grnn{} & graph regulzarized neural network \\
SRP & steered response power  \\
PHAT & phase transform \\
\doa{} & direction of arrival \\
RMS & root mean square\\
MSE  & mean squared error \\
MAE   & mean average error \\
RMSE & root mean squared error \\
\isomap{}  & isometric mapping \\
NLDR  & non-linear dimensionality reduction \\
\end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% optional
%\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
%\appendix
%\section{}
%\unskip
%\subsection{}
%The appendix is an optional section that can contain details and data supplemental to the main text. For example, explanations of experimental details that would disrupt the flow of the main text, but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data is shown in the main text can be added here if brief, or as Supplementary data. Mathematical proofs of results not central to the paper can be added as an appendix.
%
%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\externalbibliography{yes}
\bibliography{SS_AS_MDPI_ASC}

%=====================================
% References, variant B: internal bibliography
%=====================================
%\begin{thebibliography}{999}
%% Reference 1
%\bibitem[Author1(year)]{ref-journal}
%Author1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
%% Reference 2
%\bibitem[Author2(year)]{ref-book}
%Author2, L. The title of the cited contribution. In {\em The Book Title}; Editor1, F., Editor2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
%\end{thebibliography}

% The following MDPI journals use author-date citation: Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, JRFM, Laws, Religions, Risks, Social Sciences. For those journals, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% optional
%\sampleavailability{Samples of the compounds ...... are available from the authors.}
%
%%% for journal Sci
%%\reviewreports{\\
%%Reviewer 1 comments and authors’ response\\
%%Reviewer 2 comments and authors’ response\\
%%Reviewer 3 comments and authors’ response
%%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

