
@inproceedings{carlettaAMIMeetingCorpus2006,
  title = {The {{AMI Meeting Corpus}}: {{A Pre}}-Announcement},
  shorttitle = {The {{AMI Meeting Corpus}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Machine Learning}} for {{Multimodal Interaction}}},
  author = {Carletta, Jean and and {others}},
  date = {2006},
  pages = {28--39},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11677482_3},
  url = {http://dx.doi.org/10.1007/11677482_3},
  urldate = {2019-04-24},
  abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QKR4AE3R\\Carletta et al. - 2006 - The AMI Meeting Corpus A Pre-announcement.pdf},
  isbn = {978-3-540-32549-9},
  options = {useprefix=true},
  series = {{{MLMI}}'05},
  venue = {Edinburgh, UK}
}

@inproceedings{carlettaj.etal.AMIMeetingCorpus2006,
  title = {The {{AMI Meeting Corpus}}: {{A Pre}}-Announcement},
  shorttitle = {The {{AMI Meeting Corpus}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Machine Learning}} for {{Multimodal Interaction}}},
  author = {Carletta, J. et al.},
  date = {2006},
  pages = {28--39},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11677482_3},
  url = {http://dx.doi.org/10.1007/11677482_3},
  urldate = {2019-04-24},
  abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\V3FH7XJM\\Carletta et al. - 2006 - The AMI Meeting Corpus A Pre-announcement.pdf},
  isbn = {978-3-540-32549-9},
  series = {{{MLMI}}'05},
  venue = {Edinburgh, UK}
}

@thesis{dibiaseHighaccuracyLowlatencyTechnique2000,
  title = {A High-Accuracy, Low-Latency Technique for Talker Localization in Reverberant Environments Using Microphone Arrays},
  author = {Dibiase, Joseph Hector},
  date = {2000-08-01},
  journaltitle = {Ph.D. Thesis},
  pages = {4877},
  url = {http://adsabs.harvard.edu/abs/2000PhDT........97D},
  urldate = {2020-06-07},
  abstract = {A combination of microphone arrays and sophisticated signal processing 
has been applied to the remote acquisition of high-quality speech audio.
These applications all exploit the spatial filtering ability of an
array, which allows the speech signal from one talker to be enhanced as
the signals from other talkers and unwanted sources are suppressed. This
process is known as ``beamforming''. Many array systems employ adaptive
algorithms that passively track the positions of one or more talkers and
adjust the array's focus accordingly. These adaptive algorithms must
produce accurate talker-location estimates at a high rate with minimal
latency. While the computation time required by the algorithm largely
determines the latency of the locator, it is the data requirements that
define theoretical limits. Hence, this thesis focuses on reducing the
size of the data segments necessary for accurate source localization in
realistic room environments. For various reasons, including the
reduction of computational costs, many source-localization algorithms
break the array into pairs of microphones. However, pairwise techniques
suffer considerably from acoustic reverberation especially when small
data segments are used. An alternative approach is one where a
beamformer is used to search over a predefined spatial region looking
for a peak (or peaks) in the power of its output signal. While this is
computationally more intensive than pairwise methods, it inherently
combines the signals from multiple microphones rather than reducing the
data from each pair to a single time-delay parameter. This approach is
able to compensate for the short duration of each data segment used for
localization by integrating the data from many, or all, of the
microphones prior to parameter estimation. A new steered-beamformer
method is proposed in this thesis, which combines the best features of
beamforming with those of a popular pairwise technique. Carefully
performed experiments, using real microphone- array data, demonstrate
that the new method produces highly-reliable location estimates, in
realistic rooms with reverberation times of 200 and 400 milliseconds,
using 25-millisecond data segments.}
}

@inproceedings{heAdaptationMultipleSound2019,
  title = {Adaptation of {{Multiple Sound Source Localization Neural Networks}} with {{Weak Supervision}} and {{Domain}}-Adversarial {{Training}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {He, Weipeng and Motlicek, Petr and Odobez, Jean-Marc},
  date = {2019-05},
  pages = {770--774},
  publisher = {{IEEE}},
  location = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682655},
  url = {https://ieeexplore.ieee.org/document/8682655/},
  urldate = {2019-05-16},
  abstract = {Despite the recent success of deep neural network-based approaches in sound source localization, these approaches suffer the limitations that the required annotation process is costly, and the mismatch between the training and test conditions undermines the performance. This paper addresses the question of how models trained with simulation can be exploited for multiple sound source localization in real scenarios by domain adaptation. In particular, two domain adaptation methods are investigated: weak supervision and domainadversarial training. Our experiments show that the weak supervision with the knowledge of the number of sources can significantly improve the performance of an unadapted model. However, the domain-adversarial training does not yield significant improvement for this particular problem.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LZCZDTXJ\\He et al. - 2019 - Adaptation of Multiple Sound Source Localization N.pdf},
  isbn = {978-1-4799-8131-1},
  langid = {english}
}

@inproceedings{heDeepNeuralNetworks2018b,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {He, W. and Motlicek, P. and Odobez, J.},
  date = {2018-05},
  pages = {74--79},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8461267},
  abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IVT56RQ8\\stamp.html},
  keywords = {acoustic generators,Artificial neural networks,deep neural networks,Delays,encoding,Encoding,Estimation,human-robot interaction,likelihood-based encoding,microphone arrays,Microphones,multiple sound sources,multiple speaker detection,network output,neural nets,neural network-based sound source localization methods,Robots,simultaneous detection,single sound source,sound mixtures,spatial spectrum-based approaches,speaker recognition}
}

@article{laufer-goldshteinSemiSupervisedSoundSource2016,
  title = {Semi-{{Supervised Sound Source Localization Based}} on {{Manifold Regularization}}},
  author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
  date = {2016-08},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {24},
  pages = {1393--1407},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2555085},
  url = {http://ieeexplore.ieee.org/document/7458213/},
  urldate = {2019-10-28},
  abstract = {Conventional speaker localization algorithms, based merely on the received microphone signals, are often sensitive to adverse conditions, such as: high reverberation or low signal-to-noise ratio (SNR). In some scenarios, e.g., in meeting rooms or cars, it can be assumed that the source position is confined to a predefined area, and the acoustic parameters of the environment are approximately fixed. Such scenarios give rise to the assumption that the acoustic samples from the region of interest have a distinct geometrical structure. In this paper, we show that the high-dimensional acoustic samples indeed lie on a low-dimensional manifold and can be embedded into a low-dimensional space. Motivated by this result, we propose a semi-supervised source localization algorithm based on two-microphone measurements, which recovers the inverse mapping between the acoustic samples and their corresponding locations. The idea is to use an optimization framework based on manifold regularization, that involves smoothness constraints of possible solutions with respect to the manifold. The proposed algorithm, termed manifold regularization for localization, is adapted while new unlabelled measurements (from unknown source locations) are accumulated during runtime. Experimental results show superior localization performance when compared with a recently presented algorithm based on a manifold learning approach and with the generalized cross-correlation algorithm as a baseline. The algorithm achieves 2◦ accuracy in typical noisy and reverberant environments (reverberation time between 200 and 800 ms and SNR between 5 and 20 dB).},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8CTVP6K7\\Laufer-Goldshtein et al. - 2016 - Semi-Supervised Sound Source Localization Based on.pdf},
  langid = {english},
  number = {8}
}

@article{martiSpeakerLocalizationDetection2011,
  title = {Speaker {{Localization}} and {{Detection}} in {{Videoconferencing Environments Using}} a {{Modified SRP}}-{{PHAT Algorithm}}},
  author = {Marti, A and Cobos, M and Aguilera, E and Lopez, J J},
  date = {2011},
  pages = {8},
  abstract = {The Steered Response Power - Phase Transform (SRP-PHAT) algorithm has been shown to be one of the most robust sound source localization approaches operating in noisy and reverberant environments. However, its practical implementation is usually based on a costly fine grid-search procedure, making the computational cost of the method a real issue. In this paper, we introduce an effective strategy which performs a full exploration of the sampled space rather than computing the SRP at discrete spatial positions, increasing its robustness and allowing for a coarser spatial grid that reduces the computational cost required in a practical implementation. The modified SRP-PHAT functional has been successfully implemented in a real time speaker localization system for multiparticipant videoconferencing environments. Moreover, a localization-based speech-non speech frame discriminator is presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G73BAHMI\\Marti et al. - 2011 - Speaker Localization and Detection in Videoconfere.pdf},
  langid = {english}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html},
  urldate = {2020-06-08},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SJHQMU3V\\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf},
  number = {85}
}

@inproceedings{scheiblerPyroomacousticsPythonPackage2018a,
  title = {Pyroomacoustics: {{A Python Package}} for {{Audio Room Simulation}} and {{Array Processing Algorithms}}},
  shorttitle = {Pyroomacoustics},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Scheibler, Robin and Bezzam, Eric and Dokmanić, Ivan},
  date = {2018-04},
  pages = {351--355},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461310},
  abstract = {We present pyroomacoustics, a software package aimed at the rapid development and testing of audio array processing algorithms. The content of the package can be divided into three main components: an intuitive Python object-oriented interface to quickly construct different simulation scenarios involving multiple sound sources and microphones in 2D and 3D rooms; a fast C implementation of the image source model for general polyhedral rooms to efficiently generate room impulse responses and simulate the propagation between sources and receivers; and finally, reference implementations of popular algorithms for beamforming, direction finding, and adaptive filtering. Together, they form a package with the potential to speed up the time to market of new algorithms by significantly reducing the implementation overhead in the performance evaluation step.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MQ2BHPVI\\8461310.html},
  keywords = {acoustic signal processing,adaptive filters,architectural acoustics,array signal processing,Array signal processing,audio array processing algorithms,audio room simulation,audio signal processing,fast C implementation,general polyhedral rooms,Generators,image source model,Mathematical model,microphone arrays,Microphones,multiple sound sources,Object oriented modeling,object-oriented programming,pyroomacoustics,Python,Python object-oriented interface,Python package,rapid prototyping,reference implementations,reproducibility,RIR,room impulse responses,simulation,software package,software packages,Three-dimensional displays,transient response}
}


