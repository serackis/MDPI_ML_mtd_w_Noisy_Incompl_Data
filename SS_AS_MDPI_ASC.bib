
@online{_about_????,
  title = {About {{AES Standards}}},
  url = {http://www.aes.org/standards/about/},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AZV38W7S\\about.html}
}

@online{_about_????,
  title = {About {{AES Standards}}},
  url = {http://www.aes.org/standards/about/},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7B5BJCDT\\about.html}
}

@misc{_aes67-2015:_2015,
  title = {{{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-Performance Streaming Audio-over-{{IP}} Interoperability},
  date = {2015-09-21},
  publisher = {{Audio Engineering Society}},
  abstract = {High-performance media networks support professional quality audio (16 bit, 44,1 kHz and higher) with low latencies (less than 10 milliseconds) compatible with live sound reinforcement. The level of network performance required to meet these requirements is available on local-area networks and is achievable on enterprise-scale networks. A number of networked audio systems have been developed to support high-performance media networking but until now there were no recommendations for operating these systems in an interoperable manner. This standard provides comprehensive interoperability recommendations in the areas of synchronization, media clock identification, network transport, encoding and streaming, session description and connection management.}
}

@article{_aes67-2015:_2015,
  title = {{{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-{{Performance}} Streaming Audio-over-{{IP}} Interoperability},
  date = {2015-09-21},
  publisher = {{Audio Engineering Society}},
  abstract = {High-performance media networks support professional quality audio (16 bit, 44,1 kHz and higher) with low latencies (less than 10 milliseconds) compatible with live sound reinforcement. The level of network performance required to meet these requirements is available on local-area networks and is achievable on enterprise-scale networks. A number of networked audio systems have been developed to support high-performance media networking but until now there were no recommendations for operating these systems in an interoperable manner. This standard provides comprehensive interoperability recommendations in the areas of synchronization, media clock identification, network transport, encoding and streaming, session description and connection management.}
}

@online{_audio_????,
  title = {An {{Audio Timeline}}},
  url = {http://www.aes.org/aeshc/docs/audio.history.timeline.html},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R4K3HSK7\\audio.history.timeline.html}
}

@online{_audio_????,
  title = {An {{Audio Timeline}}},
  url = {http://www.aes.org/aeshc/docs/audio.history.timeline.html},
  urldate = {2017-06-12}
}

@online{_electromechanical_????,
  title = {Electromechanical {{Telephone}}-{{Switching}} - {{Engineering}} and {{Technology History Wiki}}},
  url = {http://ethw.org/Electromechanical_Telephone-Switching},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WSPZXXH6\\Electromechanical_Telephone-Switching.html}
}

@online{_electromechanical_????,
  title = {Electromechanical {{Telephone}}-{{Switching}} - {{Engineering}} and {{Technology History Wiki}}},
  url = {http://ethw.org/Electromechanical_Telephone-Switching},
  urldate = {2017-06-12}
}

@article{_estimating_????,
  title = {Estimating {{Sound Levels With}} the {{Inverse Square Law}}},
  url = {http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/isprob2.html},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BY8G683U\\isprob2.html}
}

@online{_estimating_????,
  title = {Estimating {{Sound Levels With}} the {{Inverse Square Law}}},
  url = {http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/isprob2.html},
  urldate = {2017-04-03},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2N9BE78G\\isprob2.html}
}

@article{_estimating_????,
  title = {Estimating {{Sound Levels With}} the {{Inverse Square Law}}},
  url = {http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/isprob2.html}
}

@online{_estimating_????,
  title = {Estimating {{Sound Levels With}} the {{Inverse Square Law}}},
  url = {http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/isprob2.html},
  urldate = {2017-04-03},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5RXX9SAD\\isprob2.html}
}

@online{_iec_????,
  title = {{{IEC}} 62365 {{International Standard}}},
  url = {https://webstore.iec.ch/preview/info_iec62365%7Bed2.0%7Den.pdf},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QZSSUB7E\\info_iec62365 ed2.0 en.html}
}

@online{_iec_????,
  title = {{{IEC}} 62365 {{International Standard}}},
  url = {https://webstore.iec.ch/preview/info_iec62365%7Bed2.0%7Den.pdf},
  urldate = {2017-06-13}
}

@online{_isdn_????,
  title = {{{ISDN Audio Codec Guide FAQ}} at {{ISDNAUDIO}}.{{COM}}},
  url = {http://www.isdnaudio.com/isdnguide.html},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\A9CCFNNA\\isdnguide.html}
}

@online{_isdn_????,
  title = {{{ISDN Audio Codec Guide FAQ}} at {{ISDNAUDIO}}.{{COM}}},
  url = {http://www.isdnaudio.com/isdnguide.html},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GLNQLW3I\\isdnguide.html}
}

@patent{_multiplex_????,
  title = {Multiplex Telephony.},
  url = {http://www.google.com/patents/US745734},
  urldate = {2017-06-12},
  type = {patent}
}

@patent{_multiplex_????,
  title = {Multiplex Telephony.},
  url = {http://www.google.com/patents/US745734},
  urldate = {2017-06-12},
  type = {patent}
}

@online{11Audio,
  title = {11. {{Audio}}},
  url = {http://vr.cs.uiuc.edu/node339.html},
  urldate = {2017-12-09}
}

@online{11Audio,
  title = {11. {{Audio}}},
  url = {http://vr.cs.uiuc.edu/node339.html},
  urldate = {2017-12-09}
}

@online{12PDFOptimized,
  title = {(12) ({{PDF}}) {{Optimized Acoustic Localization}} with {{SRP}}-{{PHAT}} for {{Monitoring}} in {{Distributed Sensor Networks}}},
  url = {https://www.researchgate.net/publication/261467008_Optimized_Acoustic_Localization_with_SRP-PHAT_for_Monitoring_in_Distributed_Sensor_Networks},
  urldate = {2020-02-25},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VG4YEF8H\\261467008_Optimized_Acoustic_Localization_with_SRP-PHAT_for_Monitoring_in_Distributed_Sensor_Ne.html}
}

@online{12PDFOptimized,
  title = {(12) ({{PDF}}) {{Optimized Acoustic Localization}} with {{SRP}}-{{PHAT}} for {{Monitoring}} in {{Distributed Sensor Networks}}},
  url = {https://www.researchgate.net/publication/261467008_Optimized_Acoustic_Localization_with_SRP-PHAT_for_Monitoring_in_Distributed_Sensor_Networks},
  urldate = {2020-02-25},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MVJT8FMT\\261467008_Optimized_Acoustic_Localization_with_SRP-PHAT_for_Monitoring_in_Distributed_Sensor_Ne.html}
}

@online{14PDFTwoStage,
  title = {(14) ({{PDF}}) {{A Two}}-{{Stage Approach}} to {{2D DOA Estimation}} for a {{Compact Circular Microphone Array}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/281063456_A_Two-Stage_Approach_to_2D_DOA_Estimation_for_a_Compact_Circular_Microphone_Array},
  urldate = {2020-03-12},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\H93KNP3E\\281063456_A_Two-Stage_Approach_to_2D_DOA_Estimation_for_a_Compact_Circular_Microphone_Array.html},
  langid = {english}
}

@online{14PDFTwoStage,
  title = {(14) ({{PDF}}) {{A Two}}-{{Stage Approach}} to {{2D DOA Estimation}} for a {{Compact Circular Microphone Array}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/281063456_A_Two-Stage_Approach_to_2D_DOA_Estimation_for_a_Compact_Circular_Microphone_Array},
  urldate = {2020-03-12},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\F3RPPXKW\\281063456_A_Two-Stage_Approach_to_2D_DOA_Estimation_for_a_Compact_Circular_Microphone_Array.html},
  langid = {english}
}

@online{171111565Deep,
  title = {[1711.11565] {{Deep Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  url = {https://arxiv.org/abs/1711.11565#},
  urldate = {2019-04-24},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GQ59KJ5T\\1711.html}
}

@online{171111565Deep,
  title = {[1711.11565] {{Deep Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  url = {https://arxiv.org/abs/1711.11565#},
  urldate = {2019-04-24},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BV4A7PPL\\1711.html}
}

@online{180711094EndtoEnd,
  title = {[1807.11094] {{Towards End}}-to-{{End Acoustic Localization}} Using {{Deep Learning}}: From {{Audio Signal}} to {{Source Position Coordinates}}},
  url = {https://arxiv.org/abs/1807.11094},
  urldate = {2019-05-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6THDN6TD\\1807.html}
}

@online{180711094EndtoEnd,
  title = {[1807.11094] {{Towards End}}-to-{{End Acoustic Localization}} Using {{Deep Learning}}: {{From Audio Signal}} to {{Source Position Coordinates}}},
  url = {https://arxiv.org/abs/1807.11094},
  urldate = {2019-05-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\J7ZLI2MD\\1807.html}
}

@inproceedings{4538683,
  title = {A {{Real}}-{{Time Demonstrator}} for the {{2D Localization}} of Two {{Sound Sources Using Blind Adaptive MIMO System Identification}}},
  booktitle = {2008 {{Hands}}-{{Free Speech Communication}} and {{Microphone Arrays}}},
  author = {Lombard, A. and Kellermann, W. and Buchner, H.},
  date = {2008-05},
  pages = {41--44},
  doi = {10.1109/HSCMA.2008.4538683},
  abstract = {A real-time demonstrator for the 2D localization of two sound sources using two microphone pairs is presented and evaluated. The scheme relies on Blind Source Separation (BSS) to adaptively identify the acoustical MIMO system, hence allowing the estimation of relative time delays for each source and each dimension. Extending our previously presented work [1], a mechanism to solve a pairing problem occuring in the multidimensional localization of several sources is described. It exploits the inherent signal extraction abilities of BSS. Experimental evaluations with large microphone apertures show that the demonstrator can accurately localize two speech sources in a 2D space, with a precision better than one degree.},
  keywords = {2D localization,acoustic signal processing,Acoustic source localization,acoustical MIMO system,adaptive signal processing,Adaptive systems,blind adaptive MIMO system identification,blind source separation,Blind source separation,Delay effects,Delay estimation,inherent signal extraction,microphone pairs,microphones,Microphones,MIMO,MIMO systems,Multidimensional systems,Real time systems,real-time demonstrator,Source separation,System identification,TDOA estimation}
}

@inproceedings{4538683,
  title = {A {{Real}}-{{Time Demonstrator}} for the {{2D Localization}} of Two {{Sound Sources Using Blind Adaptive MIMO System Identification}}},
  booktitle = {2008 {{Hands}}-{{Free Speech Communication}} and {{Microphone Arrays}}},
  author = {Lombard, A. and Kellermann, W. and Buchner, H.},
  date = {2008-05},
  pages = {41--44},
  doi = {10.1109/HSCMA.2008.4538683},
  abstract = {A real-time demonstrator for the 2D localization of two sound sources using two microphone pairs is presented and evaluated. The scheme relies on Blind Source Separation (BSS) to adaptively identify the acoustical MIMO system, hence allowing the estimation of relative time delays for each source and each dimension. Extending our previously presented work [1], a mechanism to solve a pairing problem occuring in the multidimensional localization of several sources is described. It exploits the inherent signal extraction abilities of BSS. Experimental evaluations with large microphone apertures show that the demonstrator can accurately localize two speech sources in a 2D space, with a precision better than one degree.},
  keywords = {2D localization,acoustic signal processing,Acoustic source localization,acoustical MIMO system,adaptive signal processing,Adaptive systems,blind adaptive MIMO system identification,blind source separation,Blind source separation,Delay effects,Delay estimation,inherent signal extraction,microphone pairs,microphones,Microphones,MIMO,MIMO systems,Multidimensional systems,Real time systems,real-time demonstrator,Source separation,System identification,TDOA estimation}
}

@inproceedings{4959563,
  title = {Multidimensional Localization of Multiple Sound Sources Using Averaged Directivity Patterns of {{Blind Source Separation}} Systems},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lombard, A. and Rosenkranz, T. and Buchner, H. and Kellermann, W.},
  date = {2009-04},
  pages = {233--236},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2009.4959563},
  abstract = {In this paper, we propose a versatile acoustic source localization framework exploiting the self-steering capability of Blind Source Separation (BSS) algorithms. We provide a way to produce an acoustical map of the scene by computing the averaged directivity pattern of BSS demixing systems. Since BSS explicitly accounts for multiple sources in its signal propagation model, several simultaneously active sound sources can be located using this method. Moreover, the framework is suitable to any microphone array geometry, which allows application for multiple dimensions, in the near field as well as in the far field. Experiments demonstrate the efficiency of the proposed scheme in a reverberant environment for the localization of speech sources.},
  keywords = {acoustic radiators,Acoustic sensors,Acoustic source localization,acoustic source localization framework,averaged directivity patterns,blind source separation,Blind source separation,blind source separation systems,Finite impulse response filter,Frequency,Information filtering,Information filters,microphone arrays,Microphone arrays,multidimensional localization,multidimensional signal processing,Multidimensional systems,multiple sound sources,Sensor arrays,Source separation}
}

@inproceedings{4959563,
  title = {Multidimensional Localization of Multiple Sound Sources Using Averaged Directivity Patterns of {{Blind Source Separation}} Systems},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lombard, A. and Rosenkranz, T. and Buchner, H. and Kellermann, W.},
  date = {2009-04},
  pages = {233--236},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2009.4959563},
  abstract = {In this paper, we propose a versatile acoustic source localization framework exploiting the self-steering capability of Blind Source Separation (BSS) algorithms. We provide a way to produce an acoustical map of the scene by computing the averaged directivity pattern of BSS demixing systems. Since BSS explicitly accounts for multiple sources in its signal propagation model, several simultaneously active sound sources can be located using this method. Moreover, the framework is suitable to any microphone array geometry, which allows application for multiple dimensions, in the near field as well as in the far field. Experiments demonstrate the efficiency of the proposed scheme in a reverberant environment for the localization of speech sources.},
  keywords = {acoustic radiators,Acoustic sensors,Acoustic source localization,acoustic source localization framework,averaged directivity patterns,blind source separation,Blind source separation,blind source separation systems,Finite impulse response filter,Frequency,Information filtering,Information filters,microphone arrays,Microphone arrays,multidimensional localization,multidimensional signal processing,Multidimensional systems,multiple sound sources,Sensor arrays,Source separation}
}

@inproceedings{5463319,
  title = {Analysis of Room Reverberation Effects in Source Localization Using Small Microphone Arrays},
  booktitle = {2010 4th {{International Symposium}} on {{Communications}}, {{Control}} and {{Signal Processing}} ({{ISCCSP}})},
  author = {Cobos, M. and Lopez, J. J. and Spors, S.},
  date = {2010-03},
  pages = {1--4},
  doi = {10.1109/ISCCSP.2010.5463319},
  abstract = {Small microphone arrays provide many advantages for real-world audio applications. Together with digital signal processing, their enhanced acoustic properties can be exploited in many speech processing systems, such as hands-free devices, videoconferencing or hearing aids. Among their classical applications is source localization, which is usually based on the estimation of Time-Differences-Of-Arrival (TDOA). The accuracy of these methods depends on the degree of reverberation, due to the increased variance found in TDOA estimates. In this paper, we characterize reverberation in a room by means of a small three-microphone array. Our experiments show that the directional distribution of time-frequency estimates is highly correlated with the room's reverberation. This correlation results in a model that can be very useful for both estimating reverberation time and setting the resolution achievable in source localization tasks.},
  keywords = {acoustic radiators,Acoustic signal processing,architectural acoustics,Direction of arrival estimation,directional distribution,Frequency,Geometry,microphone arrays,Microphone arrays,Process control,reverberation,Reverberation,room reverberation effects,Signal processing algorithms,Signal resolution,source localization,Speech processing,time-differences-of-arrival estimation,time-frequency estimation}
}

@inproceedings{5463319,
  title = {Analysis of Room Reverberation Effects in Source Localization Using Small Microphone Arrays},
  booktitle = {2010 4th {{International Symposium}} on {{Communications}}, {{Control}} and {{Signal Processing}} ({{ISCCSP}})},
  author = {Cobos, M. and Lopez, J. J. and Spors, S.},
  date = {2010-03},
  pages = {1--4},
  doi = {10.1109/ISCCSP.2010.5463319},
  abstract = {Small microphone arrays provide many advantages for real-world audio applications. Together with digital signal processing, their enhanced acoustic properties can be exploited in many speech processing systems, such as hands-free devices, videoconferencing or hearing aids. Among their classical applications is source localization, which is usually based on the estimation of Time-Differences-Of-Arrival (TDOA). The accuracy of these methods depends on the degree of reverberation, due to the increased variance found in TDOA estimates. In this paper, we characterize reverberation in a room by means of a small three-microphone array. Our experiments show that the directional distribution of time-frequency estimates is highly correlated with the room's reverberation. This correlation results in a model that can be very useful for both estimating reverberation time and setting the resolution achievable in source localization tasks.},
  keywords = {acoustic radiators,Acoustic signal processing,architectural acoustics,Direction of arrival estimation,directional distribution,Frequency,Geometry,microphone arrays,Microphone arrays,Process control,reverberation,Reverberation,room reverberation effects,Signal processing algorithms,Signal resolution,source localization,Speech processing,time-differences-of-arrival estimation,time-frequency estimation}
}

@online{572f2dc2fd79c4bfd7987d53b8e739051d30Pdf,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.Pdf},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf},
  urldate = {2017-01-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\J3UVBNAS\\572f2dc2fd79c4bfd7987d53b8e739051d30.pdf.html}
}

@online{572f2dc2fd79c4bfd7987d53b8e739051d30Pdf,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf},
  urldate = {2017-01-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\H2RABVWK\\572f2dc2fd79c4bfd7987d53b8e739051d30.pdf.html}
}

@online{572f2dc2fd79c4bfd7987d53b8e739051d30Pdfa,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.Pdf},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf},
  urldate = {2017-04-03}
}

@article{572f2dc2fd79c4bfd7987d53b8e739051d30pdfa,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf}
}

@online{572f2dc2fd79c4bfd7987d53b8e739051d30Pdfa,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf},
  urldate = {2017-04-03}
}

@article{572f2dc2fd79c4bfd7987d53b8e739051d30pdfa,
  title = {572f2dc2fd79c4bfd7987d53b8e739051d30.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/358b/572f2dc2fd79c4bfd7987d53b8e739051d30.pdf}
}

@inproceedings{7989450,
  title = {Bringing {{Mobile Robot Olfaction}} to the next Dimension — {{UAV}}-Based Remote Sensing of Gas Clouds and Source Localization},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Neumann, P. P. and Kohlhoff, H. and Hüllmann, D. and Lilienthal, A. J. and Kluge, M.},
  date = {2017-05},
  pages = {3910--3916},
  doi = {10.1109/ICRA.2017.7989450},
  abstract = {This paper introduces a novel robotic platform for aerial remote gas sensing. Spectroscopic measurement methods for remote sensing of selected gases lend themselves for use on mini-copters, which offer a number of advantages for inspection and surveillance. No direct contact with the target gas is needed and thus the influence of the aerial platform on the measured gas plume can be kept to a minimum. This allows to overcome one of the major issues with gas-sensitive mini-copters. On the other hand, remote gas sensors, most prominently Tunable Diode Laser Absorption Spectroscopy (TDLAS) sensors have been too bulky given the payload and energy restrictions of mini-copters. Here, we introduce and present the Unmanned Aerial Vehicle for Remote Gas Sensing (UAV-REGAS), which combines a novel lightweight TDLAS sensor with a 3-axis aerial stabilization gimbal for aiming on a versatile hexacopter. The proposed system can be deployed in scenarios that cannot be addressed by currently available robots and thus constitutes a significant step forward for the field of Mobile Robot Olfaction (MRO). It enables tomographic reconstruction of gas plumes and a localization of gas sources. We also present first results showing the gas sensing and aiming capabilities under realistic conditions.},
  keywords = {3-axis aerial stabilization gimbal,aerial platform,aerial remote gas sensing,autonomous aerial vehicles,gas clouds,Gas detectors,gas plumes,gas sources,gas-sensitive mini-copters,inspection,Laser beams,Measurement by laser beam,mobile robot olfaction,mobile robots,MRO,Payloads,remote gas sensors,Robot sensing systems,robotic platform,source localization,spectroscopic measurement methods,surveillance,TDLAS sensors,Tunable Diode Laser Absorption Spectroscopy,UAV-based remote sensing,UAV-REGAS,unmanned aerial vehicle,versatile hexacopter}
}

@inproceedings{7989450,
  title = {Bringing {{Mobile Robot Olfaction}} to the next Dimension — {{UAV}}-{{Based}} Remote Sensing of Gas Clouds and Source Localization},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Neumann, P. P. and Kohlhoff, H. and Hüllmann, D. and Lilienthal, A. J. and Kluge, M.},
  date = {2017-05},
  pages = {3910--3916},
  doi = {10.1109/ICRA.2017.7989450},
  abstract = {This paper introduces a novel robotic platform for aerial remote gas sensing. Spectroscopic measurement methods for remote sensing of selected gases lend themselves for use on mini-copters, which offer a number of advantages for inspection and surveillance. No direct contact with the target gas is needed and thus the influence of the aerial platform on the measured gas plume can be kept to a minimum. This allows to overcome one of the major issues with gas-sensitive mini-copters. On the other hand, remote gas sensors, most prominently Tunable Diode Laser Absorption Spectroscopy (TDLAS) sensors have been too bulky given the payload and energy restrictions of mini-copters. Here, we introduce and present the Unmanned Aerial Vehicle for Remote Gas Sensing (UAV-REGAS), which combines a novel lightweight TDLAS sensor with a 3-axis aerial stabilization gimbal for aiming on a versatile hexacopter. The proposed system can be deployed in scenarios that cannot be addressed by currently available robots and thus constitutes a significant step forward for the field of Mobile Robot Olfaction (MRO). It enables tomographic reconstruction of gas plumes and a localization of gas sources. We also present first results showing the gas sensing and aiming capabilities under realistic conditions.},
  keywords = {3-axis aerial stabilization gimbal,aerial platform,aerial remote gas sensing,autonomous aerial vehicles,gas clouds,Gas detectors,gas plumes,gas sources,gas-sensitive mini-copters,inspection,Laser beams,Measurement by laser beam,mobile robot olfaction,mobile robots,MRO,Payloads,remote gas sensors,Robot sensing systems,robotic platform,source localization,spectroscopic measurement methods,surveillance,TDLAS sensors,Tunable Diode Laser Absorption Spectroscopy,UAV-based remote sensing,UAV-REGAS,unmanned aerial vehicle,versatile hexacopter}
}

@inproceedings{8448644,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Löllmann, H. W. and Evers, C. and Schmidt, A. and Mellmann, H. and Barfuss, H. and Naylor, P. A. and Kellermann, W.},
  date = {2018-07},
  pages = {410--414},
  issn = {2151-870X},
  doi = {10.1109/SAM.2018.8448644},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  keywords = {acoustic signal processing,acoustic source localization,Acoustics,Array signal processing,Arrays,audio recording,audio signal processing,autonomous systems,Databases,hearing aids,IEEE-AASP Challenge,LOCATA Challenge data corpus,loudspeakers,microphone array,microphone arrays,Microphone arrays,numerous algorithms,signal classification,sound source localization,spherical microphone array,state-of-the-art algorithms,static microphone array,Task analysis,tele-conferencing systems}
}

@inproceedings{8448644,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Löllmann, H. W. and Evers, C. and Schmidt, A. and Mellmann, H. and Barfuss, H. and Naylor, P. A. and Kellermann, W.},
  date = {2018-07},
  pages = {410--414},
  issn = {2151-870X},
  doi = {10.1109/SAM.2018.8448644},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  keywords = {acoustic signal processing,acoustic source localization,Acoustics,Array signal processing,Arrays,audio recording,audio signal processing,autonomous systems,Databases,hearing aids,IEEE-AASP Challenge,LOCATA Challenge data corpus,loudspeakers,microphone array,microphone arrays,Microphone arrays,numerous algorithms,signal classification,sound source localization,spherical microphone array,state-of-the-art algorithms,static microphone array,Task analysis,tele-conferencing systems}
}

@article{abidConcreteAutoencodersDifferentiable2019,
  title = {Concrete {{Autoencoders}} for {{Differentiable Feature Selection}} and {{Reconstruction}}},
  author = {Abid, Abubakar and Balin, Muhammad Fatih and Zou, James},
  date = {2019-01-30},
  url = {http://arxiv.org/abs/1901.09346},
  urldate = {2019-10-29},
  abstract = {We introduce the concrete autoencoder, an endto-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-theart methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be use to impute the expression levels of the remaining genes. In doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20\%. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder.},
  archivePrefix = {arXiv},
  eprint = {1901.09346},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QJVLJS3J\\Abid et al. - 2019 - Concrete Autoencoders for Differentiable Feature S.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{abidConcreteAutoencodersDifferentiable2019,
  title = {Concrete {{Autoencoders}} for {{Differentiable Feature Selection}} and {{Reconstruction}}},
  author = {Abid, Abubakar and Balin, Muhammad Fatih and Zou, James},
  date = {2019-01-30},
  url = {http://arxiv.org/abs/1901.09346},
  urldate = {2019-10-29},
  abstract = {We introduce the concrete autoencoder, an endto-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-theart methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be use to impute the expression levels of the remaining genes. In doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20\%. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder.},
  archivePrefix = {arXiv},
  eprint = {1901.09346},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JTJL6EVQ\\Abid et al. - 2019 - Concrete Autoencoders for Differentiable Feature S.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{abutalebiPerformanceImprovementTDOABased2011,
  title = {Performance {{Improvement}} of {{TDOA}}-{{Based Speaker Localization}} in {{Joint Noisy}} and {{Reverberant Conditions}}},
  author = {Abutalebi, Hamid Reza and Momenzadeh, Hossein},
  date = {2011-12},
  journaltitle = {EURASIP Journal on Advances in Signal Processing},
  volume = {2011},
  issn = {1687-6180},
  doi = {10.1155/2011/621390},
  url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2011/621390},
  urldate = {2018-05-21},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VQCRGJSK\\Abutalebi and Momenzadeh - 2011 - Performance Improvement of TDOA-Based Speaker Loca.pdf},
  langid = {english},
  number = {1}
}

@article{abutalebiPerformanceImprovementTDOABased2011,
  title = {Performance {{Improvement}} of {{TDOA}}-{{Based Speaker Localization}} in {{Joint Noisy}} and {{Reverberant Conditions}}},
  author = {Abutalebi, Hamid Reza and Momenzadeh, Hossein},
  date = {2011-12},
  journaltitle = {EURASIP Journal on Advances in Signal Processing},
  volume = {2011},
  issn = {1687-6180},
  doi = {10.1155/2011/621390},
  url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2011/621390},
  urldate = {2018-05-21},
  langid = {english},
  number = {1}
}

@online{AccelerationDecisionMaking,
  title = {Acceleration of Decision Making in Sound Event Recognition Employing Supercomputing Cluster},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025513008414},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SEHSNX48\\S0020025513008414.html}
}

@online{AccelerationDecisionMaking,
  title = {Acceleration of Decision Making in Sound Event Recognition Employing Supercomputing Cluster},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025513008414},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JANYGGWZ\\S0020025513008414.html}
}

@online{AcousticBeamforming,
  title = {(3) {{Acoustic Beamforming}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/315695379_Acoustic_Beamforming},
  urldate = {2018-05-21},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RZKMVUML\\(3) Acoustic Beamforming.pdf;C\:\\Users\\sauli\\Zotero\\storage\\R35N2K6M\\315695379_Acoustic_Beamforming.html},
  langid = {english}
}

@online{AcousticBeamforming,
  title = {(3) {{Acoustic Beamforming}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/315695379_Acoustic_Beamforming},
  urldate = {2018-05-21},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english}
}

@online{AcousticFieldAnalysis,
  title = {Acoustic {{Field Analysis}} in {{Small Microphone Arrays}} - 4735.Pdf},
  url = {http://publications.rwth-aachen.de/record/229274/files/4735.pdf},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CW2FC4KE\\4735.html}
}

@online{AcousticFieldAnalysis,
  title = {Acoustic {{Field Analysis}} in {{Small Microphone Arrays}} - 4735.{{Pdf}}},
  url = {http://publications.rwth-aachen.de/record/229274/files/4735.pdf},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3RF68DB3\\4735.html}
}

@online{AcousticFieldAnalysisa,
  title = {Acoustic {{Field Analysis}} in {{Small Microphone Arrays}} - 4735.Pdf},
  url = {http://publications.rwth-aachen.de/record/229274/files/4735.pdf},
  urldate = {2017-01-10}
}

@online{AcousticFieldAnalysisa,
  title = {Acoustic {{Field Analysis}} in {{Small Microphone Arrays}} - 4735.{{Pdf}}},
  url = {http://publications.rwth-aachen.de/record/229274/files/4735.pdf},
  urldate = {2017-01-10}
}

@online{ActiveSpeechSource,
  title = {Active Speech Source Localization by a Dual Coarse-to-Fine Search - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/940366},
  urldate = {2020-04-11}
}

@online{ActiveSpeechSource,
  title = {Active Speech Source Localization by a Dual Coarse-to-Fine Search - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/940366},
  urldate = {2020-04-11}
}

@online{AdaptiveBeamformerWikipedia,
  title = {Adaptive Beamformer - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Adaptive_beamformer},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\28SIJBUU\\Adaptive_beamformer.html}
}

@online{AdaptiveBeamformerWikipedia,
  title = {Adaptive Beamformer - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Adaptive_beamformer},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6W4IZAL7\\Adaptive_beamformer.html}
}

@online{AdaptiveBeamformingRecursive,
  title = {Adaptive {{Beamforming}} and {{Recursive DOA Estimation Using Frequency}}-{{Invariant Uniform Concentric Spherical Arrays}} - {{Semantic Scholar}}},
  url = {/paper/Adaptive-Beamforming-and-Recursive-DOA-Estimation-Chen-Chan/28d67ae4234e9e6b6c83008e9886730adf064140},
  urldate = {2018-05-29},
  abstract = {This paper proposes recursive adaptive beamforming and broadband 2D direction-of-arrival (DOA) estimation algorithms for uniform concentric spherical arrays (UCSAs) having nearly frequency-invariant (FI) characteristics. The basic principle of the FI-UCSA is to transform the received signals to the phase mode and remove the frequency dependency of individual phase modes through a digital beamforming network. Hence, the far-field pattern of the array is determined by a set of weights. Thanks to the FI characteristic, traditional narrowband adaptive beamforming algorithms such as minimum variance beamforming and the generalized sidelobe canceller method can be applied to the FI-UCSA. Simulation results show that the proposed adaptive FI-UCSA beamformer achieves a lower steady-state error and converges faster than the conventional tapped-delay line approach while requiring fewer adaptive coefficients. A new broadband 2-D DOA estimation algorithm using ESPRIT techniques for FI-UCSA is proposed to recursively estimate the DOAs of the moving targets. Simulation results show that the proposed DOA estimation algorithm achieves a satisfactory performance for slowly varying sources at low arithmetic complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2JSN5WNE\\28d67ae4234e9e6b6c83008e9886730adf064140.html}
}

@online{AdaptiveBeamformingRecursive,
  title = {Adaptive {{Beamforming}} and {{Recursive DOA Estimation Using Frequency}}-{{Invariant Uniform Concentric Spherical Arrays}} - {{Semantic Scholar}}},
  url = {/paper/Adaptive-Beamforming-and-Recursive-DOA-Estimation-Chen-Chan/28d67ae4234e9e6b6c83008e9886730adf064140},
  urldate = {2018-05-29},
  abstract = {This paper proposes recursive adaptive beamforming and broadband 2D direction-of-arrival (DOA) estimation algorithms for uniform concentric spherical arrays (UCSAs) having nearly frequency-invariant (FI) characteristics. The basic principle of the FI-UCSA is to transform the received signals to the phase mode and remove the frequency dependency of individual phase modes through a digital beamforming network. Hence, the far-field pattern of the array is determined by a set of weights. Thanks to the FI characteristic, traditional narrowband adaptive beamforming algorithms such as minimum variance beamforming and the generalized sidelobe canceller method can be applied to the FI-UCSA. Simulation results show that the proposed adaptive FI-UCSA beamformer achieves a lower steady-state error and converges faster than the conventional tapped-delay line approach while requiring fewer adaptive coefficients. A new broadband 2-D DOA estimation algorithm using ESPRIT techniques for FI-UCSA is proposed to recursively estimate the DOAs of the moving targets. Simulation results show that the proposed DOA estimation algorithm achieves a satisfactory performance for slowly varying sources at low arithmetic complexity.}
}

@article{adavanneDirectionArrivalEstimation2017,
  title = {Direction of Arrival Estimation for Multiple Sound Sources Using Convolutional Recurrent Neural Network},
  author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
  date = {2017-10-27},
  url = {http://arxiv.org/abs/1710.10059},
  urldate = {2019-08-07},
  abstract = {This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio.},
  archivePrefix = {arXiv},
  eprint = {1710.10059},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UPPZCG4E\\Adavanne et al. - 2017 - Direction of arrival estimation for multiple sound.pdf},
  keywords = {*****,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,MUST READ},
  langid = {english},
  primaryClass = {cs, eess}
}

@article{adavanneDirectionArrivalEstimation2017,
  title = {Direction of Arrival Estimation for Multiple Sound Sources Using Convolutional Recurrent Neural Network},
  author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
  date = {2017-10-27},
  url = {http://arxiv.org/abs/1710.10059},
  urldate = {2019-08-07},
  abstract = {This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio.},
  archivePrefix = {arXiv},
  eprint = {1710.10059},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\27546JQX\\Adavanne et al. - 2017 - Direction of arrival estimation for multiple sound.pdf},
  keywords = {*****,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,MUST READ},
  langid = {english},
  primaryClass = {cs, eess}
}

@article{adel_beamforming_2012,
  title = {Beamforming {{Techniques}} for {{Multichannel}} Audio {{Signal Separation}}},
  author = {Adel, Hidri and Souad, Meddeb and Alaqeeli, Abdulqadir and Hamid, Amiri},
  date = {2012-12-25},
  journaltitle = {ResearchGate},
  volume = {6},
  issn = {1975-9339},
  doi = {10.4156/jdcta.vol6.issue20.72},
  url = {https://www.researchgate.net/publication/233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Beamforming Techniques for Multichannel audio Signal Separation on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\S43MXG8A\\Adel et al. - 2012 - Beamforming Techniques for Multichannel audio Sign.pdf;C\:\\Users\\sauli\\Zotero\\storage\\RKM543S6\\233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation.html},
  number = {20}
}

@article{adel_beamforming_2012,
  title = {Beamforming {{Techniques}} for {{Multichannel}} Audio {{Signal Separation}}},
  author = {Adel, Hidri and Souad, Meddeb and Alaqeeli, Abdulqadir and Hamid, Amiri},
  date = {2012-12-25},
  journaltitle = {ResearchGate},
  volume = {6},
  issn = {1975-9339},
  doi = {10.4156/jdcta.vol6.issue20.72},
  url = {https://www.researchgate.net/publication/233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Beamforming Techniques for Multichannel audio Signal Separation on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CGVE7CJ2\\Adel et al. - 2012 - Beamforming Techniques for Multichannel audio Sign.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ISAEVZ56\\233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation.html},
  number = {20}
}

@misc{AES672015AESStandard,
  title = {{{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-Performance Streaming Audio-over-{{IP}} Interoperability},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=96},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7789QBRA\\search.html}
}

@article{AES672015AESStandard,
  title = {{{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-{{Performance}} Streaming Audio-over-{{IP}} Interoperability},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=96},
  urldate = {2017-06-13}
}

@online{AESJournalForum,
  title = {{{AES Journal Forum}} » {{Comparison}} of {{Different Impulse Response Measurement Techniques}}},
  url = {https://secure.aes.org/forum/pubs/journal/?elib=11083},
  urldate = {2019-06-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8R7J8WSN\\journal.html}
}

@online{AESJournalForum,
  title = {{{AES Journal Forum}} » {{Comparison}} of {{Different Impulse Response Measurement Techniques}}},
  url = {https://secure.aes.org/forum/pubs/journal/?elib=11083},
  urldate = {2019-06-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XZ7D6P6I\\journal.html}
}

@online{AESStandardAES112009,
  title = {{{AES Standard}} » {{AES11}}-2009 (R2014): {{AES}} Recommended Practice for Digital Audio Engineering - {{Synchronization}} of Digital Audio Equipment in Studio Operations. ({{Revision}} of {{AES11}}-2003)},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=18},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RZCPAU3M\\search.html}
}

@online{AESStandardAES112009,
  title = {{{AES Standard}} » {{AES11}}-2009 ({{R2014}}): {{AES}} Recommended Practice for Digital Audio Engineering - {{Synchronization}} of Digital Audio Equipment in Studio Operations. ({{Revision}} of {{AES11}}-2003)},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=18},
  urldate = {2017-06-13}
}

@online{AESStandardAES472006,
  title = {{{AES Standard}} » {{AES47}}-2006 (S2017): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Transmission}} of Digital Audio over Asynchronous Transfer Mode ({{ATM}}) Networks},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=42},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PU9P5CIR\\search.html}
}

@online{AESStandardAES472006,
  title = {{{AES Standard}} » {{AES47}}-2006 ({{S2017}}): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Transmission}} of Digital Audio over Asynchronous Transfer Mode ({{ATM}}) Networks},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=42},
  urldate = {2017-06-13}
}

@online{AESStandardAES512006,
  title = {{{AES Standard}} » {{AES51}}-2006 (S2017): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Transmission}} of {{ATM}} Cells over {{Ethernet}} Physical Layer},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=49},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DRBBE2NF\\search.html}
}

@online{AESStandardAES512006,
  title = {{{AES Standard}} » {{AES51}}-2006 ({{S2017}}): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Transmission}} of {{ATM}} Cells over {{Ethernet}} Physical Layer},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=49},
  urldate = {2017-06-13}
}

@online{AESStandardAES532006,
  title = {{{AES Standard}} » {{AES53}}-2006 (R2011): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Sample}}-Accurate Timing in {{AES47}}},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=55},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GU4AQ6V4\\search.html}
}

@online{AESStandardAES532006,
  title = {{{AES Standard}} » {{AES53}}-2006 ({{R2011}}): {{AES}} Standard for Digital Audio - {{Digital}} Input-Output Interfacing - {{Sample}}-{{Accurate}} Timing in {{AES47}}},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=55},
  urldate = {2017-06-13}
}

@online{AESStandardAES672015,
  title = {{{AES Standard}} » {{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-Performance Streaming Audio-over-{{IP}} Interoperability},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=96},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\29ED9IJD\\search.html}
}

@online{AESStandardAES672015,
  title = {{{AES Standard}} » {{AES67}}-2015: {{AES}} Standard for Audio Applications of Networks - {{High}}-{{Performance}} Streaming Audio-over-{{IP}} Interoperability},
  url = {http://www.aes.org/publications/standards/search.cfm?docID=96},
  urldate = {2017-06-13}
}

@incollection{ahrens_physical_2012,
  title = {Physical {{Fundamentals}} of {{Sound Fields}}},
  booktitle = {Analytic {{Methods}} of {{Sound Field Synthesis}}},
  author = {Ahrens, Jens},
  date = {2012},
  pages = {21--55},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-25743-8_2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-25743-8_2},
  urldate = {2017-01-10},
  abstract = {The present chapter outlines the mathematical and physical tools that are employed in the subsequent chapters. It is not written in a tutorial style but serves rather as a reference. The wave equation and its solutions in Cartesian as well as in spherical coordinates are introduced. Then, a number of useful representations of sound fields such as the wavenumber domain, spherical harmonics expansions, the angular spectrum representation, and alike are presented. The basis for the solutions to the problem of sound field synthesis is set by a discussion of useful integral relations such as the Rayleigh Integral and the Kirchhoff Helmholtz Integral.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HGRSMHTN\\Ahrens - 2012 - Physical Fundamentals of Sound Fields.pdf;C\:\\Users\\sauli\\Zotero\\storage\\6CQ57QIS\\10.html},
  isbn = {978-3-642-25742-1 978-3-642-25743-8},
  keywords = {Acoustics,Engineering Acoustics},
  langid = {english},
  series = {T-{{Labs Series}} in {{Telecommunication Services}}}
}

@incollection{ahrens_physical_2012,
  title = {Physical {{Fundamentals}} of {{Sound Fields}}},
  booktitle = {Analytic {{Methods}} of {{Sound Field Synthesis}}},
  author = {Ahrens, Jens},
  date = {2012},
  pages = {21--55},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-25743-8_2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-25743-8_2},
  urldate = {2017-01-10},
  abstract = {The present chapter outlines the mathematical and physical tools that are employed in the subsequent chapters. It is not written in a tutorial style but serves rather as a reference. The wave equation and its solutions in Cartesian as well as in spherical coordinates are introduced. Then, a number of useful representations of sound fields such as the wavenumber domain, spherical harmonics expansions, the angular spectrum representation, and alike are presented. The basis for the solutions to the problem of sound field synthesis is set by a discussion of useful integral relations such as the Rayleigh Integral and the Kirchhoff Helmholtz Integral.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EDY59TE3\\Ahrens - 2012 - Physical Fundamentals of Sound Fields.pdf;C\:\\Users\\sauli\\Zotero\\storage\\DWEQ5L9H\\10.html},
  isbn = {978-3-642-25742-1 978-3-642-25743-8},
  keywords = {Acoustics,Engineering Acoustics},
  langid = {english},
  series = {T-{{Labs Series}} in {{Telecommunication Services}}}
}

@article{alainWhatRegularizedAutoEncoders,
  title = {What {{Regularized Auto}}-{{Encoders Learn}} from the {{Data}}-{{Generating Distribution}}},
  author = {Alain, Guillaume and Bengio, Yoshua},
  pages = {31},
  abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6ZFBNVVF\\Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf},
  langid = {english}
}

@article{alainWhatRegularizedAutoEncoders,
  title = {What {{Regularized Auto}}-{{Encoders Learn}} from the {{Data}}-{{Generating Distribution}}},
  author = {Alain, Guillaume and Bengio, Yoshua},
  pages = {31},
  abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XVBKK8A9\\Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf},
  langid = {english}
}

@inproceedings{alameda-pinedaGeometricallyconstrainedRobustTime2012,
  title = {Geometrically-Constrained Robust Time Delay Estimation Using Non-Coplanar Microphone Arrays},
  booktitle = {2012 {{Proceedings}} of the 20th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Alameda-Pineda, X. and Horaud, R.},
  date = {2012-08},
  pages = {1309--1313},
  abstract = {In this paper we present a geometrically-constrained time delay estimation method for sound source localization (gTDE). An algebraic analysis reveals that the method can deal with an arbitrary number of non-coplanar microphones. We derive a constrained non-linear optimization problem that can be solved using local convex programming. Unlike existing techniques, which consider pairwise TDE's, the proposed method optimally estimates a set of time delays that are consistent with the source's location. Extensive simulated experiments validate the method in the presence of noise and of reverberations.},
  eventtitle = {2012 {{Proceedings}} of the 20th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BAPRMCLP\\Alameda-Pineda_Horaud_2012_Geometrically-constrained robust time delay estimation using non-coplanar.pdf;C\:\\Users\\sauli\\Zotero\\storage\\8FIU68FQ\\6333857.html},
  keywords = {algebra,algebraic analysis,convex programming,Delay effects,delays,Estimation,geometrically-constrained robust time delay estimation,gTDE,local convex programming,microphone arrays,Microphones,noncoplanar microphone array,nonlinear optimization problem,Optimization,pairwise TDE,reverberation,Reverberation,reverberations,Signal to noise ratio,sound source localization}
}

@article{alameda-pinedaGeometricApproachSound2014,
  title = {A {{Geometric Approach}} to {{Sound Source Localization}} from {{Time}}-{{Delay Estimates}}},
  author = {Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2014-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  pages = {1082--1095},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2317989},
  url = {http://arxiv.org/abs/1311.1047},
  urldate = {2017-05-11},
  abstract = {This paper addresses the problem of sound-source localization from time-delay estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel geometric formulation is proposed, together with a thorough algebraic analysis and a global optimization solver. The proposed model is thoroughly described and evaluated. The geometric analysis, stemming from the direct acoustic propagation model, leads to necessary and sufficient conditions for a set of time delays to correspond to a unique position in the source space. Such sets of time delays are referred to as feasible sets. We formally prove that every feasible set corresponds to exactly one position in the source space, whose value can be recovered using a closed-form localization mapping. Therefore we seek for the optimal feasible set of time delays given, as input, the received microphone signals. This time delay estimation problem is naturally cast into a programming task, constrained by the feasibility conditions derived from the geometric analysis. A global branch-and-bound optimization technique is proposed to solve the problem at hand, hence estimating the best set of feasible time delays and, subsequently, localizing the sound source. Extensive experiments with both simulated and real data are reported; we compare our methodology to four state-of-the-art techniques. This comparison clearly shows that the proposed method combined with the branch-and-bound algorithm outperforms existing methods. These in-depth geometric understanding, practical algorithms, and encouraging results, open several opportunities for future work.},
  archivePrefix = {arXiv},
  eprint = {1311.1047},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3R2KI5EG\\Alameda-Pineda and Horaud - 2014 - A Geometric Approach to Sound Source Localization .pdf;C\:\\Users\\sauli\\Zotero\\storage\\SEZ9HQZM\\1311.html},
  keywords = {Computer Science - Sound},
  number = {6}
}

@article{Alameda-PinedaGeometricApproachSound2014,
  title = {A {{Geometric Approach}} to {{Sound Source Localization}} from {{Time}}-{{Delay Estimates}}},
  author = {Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2014-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  pages = {1082--1095},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2317989},
  abstract = {This paper addresses the problem of sound-source localization from time-delay estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel geometric formulation is proposed, together with a thorough algebraic analysis and a global optimization solver. The proposed model is thoroughly described and evaluated. The geometric analysis, stemming from the direct acoustic propagation model, leads to necessary and sufficient conditions for a set of time delays to correspond to a unique position in the source space. Such sets of time delays are referred to as feasible sets. We formally prove that every feasible set corresponds to exactly one position in the source space, whose value can be recovered using a closed-form localization mapping. Therefore we seek for the optimal feasible set of time delays given, as input, the received microphone signals. This time delay estimation problem is naturally cast into a programming task, constrained by the feasibility conditions derived from the geometric analysis. A global branch-and-bound optimization technique is proposed to solve the problem at hand, hence estimating the best set of feasible time delays and, subsequently, localizing the sound source. Extensive experiments with both simulated and real data are reported; we compare our methodology to four state-of-the-art techniques. This comparison clearly shows that the proposed method combined with the branch-and-bound algorithm outperforms existing methods. These in-depth geometric understanding, practical algorithms, and encouraging results, open several opportunities for future work.},
  archivePrefix = {arXiv},
  eprint = {1311.1047},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AH7RCTXU\\Alameda-Pineda and Horaud - 2014 - A Geometric Approach to Sound Source Localization .pdf;C\:\\Users\\sauli\\Zotero\\storage\\WHSX42YE\\1311.html},
  keywords = {Computer Science - Sound},
  number = {6}
}

@article{alameda-pinedaGeometricApproachSound2014a,
  title = {A {{Geometric Approach}} to {{Sound Source Localization}} from {{Time}}-{{Delay Estimates}}},
  author = {Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2014-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  pages = {1082--1095},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2317989},
  url = {http://arxiv.org/abs/1311.1047},
  urldate = {2018-04-24},
  abstract = {This paper addresses the problem of sound-source localization from time-delay estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel geometric formulation is proposed, together with a thorough algebraic analysis and a global optimization solver. The proposed model is thoroughly described and evaluated. The geometric analysis, stemming from the direct acoustic propagation model, leads to necessary and sufficient conditions for a set of time delays to correspond to a unique position in the source space. Such sets of time delays are referred to as feasible sets. We formally prove that every feasible set corresponds to exactly one position in the source space, whose value can be recovered using a closed-form localization mapping. Therefore we seek for the optimal feasible set of time delays given, as input, the received microphone signals. This time delay estimation problem is naturally cast into a programming task, constrained by the feasibility conditions derived from the geometric analysis. A global branch-and-bound optimization technique is proposed to solve the problem at hand, hence estimating the best set of feasible time delays and, subsequently, localizing the sound source. Extensive experiments with both simulated and real data are reported; we compare our methodology to four state-of-the-art techniques. This comparison clearly shows that the proposed method combined with the branch-and-bound algorithm outperforms existing methods. These in-depth geometric understanding, practical algorithms, and encouraging results, open several opportunities for future work.},
  archivePrefix = {arXiv},
  eprint = {1311.1047},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7Y3ILGZF\\Alameda-Pineda and Horaud - 2014 - A Geometric Approach to Sound Source Localization .pdf;C\:\\Users\\sauli\\Zotero\\storage\\K6TZZA5T\\1311.html},
  keywords = {Computer Science - Sound},
  number = {6}
}

@article{alameda-pinedaGeometricApproachSound2014b,
  title = {A {{Geometric Approach}} to {{Sound Source Localization}} from {{Time}}-{{Delay Estimates}}},
  author = {Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2014-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {22},
  pages = {1082--1095},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2317989},
  url = {http://arxiv.org/abs/1311.1047},
  urldate = {2019-09-18},
  abstract = {This paper addresses the problem of sound-source localization from time-delay estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel geometric formulation is proposed, together with a thorough algebraic analysis and a global optimization solver. The proposed model is thoroughly described and evaluated. The geometric analysis, stemming from the direct acoustic propagation model, leads to necessary and sufficient conditions for a set of time delays to correspond to a unique position in the source space. Such sets of time delays are referred to as feasible sets. We formally prove that every feasible set corresponds to exactly one position in the source space, whose value can be recovered using a closed-form localization mapping. Therefore we seek for the optimal feasible set of time delays given, as input, the received microphone signals. This time delay estimation problem is naturally cast into a programming task, constrained by the feasibility conditions derived from the geometric analysis. A global branch-and-bound optimization technique is proposed to solve the problem at hand, hence estimating the best set of feasible time delays and, subsequently, localizing the sound source. Extensive experiments with both simulated and real data are reported; we compare our methodology to four state-of-the-art techniques. This comparison clearly shows that the proposed method combined with the branch-and-bound algorithm outperforms existing methods. These in-depth geometric understanding, practical algorithms, and encouraging results, open several opportunities for future work.},
  archivePrefix = {arXiv},
  eprint = {1311.1047},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XQJS5SIM\\Alameda-Pineda_Horaud_2014_A Geometric Approach to Sound Source Localization from Time-Delay Estimates.pdf;C\:\\Users\\sauli\\Zotero\\storage\\TED6IUWL\\1311.html},
  keywords = {Computer Science - Sound},
  number = {6}
}

@inproceedings{alameda-pinedaGeometrySoundsourceLocalization2013,
  title = {The Geometry of Sound-Source Localization Using Non-Coplanar Microphone Arrays},
  booktitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}}},
  author = {Alameda-Pineda, X. and Horaud, R. and Mourrain, B.},
  date = {2013-10},
  pages = {1--4},
  doi = {10.1109/WASPAA.2013.6701849},
  abstract = {This paper addresses the task of sound-source localization from time delay estimates using arbitrarily shaped non-coplanar microphone arrays. We fully exploit the direct path propagation model and our contribution is threefold: we provide a necessary and sufficient condition for a set of time delays to correspond to a sound source position, a proof of the uniqueness of this position, and a localization mapping to retrieve it. The time delay estimation task is casted into a non-linear multivariate optimization problem constrained by necessary and sufficient conditions on time delays. Two global optimization techniques to estimate time delays and localize the sound source are investigated. We report an extensive set of experiments and comparisons with state-of-the-art methods on simulated and real data in the presence of noise and reverberations.},
  eventtitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PIHMJGE8\\Alameda-Pineda et al_2013_The geometry of sound-source localization using non-coplanar microphone arrays.pdf;C\:\\Users\\sauli\\Zotero\\storage\\UBPI4RN3\\6701849.html},
  keywords = {Acoustics,constrained multivariate non-linear optimization,Delay effects,Equations,Estimation,Microphone arrays,Optimization,Sound source localization,time delay estimate}
}

@article{alexandridisMultipleSoundSource2018,
  title = {Multiple {{Sound Source Location Estimation}} in {{Wireless Acoustic Sensor Networks Using DOA Estimates}}: {{The Data}}-{{Association Problem}}},
  author = {Alexandridis, Anastasios and Mouchtaris, Athanasios},
  date = {2018-02},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{342-356\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2017.2772831\}},
  abstract = {In this paper, we consider the data-association problem for the localization of multiple sound sources in a wireless acoustic sensor network, where each node is a microphone array, using direction of arrival (DOA) estimates. The data-association problem arises because the central node that receives the multiple DOA estimates from the nodes cannot know to which source they belong. Hence, the DOAs from the different nodes that correspond to the same source must be found in order to perform accurate localization. We present a method to identify the correct association of DOAs to the sources and thus accurately estimate their locations. Our method results in high association and localization accuracy in realistic scenarios with missed detections, reverberation, noise, and moving sources and outperforms other recently proposed methods. It also incorporates a bitrate reduction scheme in order to keep the amount of information that needs to be transmitted in the network at low levels without affecting performance.},
  affiliation = {Alexandridis, A (Reprint Author), Fdn Res & Technol Hellas, Inst Comp Sci, Iraklion 70013, Greece. Alexandridis, Anastasios; Mouchtaris, Athanasios, Fdn Res & Technol Hellas, Inst Comp Sci, Iraklion 70013, Greece. Alexandridis, Anastasios; Mouchtaris, Athanasios, Univ Crete, Dept Comp Sci, Iraklion 70013, Greece.},
  author-email = {analexan@ics.forth.gr mouchtar@ics.forth.gr},
  cited-references = {Aarabi P, 2003, EURASIP J APPL SIG P, V2003, P338, DOI 10.1155/S1110865703212014. Alexandre-Collier A, 2015, PALGRAVE STUD POLIT, P1. Alexandridis A, 2017, INT CONF ACOUST SPEE, P6140, DOI 10.1109/ICASSP.2017.7953336. Alexandridis A, 2015, EUR SIGNAL PR CONF, P1551, DOI 10.1109/EUSIPCO.2015.7362644. Argentieri Sylvain, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P2009, DOI 10.1109/IROS.2007.4399422. Bertrand A., 2011, P IEEE S COMM VEH TE, P1, DOI DOI 10.1109/SCVT.2011.6101302. Bishop AN, 2009, IEEE T AERO ELEC SYS, V45, P308, DOI 10.1109/TAES.2009.4805281. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. CARPANETO G, 1981, COMPUTING, V27, P179, DOI 10.1007/BF02243552. Cobos M, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/3956282. Cobos M, 2011, IEEE SIGNAL PROC LET, V18, P71, DOI 10.1109/LSP.2010.2091502. Compagnoni M, 2012, IEEE T AUDIO SPEECH, V20, P1964, DOI 10.1109/TASL.2012.2191958. Deb S, 1997, IEEE T AERO ELEC SYS, V33, P523, DOI 10.1109/7.575891. Do H., 2007, P IEEE WORKSH APPL S, P295. Dogancay K, 2006, IEEE T SIGNAL PROCES, V54, P59, DOI 10.1109/TSP.2005.861088. Dogancay K, 2005, SIGNAL PROCESS, V85, P1695, DOI 10.1016/j.sigpro.2005.03.007. Dogancay Kutluyil, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1123. Dogancay K, 2004, SIGNAL PROCESS, V84, P487, DOI 10.1016/j.sigpro.2003.11.014. Griffin A, 2015, SIGNAL PROCESS, V107, P54, DOI 10.1016/j.sigpro.2014.08.013. Griffin A, 2014, EUR SIGNAL PR CONF, P306. Griffin A, 2012, EUR SIGNAL PR CONF, P2303. Griffini A., 1913, Pavia. Kaplan LM, 2005, J FRANKLIN I, V342, P193, DOI 10.1016/j.jfranklin.2004.10.003. Kaplan LM, 2001, P SOC PHOTO-OPT INS, V4393, P40, DOI 10.1117/12.441279. KAPLAN LM, 2001, ACOUST SPEECH SIG PR, P3001. Karbasi Amin, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P778. Lehmann EA, 2010, IEEE T AUDIO SPEECH, V18, P1429, DOI 10.1109/TASL.2009.2035038. Li D., 2003, EURASIP J ADV SIG PR, V2003. Mennill DJ, 2012, METHODS ECOL EVOL, V3, P704, DOI 10.1111/j.2041-210X.2012.00209.x. NARDONE SC, 1984, IEEE T AUTOMAT CONTR, V29, P775, DOI 10.1109/TAC.1984.1103664. Nesta F, 2012, IEEE T AUDIO SPEECH, V20, P246, DOI 10.1109/TASL.2011.2160168. PATTIPATI KR, 1992, IEEE T AUTOMAT CONTR, V37, P198, DOI 10.1109/9.121621. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Popp RL, 2001, IEEE T AERO ELEC SYS, V37, P22, DOI 10.1109/7.913665. Reed J. D., 2008, P IEEE MIL COMM C NO, P1. Roweis S., 2003, P EUR, P1009. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. Schulz S., 2008, P DAFX 08, P241. Sheng XH, 2005, IEEE T SIGNAL PROCES, V53, P44, DOI 10.1109/TSP.2004.838930. Stansfield R.G., 1947, Journal of the Institution of Electrical Engineers. IIIA. Radiocommunication, V94, P762. Swartling M, 2011, SIGNAL PROCESS, V91, P1781, DOI 10.1016/j.sigpro.2011.02.002. Taseska M, 2014, IEEE-ACM T AUDIO SPE, V22, P1195, DOI 10.1109/TASLP.2014.2327294. Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1. Wang H., 2005, SPIE P, V5910, P12. Wang Z, 2012, IEEE T SIGNAL PROCES, V60, P6166, DOI 10.1109/TSP.2012.2218809.},
  da = {2018-10-18},
  doc-delivery-number = {FQ3ZR},
  funding-acknowledgement = {European Union's Horizon research and innovation programme under the Marie Sklodowska-Curie Grant [644283]},
  funding-text = {This work was supported by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant 644283, Project LISTEN.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Data-association,direction of arrival estimates,localization,microphone arrays,wireless acoustic sensor networks)},
  keywords-plus = {ONLY TARGET LOCALIZATION; DISTRIBUTED MICROPHONE ARRAYS; D ASSIGNMENT ALGORITHM},
  langid = {english},
  number = {2},
  number-of-cited-references = {45},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000418297800011},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{allenImageMethodEfficiently1976,
  title = {Image Method for Efficiently Simulating Small‐room Acoustics},
  author = {Allen, Jont B. and Berkley, D. A.},
  date = {1976},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {65},
  pages = {943--950},
  doi = {10.1121/1.382599},
  abstract = {Image methods are commonly used for the analysis of the acoustic properties of enclosures. In this paper we discuss the theoretical and practical use of image techniques for simulating, on a digital computer, the impulse response between two points in a small rectangular room. The resulting impulse response, when convolved with any desired input signal, such as speech, simulates room reverberation of the input signal. This technique is useful in signal processing or psychoacoustic studies. The entire process is carried out on a digital computer so that a wide range of room parameters can be studied with accurate control over the experimental conditions. A fortran implementation of this model has been included.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9SQJQ4SI\\Allen_Berkley_1976_Image method for efficiently simulating small‐room acoustics.pdf},
  keywords = {Acoustic cryptanalysis,Computer,Convolution,Psychoacoustics,Signal processing,Simulation},
  number = {4}
}

@online{AmbientIntelligenceNext,
  title = {Ambient {{Intelligence}}—the {{Next Step}} for {{Artificial Intelligence}} - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/abstract/document/4475854},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MT8QYW4K\\4475854.html}
}

@online{AmbientIntelligenceWerner,
  title = {Ambient {{Intelligence}} - {{Werner Weber}}, {{Jan Rabaey}} - {{Google}} Knygos},
  url = {https://books.google.lt/books?hl=lt&lr=&id=6eYnS8wnxkIC&oi=fnd&pg=PA1&dq=ambient+intelligence+system&ots=RRRK93OsML&sig=jlGd62qciwNj6QlkpXyhcbIJ_e4&redir_esc=y#v=onepage&q=ambient%20intelligence%20system&f=false},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9FT8JDJJ\\books.html}
}

@patent{anguelovEstimationPanoramicCamera2014,
  title = {Estimation of Panoramic Camera Orientation Relative to a Vehicle Coordinate Frame},
  author = {ANGUELOV, Dragomir D. and Filip, Daniel},
  date = {2014-04-15},
  url = {https://patents.google.com/patent/US8698875B2/en},
  urldate = {2020-02-27},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8SNU6MUB\\ANGUELOV and Filip - 2014 - Estimation of panoramic camera orientation relativ.pdf},
  holder = {{Google LLC}},
  keywords = {correction parameter,images,pose,represents,vehicle},
  langid = {english},
  number = {8698875B2},
  type = {patentus}
}

@inproceedings{anReflectionAwareSoundSource2018,
  title = {Reflection-{{Aware Sound Source Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {An, I. and Son, M. and Manocha, D. and Yoon, S.},
  date = {2018-05},
  pages = {66--73},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8461268},
  abstract = {We present a novel, reflection-aware method for 3D sound localization in indoor environments. Unlike prior approaches, which are mainly based on continuous sound signals from a stationary source, our formulation is designed to localize the position instantaneously from signals within a single frame. We consider direct sound and indirect sound signals that reach the microphones after reflecting off surfaces such as ceilings or walls. We then generate and trace direct and reflected acoustic paths using inverse acoustic ray tracing and utilize these paths with Monte Carlo localization to estimate a 3D sound source position. We have implemented our method on a robot with a cube-shaped microphone array and tested it against different settings with continuous and intermittent sound signals with a stationary or a mobile source. Across different settings, our approach can localize the sound with an average distance error of 0.8 m tested in a room of 7 m by 7 m area with 3 m height, including a mobile and non-line-of-sight sound source. We also reveal that the modeling of indirect rays increases the localization accuracy by 40\% compared to only using direct acoustic rays.},
  keywords = {3D sound localization,3D sound source position,acoustic signal processing,Acoustics,array signal processing,continuous sound signals,cube-shaped microphone array,direct acoustic paths,direct acoustic rays,direct sound signal,indirect sound signals,indoor environments,Indoor environments,intermittent sound signals,inverse acoustic ray tracing,localization accuracy,microphone arrays,Microphone arrays,mobile robots,mobile sound source,mobile source,Monte Carlo localization,Monte Carlo methods,nonline-of-sight sound source,ray tracing,Ray tracing,reflected acoustic paths,reflection-aware method,reflection-aware sound source localization,Robots,single frame,size 0.8 m,size 3.0 m,size 7.0 m,stationary source,Three-dimensional displays,time 3.0 d}
}

@inproceedings{antonelloJointSourceLocalization2018,
  title = {Joint {{Source Localization}} and {{Dereverberation}} by {{Sound Field Interpolation Using Sparse Regularization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Antonello, N. and Sena, E. De and Moonen, M. and Naylor, P. A. and van Waterschoot, T.},
  date = {2018-04},
  pages = {6892--6896},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462451},
  abstract = {In this paper, source localization and dereverberation are formulated jointly as an inverse problem. The inverse problem consists in the interpolation of the sound field measured by a set of microphones by matching the recorded sound pressure with that of a particular acoustic model. This model is based on a collection of equivalent sources creating either spherical or plane waves. In order to achieve meaningful results, spatial, spatio-temporal and spatio-spectral sparsity can be promoted in the signals originating from the equivalent sources. The inverse problem consists of a large-scale optimization problem that is solved using a first order matrix-free optimization algorithm. It is shown that once the equivalent source signals capable of effectively interpolating the sound field are obtained, they can be readily used to localize a speech sound source in terms of Direction of Arrival (DOA) and to perform dereverberation in a highly reverberant environment.},
  keywords = {Acoustic measurements,Acoustics,dereverberation,Dereverberation,direction of arrival method,Direction-of-arrival estimation,DOA method,equivalent source signal collection,first order matrix-free optimization algorithm,interpolation,Interpolation,inverse problem,inverse problems,Inverse problems,Large-scale optimization,large-scale optimization problem,microphones,Microphones,optimisation,Optimization,reverberation,sound field interpolation,sound field measurement,sound pressure recording,Source localization,sparse regularization,Sparse sensing,speech sound source localization},
  options = {useprefix=true}
}

@online{AppleStockPrice,
  title = {Apple Stock Price History - „{{Google}}“ Paieška},
  url = {https://www.google.com/search?q=apple+stock+price+history&oq=apple+stock+price+history&aqs=chrome..69i57j0l7.10865j1j7&sourceid=chrome&ie=UTF-8},
  urldate = {2020-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IXQNGFYH\\search.html}
}

@online{ApplicationVectorSensors,
  title = {(7) {{Application}} of {{Vector Sensors}} to {{Acoustic Surveillance}} of a {{Public Interior Space}} | {{Kuba Łopatka}} | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/269510699_Application_of_Vector_Sensors_to_Acoustic_Surveillance_of_a_Public_Interior_Space},
  urldate = {2019-07-29},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KYS9WNHD\\269510699_Application_of_Vector_Sensors_to_Acoustic_Surveillance_of_a_Public_Interior_Space.html}
}

@online{ApproachModellingDiffuse,
  title = {Approach in Modelling Diffuse Sound Field Excitation},
  url = {https://www.comsol.com/forum/thread/127753/approach-in-modelling-diffuse-sound-field-excitation?last=2016-10-18T11:00:29Z},
  urldate = {2017-12-08}
}

@article{arakiNOVELBLINDSOURCE,
  title = {A {{NOVEL BLIND SOURCE SEPARATION METHOD WITH OBSERVATION VECTOR CLUSTERING}}},
  author = {Araki, Shoko and Sawada, Hiroshi and Mukai, Ryo and Makino, Shoji},
  pages = {4},
  abstract = {We propose a new method for separating sparse signals from their mixtures. Separation is achieved by clustering the normalized observation vectors and extracting each cluster as each separated signal. We show the practical result of speech separation with non-linear sensor arrangements in both a determined and an underdetermined scenarios. We also consider the musical noise problem and show the listening test results.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PHSCSJHZ\\Araki et al. - A NOVEL BLIND SOURCE SEPARATION METHOD WITH OBSERV.pdf},
  langid = {english}
}

@article{argentieri_survey_2015,
  title = {A Survey on Sound Source Localization in Robotics: {{From}} Binaural to Array Processing Methods},
  shorttitle = {A Survey on Sound Source Localization in Robotics},
  author = {Argentieri, S. and Danès, P. and Souères, P.},
  date = {2015-11},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {34},
  pages = {87--112},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2015.03.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230815000236},
  urldate = {2017-01-09},
  abstract = {This paper attempts to provide a state-of-the-art of sound source localization in robotics. Noticeably, this context raises original constraints—e.g. embeddability, real time, broadband environments, noise and reverberation—which are seldom simultaneously taken into account in acoustics or signal processing. A comprehensive review is proposed of recent robotics achievements, be they binaural or rooted in array processing techniques. The connections are highlighted with the underlying theory as well as with elements of physiology and neurology of human hearing.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VSBFJJZX\\Argentieri et al. - 2015 - A survey on sound source localization in robotics.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SX55UJB4\\S0885230815000236.html},
  keywords = {Array processing,Binaural audition,Robot audition,Source localization},
  number = {1}
}

@inproceedings{argentieriPrototypingFilterSumBeamformers2005,
  title = {Prototyping {{Filter}}-{{Sum Beamformers}} for {{Sound Source Localization}} in {{Mobile Robotics}}},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Argentieri, S. and Danes, P. and Soueres, P.},
  date = {2005-04},
  pages = {3551--3556},
  doi = {10.1109/ROBOT.2005.1570660},
  abstract = {The work presented in this paper comes as a part of a project which aims at developing an auditory system for a mobile robot. It presents a sound source localization strategy which enables the sensing of signals within a direction of arrival and frequency domain of interest while rejecting other data. A rapid prototyping method is proposed to design filter-sum beamformers on the basis of convex optimization. This method is well-suited to robotics applications as it copes with real-time constraints and allows the localization of broadband signals such as human voice. Numerous simulation results are used to illustrate the reasoning.},
  eventtitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ITRW828Z\\Argentieri et al. - 2005 - Prototyping Filter-Sum Beamformers for Sound Sourc.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5VDEDKEY\\1570660.html},
  keywords = {Acoustic sensors,Array signal processing,Auditory system,beamforming,Frequency domain analysis,Microphone arrays,mobile robotics,Mobile robots,Navigation,Prototypes,Robot sensing systems,Sensor arrays,sound source localization}
}

@inproceedings{argentieriPrototypingFilterSumBeamformers2005a,
  title = {Prototyping {{Filter}}-{{Sum Beamformers}} for {{Sound Source Localization}} in {{Mobile Robotics}}},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Argentieri, S. and Danes, P. and Soueres, P.},
  date = {2005-04},
  pages = {3551--3556},
  doi = {10.1109/ROBOT.2005.1570660},
  abstract = {The work presented in this paper comes as a part of a project which aims at developing an auditory system for a mobile robot. It presents a sound source localization strategy which enables the sensing of signals within a direction of arrival and frequency domain of interest while rejecting other data. A rapid prototyping method is proposed to design filter-sum beamformers on the basis of convex optimization. This method is well-suited to robotics applications as it copes with real-time constraints and allows the localization of broadband signals such as human voice. Numerous simulation results are used to illustrate the reasoning.},
  eventtitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MFHGHT4M\\1570660.html},
  keywords = {Acoustic sensors,Array signal processing,Auditory system,beamforming,Frequency domain analysis,Microphone arrays,mobile robotics,Mobile robots,Navigation,Prototypes,Robot sensing systems,Sensor arrays,sound source localization}
}

@online{Art3A101155,
  title = {Art\%{{3A10}}.1155\%{{2FS1110865703212075}}.Pdf},
  url = {http://download.springer.com/static/pdf/331/art%253A10.1155%252FS1110865703212075.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1155%2FS1110865703212075&token2=exp=1494588628~acl=%2Fstatic%2Fpdf%2F331%2Fart%25253A10.1155%25252FS1110865703212075.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1155%252FS1110865703212075*~hmac=528a6638d81df7b47d6ee7c4fae9eac805581f3afd4fa03429322a253f48543c},
  urldate = {2017-05-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9CBTTCWZ\\art%3A10.1155%2FS1110865703212075.html}
}

@article{art3A101155,
  title = {Art\%{{3A10}}.1155\%{{2FS1110865703212075}}.{{Pdf}}},
  url = {http://download.springer.com/static/pdf/331/art%253A10.1155%252FS1110865703212075.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1155%2FS1110865703212075&token2=exp=1494588628~acl=%2Fstatic%2Fpdf%2F331%2Fart%25253A10.1155%25252FS1110865703212075.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1155%252FS1110865703212075*~hmac=528a6638d81df7b47d6ee7c4fae9eac805581f3afd4fa03429322a253f48543c},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2IC3FBUT\\art%3A10.1155%2FS1110865703212075.html}
}

@inproceedings{astapovOptimizedAcousticLocalization2013,
  title = {Optimized {{Acoustic Localization}} with {{SRP}}-{{PHAT}} for {{Monitoring}} in {{Distributed Sensor Networks}}},
  author = {Astapov, Sergei and Berdnikova, Julia and Preden, Jürgo-Sören},
  date = {2013},
  doi = {10.2478/eletel-2013-0047},
  abstract = {Acoustic localization by means of sensor arrays has a variety of applications, from conference telephony to environment monitoring. Many of these tasks are appealing for implementation on embedded systems, however large dataflows and computational complexity of multi-channel signal processing impede the development of such systems. This paper proposes a method of acoustic localization targeted for distributed systems, such as Wireless Sensor Networks (WSN). The method builds on an optimized localization algorithm of Steered Response Power with Phase Transform (SRP-PHAT) and simplifies it further by reducing the initial search region, in which the sound source is contained. The sensor array is partitioned into sub-blocks, which may be implemented as independent nodes of WSN. For the region reduction two approaches are handled. One is based on Direction of Arrival estimation and the other - on multilateration. Both approaches are tested on real signals for speaker localization and industrial machinery monitoring applications. Experiment results indicate the method’s potency in both these tasks.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XKER6KQN\\Astapov et al. - 2013 - Optimized Acoustic Localization with SRP-PHAT for .pdf}
}

@inproceedings{astapovSimplifiedAcousticLocalization2013,
  title = {Simplified Acoustic Localization by Linear Arrays for Wireless Sensor Networks},
  booktitle = {2013 18th {{International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  author = {Astapov, Sergei and Preden, Jürgo-Sören and Berdnikova, Julia},
  date = {2013-07},
  pages = {1--6},
  issn = {1546-1874},
  doi = {10.1109/ICDSP.2013.6622741},
  abstract = {The application of wireless sensor networks (WSN) for the task of acoustic localization provides great opportunities for distributed cooperative tracking of sound sources in large areas. However WSNs are significantly more limited in terms of computational resources and power than typical computer systems. Therefore the methods applied for acoustic localization in WSN must be optimized for minimal resource consumption. This paper builds on the advances of Steered Response Power with Phase Transform (SRP-PHAT) optimization and proposes a further simplification in terms of additional minimization of the initial search volume. By using several linear microphone arrays we are able to estimate the initial region of sound source and reduce the number of computations by at least one order of magnitude. The results of several experiments on real signals confirm the achieved improvements.},
  eventtitle = {2013 18th {{International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LKEJK2KR\\Astapov et al. - 2013 - Simplified acoustic localization by linear arrays .pdf;C\:\\Users\\sauli\\Zotero\\storage\\MHK523G4\\6622741.html},
  keywords = {*****,acoustic arrays,acoustic localization,acoustic signal processing,Acoustics,computational resources,direction of arrival,direction-of-arrival estimation,Direction-of-arrival estimation,distributed cooperative tracking,Estimation,initial search volume,linear microphone arrays,microphone arrays,Microphone arrays,minimal resource consumption,MUST READ,Nickel,optimisation,sound sources,SRP-PHAT,SRP-PHAT optimization,steered response power with phase transform optimization,time-of-arrival estimation,wireless sensor networks,Wireless sensor networks,WSN}
}

@article{athanasopoulos_robust_2015,
  title = {Robust Speaker Localization for Real-World Robots},
  author = {Athanasopoulos, Georgios and Verhelst, Werner and Sahli, Hichem},
  date = {2015-11},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {34},
  pages = {129--153},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2015.03.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230815000388},
  urldate = {2017-01-10},
  abstract = {Autonomous human–robot interaction ultimately requires an artificial audition module that allows the robot to process and interpret a combination of verbal and non-verbal auditory inputs. A key component of such a module is the acoustic localization. The acoustic localization not only enables the robot to simultaneously localize multiple persons and auditory events of interest in the environment, but also provides input to auditory tasks such as speech enhancement and speech recognition. The use of microphone arrays in robots is an efficient and commonly applied approach to the localization problem. In this paper, moving away from simulated environments, we look at the acoustic localization under real-world conditions and limitations. Our approach proposes a series of enhancements, taking into account the imperfect frequency response of the array microphones and addressing the influence of the robot's shape and surface material. Motivated by the importance of the signal's phase information, we introduce a novel pre-processing step for enhancing the acoustic localization. Results show that the proposed approach improves the localization performance in joint noisy and reverberant conditions and allows a humanoid robot to locate multiple speakers in a real-world environment.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SD8PAM7F\\Athanasopoulos et al. - 2015 - Robust speaker localization for real-world robots.pdf;C\:\\Users\\sauli\\Zotero\\storage\\CTRUHCKT\\S0885230815000388.html},
  keywords = {Acoustic localization,microphone arrays,Phase spectrum enhancement,Steered response power,Time delay estimation},
  number = {1}
}

@online{AutomaticIdentificationSound,
  title = {Automatic Identification of Sound Source Position Employing Neural Networks and Rough Sets},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/222529446_Automatic_identification_of_sound_source_position_employing_neural_networks_and_rough_sets},
  urldate = {2017-05-03},
  abstract = {Automatic identification of sound source position employing neural networks and rough sets on ResearchGate, the professional network for scientists.}
}

@article{Automaticidentificationsound,
  title = {Automatic {{Identification}} of {{Sound Source Position Employing Neural Networks}} and {{Rough Sets}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/222529446_Automatic_identification_of_sound_source_position_employing_neural_networks_and_rough_sets},
  abstract = {Automatic identification of sound source position employing neural networks and rough sets on ResearchGate, the professional network for scientists.}
}

@online{AutomaticIdentificationSounda,
  title = {Automatic Identification of Sound Source Position Employing Neural Networks and Rough Sets},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/222529446_Automatic_identification_of_sound_source_position_employing_neural_networks_and_rough_sets},
  urldate = {2017-05-03},
  abstract = {Automatic identification of sound source position employing neural networks and rough sets on ResearchGate, the professional network for scientists.}
}

@article{Automaticidentificationsounda,
  title = {Automatic {{Identification}} of {{Sound Source Position Employing Neural Networks}} and {{Rough Sets}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/222529446_Automatic_identification_of_sound_source_position_employing_neural_networks_and_rough_sets},
  abstract = {Automatic identification of sound source position employing neural networks and rough sets on ResearchGate, the professional network for scientists.}
}

@article{awad-allaTwostageApproachPassive2020,
  title = {A Two-Stage Approach for Passive Sound Source Localization Based on the {{SRP}}-{{PHAT}} Algorithm},
  author = {Awad-Alla, M. A. and Hamdy, Ahmed and Tolbah, Farid A. and Shahin, Moatasem A. and Abdelaziz, M. A.},
  year = {2020/ed},
  journaltitle = {APSIPA Transactions on Signal and Information Processing},
  volume = {9},
  publisher = {{Cambridge University Press}},
  issn = {2048-7703},
  doi = {10.1017/ATSIP.2020.6},
  url = {https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/twostage-approach-for-passive-sound-source-localization-based-on-the-srpphat-algorithm/4EC64A19B1360ACF176373194BB8D16A},
  urldate = {2020-05-14},
  abstract = {This paper presents a different approach to tackle the Sound Source Localization (SSL) problem apply on a compact microphone array that can be mounted on top of a small moving robot in an indoor environment. Sound source localization approaches can be categorized into the three main categories; Time Difference of Arrival (TDOA), high-resolution subspace-based methods, and steered beamformer-based methods. Each method has its limitations according to the search or application requirements. Steered beamformer-based method will be used in this paper because it has proven to be robust to ambient noise and reverberation to a certain extent. The most successful and used algorithm of this method is the SRP-PHAT algorithm. The main limitation of SRP-PHAT algorithm is the computational burden resulting from the search process, this limitation comes from searching among all possible candidate locations in the searching space for the location that maximizes a certain function. The aim of this paper is to develop a computationally viable approach to find the coordinate location of a sound source with acceptable accuracy. The proposed approach comprises two stages: the first stage contracts the search space by estimating the Direction of Arrival (DoA) vector from the time difference of arrival with an addition of reasonable error coefficient around the vector to make sure that the sound source locates inside the estimated region, the second stage is to apply the SRP-PHAT algorithm to search only in this contracted region for the source location. The AV16.3 corpus was used to evaluate the proposed approach, extensive experiments have been carried out to verify the reliability of the approach. The results showed that the proposed approach was successful in obtaining good results compared to the conventional SRP-PHAT algorithm.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GTYYXZSZ\\Awad-Alla et al. - 2020 - A two-stage approach for passive sound source loca.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5MT3NSSQ\\4EC64A19B1360ACF176373194BB8D16A.html},
  keywords = {Circular microphone array,Passive acoustic localization,Region contraction,Sound source localization,SRP-PHAT},
  langid = {english}
}

@article{bakerDesigningNeuralNetwork2016,
  title = {Designing {{Neural Network Architectures}} Using {{Reinforcement Learning}}},
  author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  date = {2016},
  journaltitle = {arXiv preprint arXiv:1611.02167},
  url = {https://arxiv.org/abs/1611.02167},
  urldate = {2017-03-27},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CJC7JKQH\\pdf.pdf}
}

@article{BakerDesigningNeuralNetwork2016,
  title = {Designing {{Neural Network Architectures Using Reinforcement Learning}}},
  author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  date = {2016},
  journaltitle = {arXiv preprint arXiv:1611.02167},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZCWJYYR6\\pdf.pdf}
}

@article{bancroftAlgebraicSolutionGPS1985,
  title = {An {{Algebraic Solution}} of the {{GPS Equations}}},
  author = {Bancroft, S.},
  date = {1985-01},
  journaltitle = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {AES-21},
  pages = {56--59},
  doi = {10.1109/TAES.1985.310538},
  abstract = {The global positioning system (GPS) equations are usually solved with an application of Newton's method or a variant thereof: Xn+1 = xn + H-1(t - f(xn)). (1) Here x is a vector comprising the user position coordinates together with clock offset, t is a vector of tour pseudorange measurements, and H is a measurement matrix of partial derivatives H = fx· In fact the first fix of a Kalman filter provides a solution of this type. If more than four pseudoranges are available for extended batch processing, H-1 may be replaced by a generalized inverse (HTWH)-1HTW, where W is a positive definite weighting matrix (usually taken to be the inverse of the measurement covariance matrix). This paper introduces a new method of solution that is algebraic and noniterative in nature, computationally efficient and numerically stable, admits extended batch processing, improves accuracy in bad geometric dilution of precision (GDOP) situations, and allows a "cold start" in deep space applications.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DTQ7ZW7X\\Bancroft_1985_An Algebraic Solution of the GPS Equations.pdf;C\:\\Users\\sauli\\Zotero\\storage\\E6EH4BPP\\4104017.html},
  keywords = {Clocks,Coordinate measuring machines,Covariance matrix,Earth,Global Positioning System,Newton method,Nonlinear equations,Position measurement,Satellite broadcasting,Symmetric matrices},
  number = {1}
}

@article{barkerPASCALCHiMESpeech2013,
  title = {The {{PASCAL CHiME}} Speech Separation and Recognition Challenge},
  author = {Barker, Jon and Vincent, Emmanuel and Ma, Ning and Christensen, Heidi and Green, Phil},
  date = {2013-05},
  journaltitle = {Computer Speech \& Language},
  volume = {27},
  pages = {621--633},
  issn = {08852308},
  doi = {10.1016/j.csl.2012.10.004},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0885230812000861},
  urldate = {2018-07-18},
  abstract = {Distant microphone speech recognition systems that operate with humanlike robustness remain a distant goal. The key difficulty is that operating in everyday listening conditions entails processing a speech signal that is reverberantly mixed into a noise background composed of multiple competing sound sources. This paper describes a recent speech recognition evaluation that was designed to bring together researchers from multiple communities in order to foster novel approaches to this problem. The task was to identify keywords from sentences reverberantly mixed into audio backgrounds binaurally-recorded in a busy domestic environment. The challenge was designed to model the essential difficulties of multisource environment problem while remaining on a scale that would make it accessible to a wide audience. Compared to previous ASR evaluation a particular novelty of the task is that the utterances to be recognised were provided in a continuous audio background rather than as pre-segmented utterances thus allowing a range of background modelling techniques to be employed. The challenge attracted thirteen submissions. This paper describes the challenge problem, provides an overview of the systems that were entered and provides a comparison alongside both a baseline recognition system and human performance. The paper discusses insights gained from the challenge and lessons learnt for the design of future such evaluations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N35ZRYYH\\Barker et al. - 2013 - The PASCAL CHiME speech separation and recognition.pdf},
  langid = {english},
  number = {3}
}

@article{basiriLocalizationEmergencyAcoustic2018,
  title = {Localization of Emergency Acoustic Sources by Micro Aerial Vehicles},
  author = {Basiri, Meysam and Schill, Felix and Lima, Pedro U. and Floreano, Dario},
  date = {2018-03},
  journaltitle = {JOURNAL OF FIELD ROBOTICS},
  volume = {35},
  pages = {\{187-201\}},
  publisher = {WILEY},
  location = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  issn = {1556-4959},
  doi = {\{10.1002/rob.21733\}},
  abstract = {For micro aerial vehicles (MAVs) involved in search and rescue missions, the ability to locate the source of a distress sound signal is significantly important and allows fast localization of victims and rescuers during nighttime, through foliage and in dust, fog, and smoke. Most emergency sound sources, such as safety whistles and personal alarms, generate a narrowband signal that is difficult to localize by human listeners or with the common localization methods suitable for broadband sounds. In this paper, we present three methods for MAV-based emergency sound localization system. The first method involves designing a new emergency source for immediate localization by the MAV using a common localization method. The other two novel methods allow localizing the currently available emergency sources, or other narrowband sounds in general, that are difficult to localize due to the periodicity in the sequence of sound samples. The second method exploits the Doppler shift in the sound frequency, caused due to the motion of the MAV and the dynamics of the MAV to assist with the localization. The third method involves active control of the robot's attitude and fusing acoustic and attitude measurements for achieving accurate and robust estimates. We evaluate our methods in real-world experiments with real flying robots.},
  affiliation = {Basiri, M (Reprint Author), Ecole Polytech Fed Lausanne, Lab Intelligent Syst, CH-1015 Lausanne, Switzerland. Basiri, Meysam; Floreano, Dario, Ecole Polytech Fed Lausanne, Lab Intelligent Syst, CH-1015 Lausanne, Switzerland. Schill, Felix, Ecole Polytech Fed Lausanne, Lausanne, Switzerland. Lima, Pedro U., Inst Super Tecn, Inst Syst & Robot, Lisbon, Portugal.},
  author-email = {Meysam.Basiri@gmail.com},
  cited-references = {Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374. Asoh H., 2004, Seventh International Conference on Information Fusion, P805. Basiri M, 2013, P ROB SCI SYST, V9. Basiri M, 2016, IEEE ROBOT AUTOM LET, V1, P820, DOI 10.1109/LRA.2016.2527833. Basiri M, 2014, IEEE INT CONF ROBOT, P4729, DOI 10.1109/ICRA.2014.6907551. Basiri M, 2012, IEEE INT C INT ROBOT, P4737, DOI 10.1109/IROS.2012.6385608. Brutti Alessio, 2008, 2008 Hands-Free Speech Communication and Microphone Arrays (HSCMA `08), P69, DOI 10.1109/HSCMA.2008.4538690. Buchner H, 2005, INT CONF ACOUST SPEE, P97. desBree HE, 2010, P DAMA C. DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157. Doucet A., 2001, SEQUENTIAL MONTE CAR. Ferguson BG, 1999, J ACOUST SOC AM, V106, P255, DOI 10.1121/1.427054. Fruehauf F, 2009, IEEE T GEOSCI REMOTE, V47, P2240, DOI 10.1109/TGRS.2009.2012717. Gilks WR, 1996, MARKOV CHAIN MONTE C. Goodrich MA, 2008, J FIELD ROBOT, V25, P89, DOI 10.1002/rob.20226. Hauert S, 2010, INT C ROB AUT WORKSH. Hollinger GA, 2012, AUTON ROBOT, V32, P1, DOI 10.1007/s10514-011-9239-y. Jwu-Sheng Hu, 2011, 8th Asian Control Conference (ASCC 2011), P299. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Lawson Charles L, 1995, CLASSICS APPL MATH, V15. Liu ZL, 2014, IEEE T MOBILE COMPUT, V13, P287, DOI 10.1109/TMC.2012.238. Matsusaka Y, 1999, EUROSPEECH, P1723. Nakadai K, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P832. Okuno H, 2002, DEV APPL ARTIFICIAL, P140. Pack DJ, 2009, IEEE T SYST MAN CY B, V39, P959, DOI 10.1109/TSMCB.2008.2010865. Populin LC, 2006, J NEUROSCI, V26, P9820, DOI 10.1523/JNEUROSCI.3061-06.2006. Rudol P., 2008, P IEEE AER C, P1, DOI DOI 10.1109/AERO.2008.4526558. Ruffier F, 2011, IEEE SENSOR, P970. Scerri P., 2007, AIAA INF AER 2007 C, P2858. Schweizer J, 2003, COLD REG SCI TECHNOL, V37, P429, DOI 10.1016/S0165-232X(03)00082-X. Silvagni M, 2017, GEOMAT NAT HAZ RISK, V8, P18, DOI 10.1080/19475705.2016.1238852. Silverman B. W., 1998, DENSITY ESTIMATION S. STERN RM, 1988, J ACOUST SOC AM, V84, P156, DOI 10.1121/1.396982. Sundqvist J, 2016, 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1448. Valin JM, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1228. Ward DB, 2003, IEEE T SPEECH AUDI P, V11, P826, DOI 10.1109/TSA.2003.818112.},
  da = {2018-10-18},
  doc-delivery-number = {FV0PT},
  eissn = {1556-4967},
  funding-acknowledgement = {Fundacao para a Ciencia e a Tecnologia (FCT) [SFRH/BD/51070/2010]; Swiss National Centre of Competence in Research (NCCR) Robotics; FCT ISR/LARSYS [PEst-OE/EEI/LA0009/2013]; Laboratory of Intelligent Systems (LIS/EPFL)},
  funding-text = {This work was supported by a doctoral grant from Fundacao para a Ciencia e a Tecnologia (FCT) (SFRH/BD/51070/2010), the Swiss National Centre of Competence in Research (NCCR) Robotics, Laboratory of Intelligent Systems (LIS/EPFL), and FCT ISR/LARSYS strategic funding [PEst-OE/EEI/LA0009/2013].},
  journal-iso = {J. Field Robot.},
  keywords = {(aerial robotics,emergency response)},
  keywords-plus = {TIME-DELAY; SOUND; TRACKING},
  langid = {english},
  number = {2},
  number-of-cited-references = {36},
  research-areas = {Robotics},
  times-cited = {2},
  type = {Article},
  unique-id = {ISI:000424259400002},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Robotics}
}

@article{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-06-04},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2019-10-02},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archivePrefix = {arXiv},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MS4XEI95\\Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\LKHVF2YB\\1806.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{baxendaleAudioLocalizationRobots2018,
  title = {Audio {{Localization}} for {{Robots Using Parallel Cerebellar Models}}},
  author = {Baxendale, Mark D. and Pearson, Martin J. and Nibouche, Mokhtar and Secco, Emanuele Lindo and Pipe, Anthony G.},
  date = {2018-10},
  journaltitle = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  volume = {3},
  pages = {\{3185-3192\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2377-3766},
  doi = {\{10.1109/LRA.2018.2850447\}},
  abstract = {A robot audio localization system is presented that combines the outputs of multiple adaptive filter models of the Cerebellum to calibrate a robot's audio map for various acoustic environments. The system is inspired by the MOdular Selection for Identification and Control (MOSAIC) framework. This study extends our previous work that used multiple cerebellar models to determine the acoustic environment in which a robot is operating. Here, the system selects a set of models and combines their outputs in proportion to the likelihood that each is responsible for calibrating the audio map as a robot moves between different acoustic environments or contexts. The system was able to select an appropriate set of models, achieving a performance better than that of a single model trained in all contexts, including novel contexts, as well as a baseline generalized cross correlation with phase transform sound source localization algorithm. The main contribution of this letter is the combination of multiple calibrators to allow a robot operating in the field to adapt to a range of different acoustic environments. The best performances were observed where the presence of a Responsibility Predictor was simulated.},
  affiliation = {Baxendale, MD (Reprint Author), Univ West England, Bristol BS16 1QY, Avon, England. Baxendale, MD (Reprint Author), Liverpool Hope Univ, Liverpool L16 9JD, Merseyside, England. Baxendale, Mark D.; Nibouche, Mokhtar, Univ West England, Bristol BS16 1QY, Avon, England. Baxendale, Mark D.; Secco, Emanuele Lindo, Liverpool Hope Univ, Liverpool L16 9JD, Merseyside, England. Pearson, Martin J.; Pipe, Anthony G., Bristol Robot Lab, Bristol BS16 1QY, Avon, England.},
  author-email = {mark2.baxendale@live.uwe.ac.uk martin.pearson@brl.ac.uk mokhtar.nibouche@uwe.ac.uk seccoe@hope.ac.uk tony.pipe@brl.ac.uk},
  cited-references = {Argentieri S, 2015, COMPUT SPEECH LANG, V34, P87, DOI 10.1016/j.csl.2015.03.003. Argentieri S., 2013, TECHNOLOGY BINAURAL, P225. Assaf T, 2016, IEEE INT CONF ROBOT, P967, DOI 10.1109/ICRA.2016.7487228. Badali A, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2033, DOI 10.1109/IROS.2009.5354308. Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181. Baxendale M. D., 2017, Towards Autonomous Robotic Systems. 18th Annual Conference, TAROS 2017. Proceedings: LNAI 10454, P66, DOI 10.1007/978-3-319-64107-2_6. Blauert J., 1997, SPATIAL HEARING PSYC. Chan VYS, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00021. Dean P, 2010, NAT REV NEUROSCI, V11, P30, DOI 10.1038/nrn2756. FUJITA M, 1982, BIOL CYBERN, V45, P195, DOI 10.1007/BF00336192. Haruno M, 2001, NEURAL COMPUT, V13, P2201, DOI 10.1162/089976601750541778. Huo J, 2009, NEURAL NETWORKS, V22, P913, DOI 10.1016/j.neunet.2008.10.007. Hyun-Don Kim, 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2411. Kagami S., 2004, P INT C INF RES DEV, P3689. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Lee D.-H., 2013, P 15 EUR C POW EL AP, P1, DOI DOI 10.1145/2513383.2513451. Li XF, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/51307. Nakadai K, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P832. Narang G, 2014, IEEE SYS MAN CYBERN, P4021, DOI 10.1109/SMC.2014.6974560. Okuno HG, 2004, INTERNATIONAL CONFERENCE ON INFORMATICS RESEARCH FOR DEVELOPMENT OF KNOWLEDGE SOCIETY INFRASTRUCTURE, PROCEEDINGS, P73, DOI 10.1109/ICKS.2004.1313411. Okuno HG, 2015, INT CONF ACOUST SPEE, P5610, DOI 10.1109/ICASSP.2015.7179045. Porrill J, 2004, P ROY SOC B-BIOL SCI, V271, P789, DOI 10.1098/rspb.2003.2658. Porrill J, 2013, NEURAL NETWORKS, V47, P134, DOI 10.1016/j.neunet.2012.12.005. Rascon C, 2017, ROBOT AUTON SYST, V96, P184, DOI 10.1016/j.robot.2017.07.011. SEJNOWSKI TJ, 1977, J MATH BIOL, V4, P303, DOI 10.1007/BF00275079. Wall JA, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1981, DOI 10.1109/IJCNN.2011.6033468. Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5. Youssef K, 2012, INT CONF ACOUST SPEE, P217, DOI 10.1109/ICASSP.2012.6287856. Youssef K, 2012, IEEE INT C INT ROBOT, P1004, DOI 10.1109/IROS.2012.6385554.},
  da = {2018-10-18},
  doc-delivery-number = {GN4CH},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {(Localization,learning and adaptive systems,robot audition)},
  keywords-plus = {SOUND SOURCE LOCALIZATION},
  langid = {english},
  number = {4},
  number-of-cited-references = {29},
  orcid-numbers = {Baxendale, Mark/0000-0003-4581-4844 Secco, Emanuele Lindo/0000-0002-3269-6749},
  research-areas = {Robotics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000438961300004},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Robotics}
}

@article{bayestehtashkRobustAutomaticSpeech2016,
  title = {Robust {{Automatic Speech Recognition}} for the 4th {{CHiME Challenge Using Copula}}-Based {{Feature Enhancement}}},
  author = {Bayestehtashk, Alireza and Shafran, Izhak},
  date = {2016},
  pages = {2},
  abstract = {In this paper, we investigate the application of the copula model for enhancing features in automatic speech recognition task. We compute a set of utterance-specific nonlinear transformations based on the copula model and use these transformations to obtain the enhanced features for every utterance in the dataset. These features improve the performance of the baseline system by about 4.3\%, 1.4\%, and 0.5\% (absolute) respectively for 1channel, 2-channel and, 6-channel. Further gains were obtained when our system was combined with the baseline system using minimum Bayes risk decoding to achieve 4.3\%, 2.4\%, and 1.2\% absolute WER improvements for the respective channels.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CIXW3EZ8\\Bayestehtashk and Shafran - Robust Automatic Speech Recognition for the 4th CH.pdf},
  langid = {english}
}

@online{BBCWHP074,
  title = {{{BBC R}}\&{{D WHP}} 074 - {{WHP074}}.Pdf},
  url = {http://downloads.bbc.co.uk/rd/pubs/whp/whp-pdf-files/WHP074.pdf},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5EFT8A32\\WHP074.html}
}

@online{BeamformingTechniquesMultichannel,
  title = {Beamforming {{Techniques}} for {{Multichannel}} Audio {{Signal Separation}} ({{PDF Download Available}})},
  url = {https://www.researchgate.net/publication/233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N7MSRTMV\\Beamforming_Techniques_for_Multichannel_audio_Sign.pdf;C\:\\Users\\sauli\\Zotero\\storage\\GIQ3QWMF\\233982178_Beamforming_Techniques_for_Multichannel_audio_Signal_Separation.html}
}

@online{BeamformingWikipedia,
  title = {Beamforming - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Beamforming},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4RFG4PYN\\Beamforming.html}
}

@article{belkinLaplacianEigenmapsDimensionality2003,
  title = {Laplacian {{Eigenmaps}} for {{Dimensionality Reduction}} and {{Data Representation}}},
  author = {Belkin, Mikhail and Niyogi, Partha},
  date = {2003-06},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {15},
  pages = {1373--1396},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321780317},
  url = {http://www.mitpressjournals.org/doi/10.1162/089976603321780317},
  urldate = {2019-11-25},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VD9YU5JR\\Belkin and Niyogi - 2003 - Laplacian Eigenmaps for Dimensionality Reduction a.pdf},
  langid = {english},
  number = {6}
}

@article{belkinManifoldRegularizationGeometric,
  title = {Manifold {{Regularization}}: {{A Geometric Framework}} for {{Learning}} from {{Labeled}} and {{Unlabeled Examples}}},
  author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  pages = {36},
  abstract = {We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5B9AW8LL\\Belkin et al. - Manifold Regularization A Geometric Framework for.pdf},
  langid = {english}
}

@article{bellochRealtimeSoundSource2013,
  title = {Real-Time {{Sound Source Localization}} on {{Graphics Processing Units}}},
  author = {Belloch, Jose A. and Gonzalez, Alberto and Vidal, Antonio M. and Cobos, Maximo},
  date = {2013-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  volume = {18},
  pages = {2549--2552},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2013.05.438},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050913005814},
  urldate = {2018-06-12},
  abstract = {Sound source localization is an important topic in microphone array signal processing applications, such as camera steering systems, human-machine interaction or surveillance systems. The Steered Response Power with Phase Transform (SRP- PHAT) algorithm is one of the most well-known approaches for sound source localization due to its good performance in noisy and reverberant environments. The algorithm analyzes the sound power captured by a microphone array on a grid of spatial points in a given room. While localization accuracy can be improved by using a high resolution spatial grid and a high number of microphones, performing the localization task in these circumstances requires a high computational demand. Graphics Processing Units (GPUs) are highly parallel programmable coprocessors that provide massive computation when the needed operations are properly parallelized. This paper analyzes the performance of a real-time sound source localization system whose processing is totally carried out on a GPU. The proposed implementation yields maximum parallelism by adapting the required computations to different GPU architectures (Tesla, Fermi and Kepler).},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\84LYMNWH\\Belloch et al. - 2013 - Real-time Sound Source Localization on Graphics Pr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\8QBY5GAD\\S1877050913005814.html},
  keywords = {Audio Processing,GPU,Microphone Arrays,Sound Source Localization},
  series = {2013 {{International Conference}} on {{Computational Science}}}
}

@article{benaroyaBinauralLocalizationMultiple2018,
  title = {Binaural {{Localization}} of {{Multiple Sound Sources}} by {{Non}}-{{Negative Tensor Factorization}}},
  author = {Benaroya, E. L. and Obin, N. and Liuni, M. and Roebel, A. and Raumel, W. and Argentieri, S.},
  date = {2018-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {1072--1082},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2806745},
  abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Non-negative tensor factorization (NTF) provides a sparse representation of multichannel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by means of spectral dictionaries, temporal activation, and its distribution within each channel (here, left and right ears). This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function (HRTF) of a binaural head which can then be converted into the estimated sound source position. Moreover, the semisupervised formulation of the non-negative factorization allows us to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound, which remains unknown and is estimated on the fly. The proposed NTF-based sound source localization is applied here to binaural sound source localization of multiple speakers within realistic sound environments.},
  keywords = {audio signal processing,binaural head,binaural localization,Binaural localization,binaural sound source localization,blind source separation,computational audio scene analysis,Ear,Image analysis,matrix decomposition,multichannel audio signals,multiple sound sources,non-negative tensor factorization,nonnegative factorization,nonnegative tensor factorization,robot audition,Robot kinematics,sound source position,Speech,Speech processing,Tensile stress,tensors},
  number = {6}
}

@article{benaroyaBinauralLocalizationMultiple2018a,
  title = {Binaural {{Localization}} of {{Multiple Sound Sources}} by {{Non}}-{{Negative Tensor Factorization}}},
  author = {Benaroya, Elie Laurent and Obin, Nicolas and Liuni, Marco and Roebel, Axel and Raumel, Wilson and Argentieri, Sylvain},
  date = {2018-06},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1068-1078\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2806745\}},
  abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Non-negative tensor factorization (NTF) provides a sparse representation of multichannel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by means of spectral dictionaries, temporal activation, and its distribution within each channel (here, left and right ears). This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function (HRTF) of a binaural head which can then be converted into the estimated sound source position. Moreover, the semisupervised formulation of the non-negative factorization allows us to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound, which remains unknown and is estimated on the fly. The proposed NTF-based sound source localization is applied here to binaural sound source localization of multiple speakers within realistic sound environments.},
  affiliation = {Benaroya, EL (Reprint Author), Sorbonne Univ, CNRS, IRCAM, F-75005 Paris, France. Benaroya, Elie Laurent, Sorbonne Univ, CNRS, IRCAM, F-75005 Paris, France. Obin, Nicolas; Liuni, Marco; Roebel, Axel, Sorbonne Univ, CNRS, IRCAM, STMS, F-75005 Paris, France. Raumel, Wilson; Argentieri, Sylvain, Sorbonne Univ, CNRS, ISIR, F-75005 Paris, France.},
  author-email = {laurent.benaroya@gmail.com nobin@ircam.fr marco.liuni@ircam.fr Axel.Roebel@ircam.fr Wilson.Raumel@gmail.com sylvain.argentieri@upmc.fr},
  cited-references = {Aarabi P, 2002, IEEE T SYST MAN CY C, V32, P474, DOI 10.1109/TSMCB.2002.804369. Benaroya L, 2006, IEEE T AUDIO SPEECH, V14, P191, DOI 10.1109/TSA.2005.854110. Bisot V, 2016, INT CONF ACOUST SPEE, P6445, DOI 10.1109/ICASSP.2016.7472918. Blandin C, 2012, SIGNAL PROCESS, V92, P1950, DOI 10.1016/j.sigpro.2011.09.032. Bouvier D, 2016, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2016.7471651. BUTLER RA, 1986, HEARING RES, V21, P67, DOI 10.1016/0378-5955(86)90047-X. Cotton C., 2011, P IEEE WORKSH APPL S, P69. Dean D, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3110. Deleforge A, 2013, INT CONF ACOUST SPEE, P76, DOI 10.1109/ICASSP.2013.6637612. Durrieu JL, 2010, IEEE T AUDIO SPEECH, V18, P564, DOI 10.1109/TASL.2010.2041114. Fevotte C, 2011, NEURAL COMPUT, V23, P2421, DOI 10.1162/NECO_a_00168. FitzGerald D., 2005, IEE Irish Signals and Systems Conference 2005, P8, DOI 10.1049/cp:20050279. GARDNER WG, 1995, J ACOUST SOC AM, V97, P3907, DOI 10.1121/1.412407. Heittola T., 2011, P WORKSH MACH LIST M, P69. JOURJINE A, 2000, ACOUST SPEECH SIG PR, P2985. Keyrouz F., 2007, P IEEE INT S INT SIG, P1. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Komatsu T, 2016, INT CONF ACOUST SPEE, P2259, DOI 10.1109/ICASSP.2016.7472079. Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565. Lee S, 2012, IEEE SIGNAL PROC LET, V19, P43, DOI 10.1109/LSP.2011.2173192. Mandel M. I., 2007, P IEEE WORKSH APPL S, P275. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. MESAROS A, 2010, PROC 18 EUR SIGN, V18, P1267. Okuno HG, 2015, INT CONF ACOUST SPEE, P5610, DOI 10.1109/ICASSP.2015.7179045. Ozerov A, 2012, IEEE T AUDIO SPEECH, V20, P1118, DOI 10.1109/TASL.2011.2172425. Ozerov A, 2010, IEEE T AUDIO SPEECH, V18, P550, DOI 10.1109/TASL.2009.2031510. Paatero P, 1997, CHEMOMETR INTELL LAB, V37, P23, DOI 10.1016/S0169-7439(96)00044-5. Parry RM, 2006, LECT NOTES COMPUT SC, V3889, P666. Piczak K J., 2015, 2015 IEEE 25 INT WOR, P1. Raspaud M, 2010, IEEE T AUDIO SPEECH, V18, P68, DOI 10.1109/TASL.2009.2023644. Rickard S, 2000, PROCEEDINGS OF THE TENTH IEEE WORKSHOP ON STATISTICAL SIGNAL AND ARRAY PROCESSING, P311, DOI 10.1109/SSAP.2000.870134. Roman N, 2003, J ACOUST SOC AM, V114, P2236, DOI 10.1121/1.1610463. Sawada H, 2013, IEEE T AUDIO SPEECH, V21, P971, DOI 10.1109/TASL.2013.2239990. Stevens SS, 1936, AM J PSYCHOL, V48, P297, DOI 10.2307/1415748. Thibaut Carpentier, 2015, P INT COMP MUS C, P270. Traa Johannes, 2015, 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Proceedings, P1, DOI 10.1109/WASPAA.2015.7336944. Via C., 2011, P IEEE RSJ INT C INT, P2921. Vina C, 2013, IEEE INT C INT ROBOT, P2921, DOI 10.1109/IROS.2013.6696770. VINCENT E, 2007, PROC INT CONF IND, V4666, P552. Viste H., 2004, P 7 INT C DIG AUD EF, P145. Woodruff J, 2012, IEEE T AUDIO SPEECH, V20, P1503, DOI 10.1109/TASL.2012.2183869. Youssef K, 2015, IEEE SYS MAN CYBERN, P407, DOI 10.1109/SMC.2015.82. ZUE V, 1990, SPEECH COMMUN, V9, P351, DOI 10.1016/0167-6393(90)90010-7.},
  da = {2018-10-18},
  doc-delivery-number = {GC7NK},
  funding-acknowledgement = {Sorbonne Universites},
  funding-text = {This research was conducted during the French EMERGENCE project ROUTE (RObot l'coUTE, 2015-2016) and supported by Sorbonne Universites.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Binaural localization,computational audio scene  analysis,non-negative tensor factorization),robot audition},
  keywords-plus = {AUDIO SOURCE SEPARATION; MATRIX FACTORIZATION; MIXTURES; SIGNALS; MODEL},
  langid = {english},
  number = {6},
  number-of-cited-references = {43},
  orcid-numbers = {Argentieri, Sylvain/0000-0001-7258-797X},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000429979800004},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{benestyAdaptiveEigenvalueDecomposition1999,
  title = {Adaptive Eigenvalue Decomposition Algorithm for Passive Acoustic Source Localization},
  author = {Benesty, Jacob},
  date = {1999-12-29},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {107},
  pages = {384--391},
  issn = {0001-4966},
  doi = {10.1121/1.428310},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.428310},
  urldate = {2017-01-12},
  abstract = {To find the position of an acoustic source in a room, the relative delay between two (or more) microphone signals for the direct sound must be determined. The generalized cross-correlation method is the most popular technique to do so and is well explained in a landmark paper by Knapp and Carter. In this paper, a new approach is proposed that is based on eigenvalue decomposition. Indeed, the eigenvector corresponding to the minimum eigenvalue of the covariance matrix of the microphone signals contains the impulse responses between the source and the microphone signals (and therefore all the information we need for time delay estimation). In experiments, the proposed algorithm performs well and is very accurate.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3JIBZSHW\\1.html},
  number = {1}
}

@article{bengioGreedyLayerWiseTraining,
  title = {Greedy {{Layer}}-{{Wise Training}} of {{Deep Networks}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo and Box, P O},
  pages = {17},
  abstract = {Deep multi-layer neural networks have many levels of non-linearities, which allows them to potentially represent very compactly highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B467EPG9\\Bengio et al. - Greedy Layer-Wise Training of Deep Networks.pdf},
  langid = {english}
}

@article{bengioLearningDeepArchitectures2009,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Y.},
  date = {2009},
  journaltitle = {Foundations and Trends® in Machine Learning},
  volume = {2},
  pages = {1--127},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000006},
  url = {http://www.nowpublishers.com/article/Details/MAL-006},
  urldate = {2017-01-30},
  langid = {english},
  number = {1}
}

@article{BengioLearningDeepArchitectures2009,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Y.},
  date = {2009},
  journaltitle = {Foundations and Trends® in Machine Learning},
  volume = {2},
  pages = {1--127},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000006},
  langid = {english},
  number = {1}
}

@article{berdugoDirectionFindingEmitting1999,
  title = {On Direction Finding of an Emitting Source from Time Delays},
  author = {Berdugo, Baruch and Doron, Miriam A. and Rosenhouse, Judith and Azhari, Haim},
  date = {1999-05-24},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {105},
  pages = {3355--3363},
  issn = {0001-4966},
  doi = {10.1121/1.424664},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.424664},
  urldate = {2018-05-17},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\46753F6G\\1.html},
  number = {6}
}

@article{betlehemAcousticSignalProcessing2005,
  title = {Acoustic Signal Processing Algorithms for Reverberant Environments},
  author = {Betlehem, Terence},
  date = {2005},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IP7VN65R\\Betlehem - 2005 - Acoustic signal processing algorithms for reverber.pdf}
}

@article{beyerMethodGenerateStructural,
  title = {A Method to Generate Structural Impulse-Responses for Measuring the Effects of Shocks in Structural Macro Models},
  author = {Beyer, Andreas and Farmer, Roger E A},
  pages = {46},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CU382YZ5\\Beyer and Farmer - A method to generate structural impulse-responses .pdf},
  langid = {english}
}

@article{bharathiUnderwaterSoundSource2018,
  title = {Underwater {{Sound Source Localization}} by {{EMD}}-{{Based Maximum Likelihood Method}}},
  author = {Bharathi, B. Marxim Rahula and Mohanty, A. R.},
  date = {2018-08},
  journaltitle = {ACOUSTICS AUSTRALIA},
  volume = {46},
  pages = {\{193-203\}},
  publisher = {SPRINGER SINGAPORE PTE LTD},
  location = {#04-01 CENCON I, 1 TANNERY RD, SINGAPORE 347719, SINGAPORE},
  issn = {1839-2571},
  doi = {\{10.1007/s40857-018-0129-8\}},
  abstract = {The underwater object localization is important in defense, underwater biological and environmental applications. Localization using a passive sonar system is a challenging task. It is more challenging when the source and receivers are in the reverberant environment. Time delay estimation (TDE)-based localization is a well-known technique to localize source for last few decades. In this work, empirical mode decomposition maximum likelihood (EMD ML TDE) method is used to estimate the time delay in a reverberant environment. The sound source location is estimated by intersecting spherical surfaces from the time delay. The experimental results prove that EMD ML time delay estimation method is effective to localize a sound source in a reverberant environment.},
  affiliation = {Bharathi, BMR (Reprint Author), Indian Inst Technol, Dept Mech Engn, Kharagpur, W Bengal, India. Bharathi, B. Marxim Rahula; Mohanty, A. R., Indian Inst Technol, Dept Mech Engn, Kharagpur, W Bengal, India.},
  author-email = {bmrb.2009@gmail.com},
  cited-references = {Argentieri S., 2007, IEEE RSJ INT C INT R. Asano F., 2001, 7 EUR C SPEECH COMM. BEDARD S, 1994, INT CONF ACOUST SPEE, P261. Boudraa A. O., 2006, P IEEE ISCCSP, V4. CARTER GC, 1973, P IEEE, V61, P1497, DOI 10.1109/PROC.1973.9300. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. Chen JD, 2004, AUDIO SIGNAL PROCESSING: FOR NEXT-GENERATION MULTIMEDIA COMMUNICATION SYSTEMS, P197, DOI 10.1007/1-4020-7769-6_8. Dhull S., 2010, INT J COMPUTER APPL, V8, P29. DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157. Duan R, 2014, ACOUST AUST, V42, P36. Gedalyahu K, 2010, IEEE T SIGNAL PROCES, V58, P3017, DOI 10.1109/TSP.2010.2044253. HANNAN EJ, 1973, BIOMETRIKA, V60, P241, DOI 10.1093/biomet/60.2.241. HINICH MJ, 1992, IEEE T SIGNAL PROCES, V40, P106, DOI 10.1109/78.157186. Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193. Hyndman RJ, 2006, INT J FORECASTING, V22, P679, DOI 10.1016/j.ijforecast.2006.03.001. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Kopsinis Y, 2009, IEEE T SIGNAL PROCES, V57, P1351, DOI 10.1109/TSP.2009.2013885. Lei YG, 2013, MECH SYST SIGNAL PR, V35, P108, DOI 10.1016/j.ymssp.2012.09.015. Liang YC, 1997, ELECTRON LETT, V33, P751, DOI 10.1049/el:19970546. Lui KWK, 2009, SIGNAL PROCESS, V89, P1835, DOI 10.1016/j.sigpro.2009.03.009. Lyon D, 2010, J OBJECT TECHNOL, V9, P17, DOI 10.5381/jot.2010.9.2.c2. Mohanty AR, 2006, IEEE T IND ELECTRON, V53, P1285, DOI 10.1109/TIE.2006.878303. OMOLOGO M, 1994, INT CONF ACOUST SPEE, P273. Qasaymeh MM, 2009, IEEE SIGNAL PROC LET, V16, P422, DOI 10.1109/LSP.2009.2016483. Rai VK, 2007, MECH SYST SIGNAL PR, V21, P2607, DOI 10.1016/j.ymssp.2006.12.004. ROTH PR, 1971, IEEE SPECTRUM, V8, P62, DOI 10.1109/MSPEC.1971.5218046. SCHAU HC, 1987, IEEE T ACOUST SPEECH, V35, P1223, DOI 10.1109/TASSP.1987.1165266. Sharma KK, 2007, SIGNAL PROCESS, V87, P853, DOI 10.1016/j.sigpro.2006.08.007. STEPHENNE A, 1995, INT CONF ACOUST SPEE, P3055, DOI 10.1109/ICASSP.1995.479490. Tsypkin M, 2013, 2013 IEEE INTERNATIONAL ELECTRIC MACHINES & DRIVES CONFERENCE (IEMDC), P117. Valin JM, 2004, IEEE INT CONF ROBOT, P1033, DOI 10.1109/ROBOT.2004.1307286. Wang H, 1997, INT CONF ACOUST SPEE, P187, DOI 10.1109/ICASSP.1997.599595. Weng Binwei, 2006, Conf Proc IEEE Eng Med Biol Soc, V1, P1. Yang KD, 2017, ACOUST AUST, V45, P51, DOI 10.1007/s40857-017-0084-9. Zhou T, 2014, SIGNAL PROCESS, V96, P110, DOI 10.1016/j.sigpro.2013.06.004.},
  da = {2018-10-18},
  doc-delivery-number = {GR3VT},
  journal-iso = {Acoust. Aust.},
  keywords = {(Empirical mode decomposition,EMD ML TDE,Generalized cross-correlation,Localization,Reverberant environment,SNR),TDE},
  keywords-plus = {TIME-DELAY ESTIMATION; EMPIRICAL MODE DECOMPOSITION; PASSIVE LOCALIZATION; FOURIER-TRANSFORM; FAULT-DIAGNOSIS; DEEP-OCEAN; SPECTRUM},
  langid = {english},
  number = {2},
  number-of-cited-references = {35},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000442525800002},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Acoustics}
}

@online{binauralfutureRayleighDuplexTheory,
  title = {Rayleigh {{Duplex Theory}}},
  author = {{binauralfuture}},
  url = {http://diamonddissertation.blogspot.com/2010/05/rayleigh-duplex-theory.html},
  urldate = {2018-02-19},
  abstract = {Duplex theory, as proposed by Lord Rayleigh, describes sound reaching the two ears as Interaural  Time Differences ( ITD ) and Interaural ...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Z9A2XJ8Y\\rayleigh-duplex-theory.html}
}

@online{BiologicallyInspiredSpeech,
  title = {Biologically {{Inspired Speech}} and {{Sound Recognition}} | {{Physiological Acoustics Lab}}},
  url = {https://escabilab.uconn.edu/biologically-inspired-speech-and-sound-recognition/},
  urldate = {2018-04-29},
  abstract = {Mammalian audition is highly resilient to acoustic variability, such as background noise and multiple talkers, yet how the brain accomplishes this seemi...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IJG56XDF\\biologically-inspired-speech-and-sound-recognition.html},
  langid = {american}
}

@inproceedings{bonnalSpeakerLocalizationSpeech2009,
  title = {Speaker Localization and Speech Extraction with the {{EAR}} Sensor},
  booktitle = {2009 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Bonnal, J. and Argentieri, S. and Danés, P. and Manhès, J.},
  date = {2009-10},
  pages = {670--675},
  doi = {10.1109/IROS.2009.5354401},
  abstract = {This paper presents the embedded audition for robotics (EAR) project internally developed at LAAS and its application to speaker localization and extraction. Hardware and software issues are first thoroughly depicted, concerning the development of an auditory sensor based on an array of microphones, a homemade dedicated acquisition chain and a FPGA based processing board. Then, the EAR sensor is assessed against various scenarios, in real noisy robotics environments. Localization results are presented when a speaker emits an utterance in the presence of a disturbing source. These validate the underlying theory and suggest further theoretical and experimental developments.},
  eventtitle = {2009 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SXR45QWX\\5354401.html},
  keywords = {Acoustic sensors,auditory sensor,data acquisition,Ear,EAR sensor,embedded audition for robotics,field programmable gate arrays,FPGA based processing board,Hardware,hearing,homemade dedicated acquisition chain,microphone array,microphone arrays,real noisy robotics environment,Robot sensing systems,Robots,Sensor arrays,Service robots,speaker localization,speaker recognition,Speech,speech extraction}
}

@inproceedings{brandstein_closed-form_1995,
  title = {A Closed-Form Method for Finding Source Locations from Microphone-Array Time-Decay Estimates},
  booktitle = {1995 {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Brandstein, M. S. and Adcock, J. E. and Silverman, H. F.},
  date = {1995-05},
  volume = {5},
  pages = {3019-3022 vol.5},
  doi = {10.1109/ICASSP.1995.479481},
  abstract = {The linear intersection (LI) estimator, a closed-form method for the localization of source positions given only the sensor array time-delay estimate information, is presented. The array is constrained to be composed of 4-element sub-arrays configured in 2 centered orthogonal pairs. A bearing line in 3-space is estimated from each sub-array and potential source locations are found via closest intersection of bearing line pairs. The final location estimate is determined by a probabilistic weighting of these potential locations. The LI estimator is shown to be robust and accurate, to closely model the ML estimator, and to outperform a representative algorithm. The computational complexity of the LI estimator is suitable for use in real-time microphone-array applications},
  eventtitle = {1995 {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QFUM987M\\Brandstein et al. - 1995 - A closed-form method for finding source locations .pdf;C\:\\Users\\sauli\\Zotero\\storage\\U7FFWWF8\\479481.html},
  keywords = {4-element sub-arrays,acoustic arrays,acoustic signal processing,acoustic transducer arrays,bearing line,closed-form method,computational complexity,Delay estimation,delays,direction-of-arrival estimation,estimation theory,Laboratories,linear intersection estimator,Maximum likelihood estimation,microphone arrays,microphone-array time-decay estimates,Microphones,orthogonal pairs,Position measurement,probabilistic weighting,real-time microphone-array applications,sensor array time-delay estimate information,Sensor arrays,Sensor systems,source locations,Speech,Switches,Systems engineering and theory}
}

@inproceedings{brandstein1997,
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Brandstein, M. S. and Silverman, H. F.},
  date = {1997-04},
  volume = {1},
  pages = {375-378 vol.1},
  doi = {10.1109/ICASSP.1997.599651},
  abstract = {Conventional time-delay estimators exhibit dramatic performance degradations in the presence of multipath signals. This limits their application in reverberant enclosures, particularly when the signal of interest is speech and it may not possible to estimate and compensate for channel effects prior to time-delay estimation. This paper details an alternative approach which reformulates the problem as a linear regression of phase data and then estimates the time-delay through minimization of a robust statistical error measure. The technique is shown to be less susceptible to room reverberation effects. Simulations are performed across a range of source placements and room conditions to illustrate the utility of the proposed time-delay estimation method relative to conventional methods.},
  eventtitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CLENUHBH\\599651.html},
  keywords = {acoustic signal processing,acoustic wave reflection,architectural acoustics,channel effects compensation,Degradation,Delay estimation,delays,error analysis,Filters,linear regression,Linear regression,Maximum likelihood estimation,multipath channels,multipath signals,performance degradations,phase data,Phase estimation,Phase measurement,reflection coefficients,reflective surfaces,reverberant enclosures,reverberant rooms,reverberation,robust statistical error measure,Robustness,room conditions,room reverberation effects,simulations,source placements,Speech analysis,speech processing,speech signal,statistical analysis,Systems engineering and theory,time delay estimation}
}

@thesis{brandsteinFrameworkSpeechSource1995,
  title = {A {{Framework}} for {{Speech Source Localization Using Sensor Arrays}}},
  author = {Brandstein, Michael Shapiro},
  date = {1995},
  institution = {{Brown University}},
  location = {{Providence, RI, USA}},
  abstract = {Electronically steerable arrays of microphones have a variety of uses in speech data acquisition systems. Applications include teleconferencing, speech recognition and speaker identification, sound capture in adverse environments, and biomedical devices for the hearing impaired. An array of microphones has a number of advantages over a single-microphone system. It may be electronically aimed to provide a high-quality signal from a desired source location while simultaneously attenuating interfering talkers and ambient noise, does not necessitate local placement of transducers or encumber the talker with a hand-held or head-mounted microphone, and does not require physical movement to alter its direction of reception. Additionally, it has capabilities that a single microphone does not; namely automatic detection, localization, and tracking of active talkers in its receptive area. A fundamental requirement of sensor array systems is the ability to locate and track a speech source. An accurate fix on the primary talker, as well as knowledge of any interfering talkers or coherent noise sources, is necessary to effectively steer the array. Source location data may also be used for purposes other than beamforming; e.g. aiming a camera in a video-conferencing system. In addition to high accuracy, the location estimator must be capable of a high update rate as well as being computationally non-demanding in order to be useful for real-time tracking and beamforming applications.This thesis addresses the specific application of source localization algorithms for estimating the position of speech sources in a real room environment given limited computational resources. The theoretical foundations of a speech source localization system are presented. This includes the development of a source-sensor geometry for talkers and sensors in the near-field environment, the evaluation of several error criteria available to the problem, and the detailing of source detection and estimate-error prediction methods. Several practical algorithms necessary for real-time implementation are then developed, specifically the derivation and evaluation of an appropriate time-delay estimator and a novel closed-form locator. Finally, results obtained from several real systems are presented to illustrate the effectiveness of the proposed source localization techniques as well as to confirm the practicality of the theoretical models.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RBLLENJ2\\Brandstein - 1995 - A Framework for Speech Source Localization Using S.pdf},
  type = {PhD Thesis}
}

@collection{brandsteinMicrophoneArraysSignal2001,
  title = {Microphone {{Arrays}}: {{Signal Processing Techniques}} and {{Applications}}},
  shorttitle = {Microphone {{Arrays}}},
  editor = {Brandstein, Michael and Ward, Darren},
  date = {2001},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-04619-7},
  url = {http://link.springer.com/10.1007/978-3-662-04619-7},
  urldate = {2019-08-05},
  editorb = {Lacroix, Arild and Venetsanopoulos, Anastasios},
  editorbtype = {redactor},
  isbn = {978-3-642-07547-6 978-3-662-04619-7},
  keywords = {****},
  langid = {english},
  series = {Digital {{Signal Processing}}}
}

@inproceedings{brandsteinRobustMethodSpeech1997,
  title = {A Robust Method for Speech Signal Time-Delay Estimation in Reverberant Rooms},
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Brandstein, M.S. and Silverman, H.F.},
  date = {1997},
  volume = {1},
  pages = {375--378},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Munich, Germany}},
  doi = {10.1109/ICASSP.1997.599651},
  url = {http://ieeexplore.ieee.org/document/599651/},
  urldate = {2019-08-16},
  abstract = {Conventional time-delay estimators exhibit dramatic performance degradations in the presence of multipath signals. This limits their application in reverberant enclosures, particularly when the signal of interest is speech and it may not possible to estimate and compensate for channel effects prior to time-delay estimation. This paper details an alternative approach which reformulates the problem as a linear regression of phase data and then estimates the time-delay through minimization of a robust statistical error measure. The technique is shown to be less susceptible to room reverberation effects. Simulations are performed across a range of source placements and room conditions to illustrate the utility of the proposed time-delay estimation method relative to conventional methods.},
  eventtitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9UWVJ47Q\\Brandstein and Silverman - 1997 - A robust method for speech signal time-delay estim.pdf},
  isbn = {978-0-8186-7919-3},
  langid = {english}
}

@article{braunLocalization3DAmbisonic,
  title = {Localization of {{3D Ambisonic Recordings}} and {{Ambisonic Virtual Sources}}},
  author = {Braun, Sebastian and Frank, Matthias},
  pages = {6},
  abstract = {The accurate recording and reproduction of real 3D sound environments is still a challenging task. In this paper, the two most established 3D microphones are evaluated: the Soundfield SPS200 and MH Acoustics’ Eigenmike EM32, according to 1st and 4th order Ambisonics. They are compared to virtual encoded sound sources of the same orders in a localization test. For the reproduction, an Ambisonics system with 24 (12+8+4) hemispherically arranged loudspeakers is used. In order to compare to existing results from the literature, this paper focuses on sound sources in the horizontal plane. As expected, the 4th order sources yield better localization as the 1st order sources. Within each order, the real recordings and the virtual encoded sources show a good correspondence.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\57UCVTWC\\Braun and Frank - Localization of 3D Ambisonic Recordings and Ambiso.pdf},
  langid = {english}
}

@article{bree_real_2008,
  title = {Real Time Sound Field Visualization in the near Field, Far Field and at Absorbing Surfaces},
  author = {Bree, Hans-Elias De and Tijs, Emiel and Basten, Tom},
  date = {2008-06-01},
  volume = {123},
  pages = {3439},
  issn = {1520-8524},
  doi = {10.1121/1.2934232},
  url = {https://www.researchgate.net/publication/5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfaces},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Real time sound field visualization in the near field, far field and at absorbing surfaces on ResearchGate, the professional network for scientists.},
  eprint = {18530972},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5FKIMCRR\\Bree et al. - 2008 - Real time sound field visualization in the near fi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\HURJR4KI\\5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfac.html},
  number = {5}
}

@article{breeRealTimeSound2008,
  title = {Real Time Sound Field Visualization in the near Field, Far Field and at Absorbing Surfaces},
  author = {Bree, Hans-Elias De and Tijs, Emiel and Basten, Tom},
  date = {2008-06-01},
  journaltitle = {ResearchGate},
  volume = {123},
  pages = {3439},
  issn = {1520-8524},
  doi = {10.1121/1.2934232},
  url = {https://www.researchgate.net/publication/5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfaces},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Real time sound field visualization in the near field, far field and at absorbing surfaces on ResearchGate, the professional network for scientists.},
  eprint = {18530972},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RJQMG6KX\\Bree et al. - 2008 - Real time sound field visualization in the near fi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EQXGK346\\5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfac.html},
  number = {5}
}

@article{breeRealTimeSound2008a,
  title = {Real Time Sound Field Visualization in the near Field, Far Field and at Absorbing Surfaces},
  author = {Bree, Hans-Elias De and Tijs, Emiel and Basten, Tom},
  date = {2008-06-01},
  volume = {123},
  pages = {3439},
  issn = {1520-8524},
  doi = {10.1121/1.2934232},
  url = {https://www.researchgate.net/publication/5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfaces},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Real time sound field visualization in the near field, far field and at absorbing surfaces on ResearchGate, the professional network for scientists.},
  eprint = {18530972},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\32WBGTMS\\Bree et al. - 2008 - Real time sound field visualization in the near fi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\QKVN4QSI\\5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfac.html},
  number = {5}
}

@article{breeRealTimeSound2008b,
  title = {Real Time Sound Field Visualization in the near Field, Far Field and at Absorbing Surfaces},
  author = {Bree, Hans-Elias De and Tijs, Emiel and Basten, Tom},
  date = {2008-06-01},
  journaltitle = {ResearchGate},
  volume = {123},
  pages = {3439},
  issn = {1520-8524},
  doi = {10.1121/1.2934232},
  url = {https://www.researchgate.net/publication/5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfaces},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Real time sound field visualization in the near field, far field and at absorbing surfaces on ResearchGate, the professional network for scientists.},
  eprint = {18530972},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\D4N7SU4E\\Bree et al. - 2008 - Real time sound field visualization in the near fi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\9TSMWVKP\\5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfac.html},
  number = {5}
}

@article{breeRealTimeSound2008c,
  title = {Real Time Sound Field Visualization in the near Field, Far Field and at Absorbing Surfaces},
  author = {Bree, Hans-Elias De and Tijs, Emiel and Basten, Tom},
  date = {2008-06-01},
  journaltitle = {ResearchGate},
  volume = {123},
  pages = {3439},
  issn = {1520-8524},
  doi = {10.1121/1.2934232},
  url = {https://www.researchgate.net/publication/5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfaces},
  urldate = {2017-01-10},
  abstract = {Official Full-Text Publication: Real time sound field visualization in the near field, far field and at absorbing surfaces on ResearchGate, the professional network for scientists.},
  eprint = {18530972},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NGMKQ3B6\\Bree et al. - 2008 - Real time sound field visualization in the near fi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\W4XUR9ZJ\\5326835_Real_time_sound_field_visualization_in_the_near_field_far_field_and_at_absorbing_surfac.html},
  number = {5}
}

@inproceedings{brendelLearningbasedAcousticSource2018,
  title = {Learning-Based {{Acoustic Source Localization}} in {{Acoustic Sensor Networks}} Using the {{Coherent}}-to-{{Diffuse Power Ratio}}},
  booktitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Brendel, A. and Kellermann, W.},
  date = {2018-09},
  pages = {1572--1576},
  doi = {10.23919/EUSIPCO.2018.8553405},
  abstract = {A distributed learning-based algorithm for the localization of acoustic sources in an acoustic sensor network is proposed. It is based on estimates of the Coherent-to-Diffuse Power Ratio (CDR), which serve as feature for the source-microphone distance, i.e., the range. The relation between the estimated CDR and the range is learned by using Gaussian processes for non-parametric regression. The range estimates obtained from evaluating the regression function are fused by a weighted least squares estimation, which is implemented recursively, allowing for a distributed version of the algorithm. The resulting method is computationally efficient, works in highly reverberant and noisy scenarios and needs only a small amount of data shared over the network. The training phase of the algorithm requires only a few labeled observations. We show the efficacy of the approach with data obtained from image-source simulation.},
  eventtitle = {2018 26th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JF4YXIUY\\Brendel and Kellermann - 2018 - Learning-based Acoustic Source Localization in Aco.pdf;C\:\\Users\\sauli\\Zotero\\storage\\N8BNZLLN\\8553405.html},
  keywords = {acoustic radiators,acoustic sensor network,Acoustic Sensor Network,acoustic signal processing,acoustic source localization,Acoustics,CDR,coherent-to-diffuse power ratio,Coherent-to-Diffuse Power Ratio,Distributed Algorithm,distributed learning-based algorithm,distributed version,Estimation,Gaussian Process Regression,Gaussian processes,image-source simulation,learning (artificial intelligence),least squares approximations,Localization,microphone arrays,microphones,nonparametric regression,regression analysis,reverberation,Sensors,Signal processing,Signal processing algorithms,source-microphone distance,telecommunication computing,Training,Training data,Weighted Least Squares,weighted least squares estimation,wireless sensor networks}
}

@inproceedings{brendelLocalizationMultipleSimultaneously2017,
  title = {Localization of Multiple Simultaneously Active Sources in Acoustic Sensor Networks Using {{ADP}}},
  booktitle = {2017 {{IEEE}} 7th {{International Workshop}} on {{Computational Advances}} in {{Multi}}-{{Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Brendel, Andreas and Kellermann, Walter},
  date = {2017-12},
  pages = {1--5},
  issn = {null},
  doi = {10.1109/CAMSAP.2017.8313167},
  abstract = {In this contribution, we propose a new localization approach for multiple simultaneously active sound sources using acoustic sensor networks. It is based on the averaged directivity pattern (ADP) approach, which explicitly models the influence of reverberation on its direction of arrival (DOA) estimates and has shown to be robust against additive noise and reverberation. We develop a framework capable of handling prior information, the integration of several DOA estimators and reliability information. In particular, we derive range information from the estimated ADP-energy map and use a contribution removal technique to tackle the problem of ghost sources. Experiments with simulated room impulse responses show the efficacy of this approach.},
  eventtitle = {2017 {{IEEE}} 7th {{International Workshop}} on {{Computational Advances}} in {{Multi}}-{{Sensor Adaptive Processing}} ({{CAMSAP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\43KFN8Y2\\Brendel and Kellermann - 2017 - Localization of multiple simultaneously active sou.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IXPXTJF4\\8313167.html},
  keywords = {*****,acoustic communication (telecommunication),acoustic sensor networks,additive noise,ADP approach,averaged directivity pattern approach,Channel estimation,Conferences,contribution removal technique,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,DOA estimators,energy map estimation,Estimation,ghost sources,Microphones,multiple simultaneously active sound sources,multiple simultaneously active source localization approach,MUST READ,reliability information,reverberation,Reverberation,simulated room impulse responses,telecommunication network reliability,wireless sensor networks}
}

@inproceedings{brendelTrackingMultipleSources2018,
  title = {Tracking of {{Multiple Sources}} in an {{Acoustic Sensor Network Using}} an {{Extended Gaussian Mixture PHD Filter}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Brendel, A. and Kellermann, W.},
  date = {2018-07},
  pages = {400--404},
  doi = {10.1109/SAM.2018.8448796},
  abstract = {We propose an approach for tracking a varying number of simultaneously active acoustic wideband signal sources in an acoustic enclosure. Relying on the assumption of W-disjoint orthogonality, the method uses narrowband position estimates of the sources for the targets. The instantaneous position estimates form clusters rather than single points, as would be required for a conventional Probability Hypothesis Density (PHD) filter. Therefore, we model the position estimates as extended targets and use a special form of the PHD filter, the extended target Gaussian mixture PHD filter, for tracking the targets. This allows to model target birth and death, which correspond to speech onset and end of utterance, respectively. With this model and by using the well-developed theory of Finite Set Statistics (FISST)-based multi-target tracking, we provide a comprehensive, strictly Bayesian treatment of the problem of tracking wideband acoustic sources using narrowband position estimates. We validate the results by tracking a varying number of targets in an enclosure simulated with the image-source method.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W64KSI3B\\Brendel and Kellermann - 2018 - Tracking of Multiple Sources in an Acoustic Sensor.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZFIHSDNA\\8448796.html},
  keywords = {acoustic communication (telecommunication),acoustic sensor network,Acoustics,active acoustic wideband signal sources,Bayes methods,Direction-of-arrival estimation,extended Gaussian mixture PHD filter,extended target Gaussian mixture PHD filter,filtering theory,Finite Set Statistics-based multitarget tracking,Gaussian processes,image-source method,Microphones,Narrowband,Probability Hypothesis Density filter,speech onset,target tracking,Target tracking,wideband acoustic sources,wireless sensor networks}
}

@online{BroadbandAcousticsourceLocalization,
  title = {Broadband Acoustic-Source Localization Using Passive Sonar via Multitask Learning: {{The Journal}} of the {{Acoustical Society}} of {{America}}: {{Vol}} 135, {{No}} 4},
  url = {http://asa.scitation.org/doi/10.1121/1.4877767},
  urldate = {2017-09-06},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MJVPUZVI\\1.html}
}

@online{BroadbandMusicOpportunities,
  title = {Broadband {{Music}}: {{Opportunities}} and {{Challenges}} for {{Multiple Source Localization}} - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/4392978},
  urldate = {2020-02-25},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9WICPZJJ\\4392978.html}
}

@inproceedings{broeckTimedomainGeneralizedCross2012,
  title = {Time-Domain Generalized Cross Correlation Phase Transform Sound Source Localization for Small Microphone Arrays},
  booktitle = {2012 5th {{European DSP Education}} and {{Research Conference}} ({{EDERC}})},
  author = {Broeck, B. Van Den and Bertrand, A. and Karsmakers, P. and Vanrumste, B. and {hamme}, H. Van and Moonen, M.},
  date = {2012-09},
  pages = {76--80},
  doi = {10.1109/EDERC.2012.6532229},
  abstract = {Due to hard- and software progress applications based on sound enhancement are gaining popularity. But such applications are often still limited by hardware costs, energy and real-time constraints, thereby bounding the available complexity. One task often accompanied with (multichannel) sound enhancement is the localization of the sound source. This paper focusses on implementing an accurate Sound Source Localizer (SSL) for estimating the position of a sound source on a digital signal processor, using as less CPU resources as possible. One of the least complex algorithms for SSL is a simple correlation, implemented in the frequency-domain for efficiency, combined with a frequency bin weighing for robustness. Together called Generalized Cross Correlation (GCC). One popular weighing called GCC PHAse Transform (GCC-PHAT) will be handled. In this paper it is explained that for small microphone arrays this frequency-domain implementation is inferior to its time-domain alternative in terms of algorithmic complexity. Therefore a time-domain PHAT equivalent will be described. Both implementations are compared in terms of complexity (clock cycles needed on a Texas Instruments C5515 DSP) and obtained results, showing a complexity gain with a factor of 146, with hardly any loss in localization accuracy.},
  eventtitle = {2012 5th {{European DSP Education}} and {{Research Conference}} ({{EDERC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G5E7J3AK\\Broeck et al. - 2012 - Time-domain generalized cross correlation phase tr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BEVZPBCR\\6532229.html},
  keywords = {algorithmic complexity,CPU resources,digital signal processor,energy constraints,frequency domain implementation,GCC phase transform,GCC-PHAT,generalized cross correlation,hardware costs,least complex algorithms,microphone arrays,phase transformations,real-time constraints,small microphone arrays,sound enhancement,sound source localizer,SSL,time domain generalized cross correlation phase transform sound source localization}
}

@inproceedings{BroeckTimedomaingeneralizedcross2012,
  title = {Time-{{Domain Generalized Cross Correlation Phase Transform Sound Source Localization}} for {{Small Microphone Arrays}}},
  booktitle = {2012 5th {{European DSP Education}} and {{Research Conference}} ({{EDERC}})},
  author = {Broeck, B. Van Den and Bertrand, A. and Karsmakers, P. and Vanrumste, B. and {hamme}, H. Van and Moonen, M.},
  date = {2012-09},
  pages = {76--80},
  doi = {10.1109/EDERC.2012.6532229},
  abstract = {Due to hard- and software progress applications based on sound enhancement are gaining popularity. But such applications are often still limited by hardware costs, energy and real-time constraints, thereby bounding the available complexity. One task often accompanied with (multichannel) sound enhancement is the localization of the sound source. This paper focusses on implementing an accurate Sound Source Localizer (SSL) for estimating the position of a sound source on a digital signal processor, using as less CPU resources as possible. One of the least complex algorithms for SSL is a simple correlation, implemented in the frequency-domain for efficiency, combined with a frequency bin weighing for robustness. Together called Generalized Cross Correlation (GCC). One popular weighing called GCC PHAse Transform (GCC-PHAT) will be handled. In this paper it is explained that for small microphone arrays this frequency-domain implementation is inferior to its time-domain alternative in terms of algorithmic complexity. Therefore a time-domain PHAT equivalent will be described. Both implementations are compared in terms of complexity (clock cycles needed on a Texas Instruments C5515 DSP) and obtained results, showing a complexity gain with a factor of 146, with hardly any loss in localization accuracy.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GIQ6SRG8\\Broeck et al. - 2012 - Time-domain generalized cross correlation phase tr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\C6S97AGB\\6532229.html},
  keywords = {algorithmic complexity,CPU resources,digital signal processor,energy constraints,frequency domain implementation,GCC phase transform,GCC-PHAT,generalized cross correlation,hardware costs,least complex algorithms,microphone arrays,phase transformations,real-time constraints,small microphone arrays,sound enhancement,sound source localizer,SSL,time domain generalized cross correlation phase transform sound source localization}
}

@incollection{brownComparativeMammalianSound2005,
  title = {Comparative {{Mammalian Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Brown, Charles H. and May, Bradford J.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {124--178},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_5},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_5},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RG5SRKFF\\0-387-28863-5_5.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{BrownComparativeMammalianSound2005,
  title = {Comparative {{Mammalian Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Brown, Charles H. and May, Bradford J.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {124--178},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_5},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZWJDKSIV\\0-387-28863-5_5.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@inproceedings{bruttiComparisonDifferentSound2008,
  title = {Comparison {{Between Different Sound Source Localization Techniques Based}} on a {{Real Data Collection}}},
  booktitle = {2008 {{Hands}}-{{Free Speech Communication}} and {{Microphone Arrays}}},
  author = {Brutti, Alessio and Omologo, Maurizio and Svaizer, Piergiorgio},
  date = {2008-05},
  pages = {69--72},
  publisher = {{IEEE}},
  location = {{Trento, Italy}},
  doi = {10.1109/HSCMA.2008.4538690},
  url = {http://ieeexplore.ieee.org/document/4538690/},
  urldate = {2019-08-12},
  abstract = {Comparing the different sound source localization techniques, proposed in the literature during the last decade, represents a relevant topic in order to establish advantages and disadvantages of a given approach in a real-time implementation. Traditionally, algorithms for sound source localization rely on an estimation of Time Difference of Arrival (TDOA) at microphone pairs through GCC-PHAT. When several microphone pairs are available the source position can be estimated as the point in space that best fits the set of TDOA measurements by applying Global Coherence Field (GCF), also known as SRP-PHAT, or Oriented Global Coherence Field (OGCF). A first interesting analysis compares the performance of GCF and OGCF to a suboptimal LS search method. In a second step, Adaptive Eigenvalue Decomposition is implemented as an alternative to GCC-PHAT in TDOA estimation. Comparative experiments are conducted on signals acquired by a linear array during WOZ experiments in an interactive-TV scenario. Changes in performance according to different SNR levels are reported.},
  eventtitle = {2008 {{Hands}}-{{Free Speech Communication}} and {{Microphone Arrays}} ({{HSCMA}} 2008)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8V3ZBICJ\\Brutti et al. - 2008 - Comparison Between Different Sound Source Localiza.pdf},
  isbn = {978-1-4244-2337-8},
  langid = {english}
}

@inproceedings{bruttiLocalizationMultipleSpeakers2008,
  title = {Localization of Multiple Speakers Based on a Two Step Acoustic Map Analysis},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Brutti, A. and Omologo, M. and Svaizer, P.},
  date = {2008-03},
  pages = {4349--4352},
  doi = {10.1109/ICASSP.2008.4518618},
  abstract = {An interface for distant-talking control of home devices requires the possibility of identifying the positions of multiple users. Acoustic maps, based either on global coherence field (GCF) or oriented global coherence field (OGCF), have already been exploited successfully to determine position and head orientation of a single speaker. This paper proposes a new method using acoustic maps to deal with the case of two simultaneous speakers. The method is based on a two step analysis of a coherence map: first the dominant speaker is localized; then the map is modified by compensating for the effects due to the first speaker and the position of the second speaker is detected. Simulations were carried out to show how an appropriate analysis of OGCF and GCF maps allows one to localize both speakers. Experiments proved the effectiveness of the proposed solution in a linear microphone array set up.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6C8GYXG4\\Brutti et al. - 2008 - Localization of multiple speakers based on a two s.pdf;C\:\\Users\\sauli\\Zotero\\storage\\452ALP9Y\\4518618.html},
  keywords = {Acoustic devices,acoustic signal detection,Active noise reduction,Ambient intelligence,Analytical models,coherence map,Computerized monitoring,distant-talking control,global coherence field,Humans,linear microphone array,Loudspeakers,microphone array,microphone arrays,Microphone arrays,multiple speaker localization,multiple speakers,oriented global coherence field,speaker localization,speaker recognition,Time difference of arrival,two step acoustic map analysis,Working environment noise}
}

@inproceedings{bruttiLocalizationMultipleSpeakers2008a,
  title = {Localization of Multiple Speakers Based on a Two Step Acoustic Map Analysis},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Brutti, A. and Omologo, M. and Svaizer, P.},
  date = {2008-03},
  pages = {4349--4352},
  doi = {10.1109/ICASSP.2008.4518618},
  abstract = {An interface for distant-talking control of home devices requires the possibility of identifying the positions of multiple users. Acoustic maps, based either on global coherence field (GCF) or oriented global coherence field (OGCF), have already been exploited successfully to determine position and head orientation of a single speaker. This paper proposes a new method using acoustic maps to deal with the case of two simultaneous speakers. The method is based on a two step analysis of a coherence map: first the dominant speaker is localized; then the map is modified by compensating for the effects due to the first speaker and the position of the second speaker is detected. Simulations were carried out to show how an appropriate analysis of OGCF and GCF maps allows one to localize both speakers. Experiments proved the effectiveness of the proposed solution in a linear microphone array set up.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\A9NE747T\\Brutti et al. - 2008 - Localization of multiple speakers based on a two s.pdf;C\:\\Users\\sauli\\Zotero\\storage\\9CYN6YX6\\4518618.html},
  keywords = {Acoustic devices,acoustic signal detection,Active noise reduction,Ambient intelligence,Analytical models,coherence map,Computerized monitoring,distant-talking control,global coherence field,Humans,linear microphone array,Loudspeakers,microphone array,microphone arrays,Microphone arrays,multiple speaker localization,multiple speakers,oriented global coherence field,speaker localization,speaker recognition,Time difference of arrival,two step acoustic map analysis,Working environment noise}
}

@inproceedings{bruttiLocalizationMultipleSpeakers2008b,
  title = {Localization of Multiple Speakers Based on a Two Step Acoustic Map Analysis},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Brutti, A. and Omologo, M. and Svaizer, P.},
  date = {2008-03},
  pages = {4349--4352},
  doi = {10.1109/ICASSP.2008.4518618},
  abstract = {An interface for distant-talking control of home devices requires the possibility of identifying the positions of multiple users. Acoustic maps, based either on global coherence field (GCF) or oriented global coherence field (OGCF), have already been exploited successfully to determine position and head orientation of a single speaker. This paper proposes a new method using acoustic maps to deal with the case of two simultaneous speakers. The method is based on a two step analysis of a coherence map: first the dominant speaker is localized; then the map is modified by compensating for the effects due to the first speaker and the position of the second speaker is detected. Simulations were carried out to show how an appropriate analysis of OGCF and GCF maps allows one to localize both speakers. Experiments proved the effectiveness of the proposed solution in a linear microphone array set up.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\M6IZVPER\\4518618.html},
  keywords = {Acoustic devices,acoustic signal detection,Active noise reduction,Ambient intelligence,Analytical models,coherence map,Computerized monitoring,distant-talking control,global coherence field,Humans,linear microphone array,Loudspeakers,microphone array,microphone arrays,multiple speaker localization,multiple speakers,oriented global coherence field,speaker localization,speaker recognition,Time difference of arrival,two step acoustic map analysis,Working environment noise}
}

@article{bruttiOrientedGlobalCoherence2005,
  title = {Oriented {{Global Coherence Field}} for the {{Estimation}} of the {{Head Orientation}} in {{Smart Rooms Equipped}} with {{Distributed Microphone Arrays}}},
  author = {Brutti, Alessio and Omologo, Maurizio and Svaizer, Piergiorgio},
  date = {2005},
  pages = {4},
  abstract = {This paper proposes a new method for estimating the talker’s head orientation in a smart room equipped with microphone arrays. The acoustic processing is based on the use of a coherence measure derived from the Cross-power spectrum phase analysis, commonly used for speaker localization and tracking purposes. An Oriented Global Coherence Field function is then introduced to assign to a given point in space different scores according to the possible orientation of the acoustic source.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7AFHMPYN\\Brutti et al. - 2005 - Oriented Global Coherence Field for the Estimation.pdf},
  langid = {english}
}

@inproceedings{buchnerSimultaneousLocalizationMultiple2005,
  title = {Simultaneous Localization of Multiple Sound Sources Using Blind Adaptive {{MIMO}} Filtering},
  booktitle = {Proceedings. ({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  author = {Buchner, H. and Aichner, R. and Stenglein, J. and Teutsch, H. and Kellennann, W.},
  date = {2005-03},
  volume = {3},
  pages = {iii/97-iii100 Vol. 3},
  doi = {10.1109/ICASSP.2005.1415655},
  abstract = {Blind adaptive filtering for time delay of arrival (TDOA) estimation is a very powerful method for acoustic source localization in reverberant environments with broadband signals like speech. Based on a recently presented generic framework for blind signal processing for convolutive mixtures, called TRINICON, we present a TDOA estimation method for simultaneous multidimensional localization of multiple sources. Moreover, an interesting link to the known single-input multiple-output (SIMO)-based adaptive eigenvalue decomposition (AED) method is shown. We evaluate the novel multiple-input multiple-output (MIMO)-based approach and compare it with the known SIMO-based method in a reverberant acoustic environment using reference data of the positions obtained from infrared sensors. The results show that the new approach is very robust against reverberation and background noise.},
  eventtitle = {Proceedings. ({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BXE54TVC\\Buchner et al. - 2005 - Simultaneous localization of multiple sound source.pdf;C\:\\Users\\sauli\\Zotero\\storage\\468ZB8HS\\1415655.html},
  keywords = {acoustic signal processing,adaptive filters,Adaptive signal processing,audio signal processing,background noise robustness,blind adaptive MIMO filtering,blind source separation,broadband speech signals,convolutive mixtures,Delay effects,Delay estimation,Eigenvalues and eigenfunctions,Filtering,MIMO,MIMO systems,multidimensional localization,Multidimensional signal processing,reverberant environments,reverberation,SIMO-based adaptive eigenvalue decomposition,simultaneous multiple sound source localization,Speech,TDOA estimation,time delay of arrival estimation}
}

@inproceedings{buchnerTRINICONVersatileFramework2004,
  title = {{{TRINICON}}: A Versatile Framework for Multichannel Blind Signal Processing},
  shorttitle = {{{TRINICON}}},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Buchner, H. and Aichner, R. and Kellermann, W.},
  date = {2004-05},
  volume = {3},
  pages = {iii-889-92 vol.3},
  doi = {10.1109/ICASSP.2004.1326688},
  abstract = {In this paper we present a framework for multichannel blind signal processing for convolutive mixtures, such as blind source separation (BSS) and multichannel blind deconvolution (MCBD). It is based on the use of multivariate pdf and a compact matrix notation which considerably simplifies the representation and handling of the algorithms. By introducing these techniques into an information theoretic cost function, we can exploit the three fundamental signal properties nonwhiteness, nongaussianity, and nonstationarity. This results in a versatile tool that we call TRINICON (Triple-N ICA for convolutive mixtures). Both, links to popular algorithms and several novel algorithms follow from the general approach. In particular, we introduce a new concept of multichannel blind partial deconvolution (MCBPD) for speech which prevents a complete whitening of the output signals, i.e., the vocal tract is excluded from the equalization. This is especially interesting for automatic speech recognition applications. Moreover, we show results for BSS using multivariate spherically invariant random processes (SIRP) to efficiently model speech, and show how the approach carries over to MCBPD. These concepts are also suitable for an efficient implementation in the frequency domain by using a rigorous broadband derivation avoiding the internal permutation problem and circularity effects.},
  eventtitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3NMP3H6S\\Buchner et al. - 2004 - TRINICON a versatile framework for multichannel b.pdf;C\:\\Users\\sauli\\Zotero\\storage\\AIXPXWUB\\1326688.html},
  keywords = {automatic speech recognition,Automatic speech recognition,Blind equalizers,blind source separation,Blind source separation,broadband derivation,BSS,compact matrix notation,convolution,convolutive mixtures,Cost function,deconvolution,Deconvolution,Independent component analysis,information theoretic cost function,matrix algebra,MCBPD,multichannel blind partial deconvolution,multichannel blind signal processing,multivariate pdf,multivariate spherically invariant random processes,nongaussianity,nonstationarity,nonwhiteness,probability,random processes,Random processes,Signal processing,Signal processing algorithms,SIRP,Source separation,speech recognition,TRINICON,Triple-N ICA for convolutive mixtures}
}

@article{BuildingAutoencodersKeras,
  title = {Building {{Autoencoders}} in {{Keras}}},
  url = {https://blog.keras.io/building-autoencoders-in-keras.html},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HCYJ7VJ8\\building-autoencoders-in-keras.html}
}

@online{BuildingAutoencodersKerasa,
  title = {Building {{Autoencoders}} in {{Keras}}},
  url = {https://blog.keras.io/building-autoencoders-in-keras.html},
  urldate = {2017-02-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GHTKQM8V\\building-autoencoders-in-keras.html}
}

@inproceedings{buiMethodEstimatingDirection2018,
  title = {Method of {{Estimating Direction}} of {{Arrival}} of {{Sound Source}} for {{Monaural Hearing Based}} on {{Temporal Modulation Perception}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bui, N. K. and Morikawa, D. and Unoki, M.},
  date = {2018-04},
  pages = {5014--5018},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461359},
  abstract = {Although humans are capable of using monaural and modulation cues for sound localization, it is not yet clear how they can use that information to estimate the direction of arrival (DOA) of a sound source in 3D space. Our previous study revealed that the head-related modulation transfer function (HR-MTF) contains significant trends and features, which can be used for DOA estimation. This paper proposes a method of estimating the DOA in a 3D space by using the monaural modulation spectrum (MMS), based on the concept of modulation transfer function (MTF) and auditory perception of temporal modulation. We carried out over 51, 840 simulations with several signal types and multiple subjects to simultaneously estimate the azimuth and the elevation of an incoming sound source. The root mean square error (RMSE) was derived to evaluate the accuracy of monaural DOA estimates. Our results indicated that the proposed method could adequately estimate the DOA in 3D space with an overall mean RMSE of 21.9 degrees.},
  keywords = {*****,3D space,acoustic generators,acoustic radiators,acoustic signal processing,auditory perception,Auditory system,Azimuth,Direction of arrival,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,Estimation,Frequency modulation,head-related modulation transfer function,hearing,HR-MTF,incoming sound source,mean square error methods,modulation cues,modulation spectra,monaural DOA estimation,monaural hearing,monaural modulation spectrum,optical transfer function,RMSE,root mean square error,sound localization,temporal modulation,temporal modulation perception,Three-dimensional displays}
}

@inproceedings{buiNeuralGraphLearning2018,
  title = {Neural {{Graph Learning}}: {{Training Neural Networks Using Graphs}}},
  shorttitle = {Neural {{Graph Learning}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}} - {{WSDM}} '18},
  author = {Bui, Thang D. and Ravi, Sujith and Ramavajjala, Vivek},
  date = {2018},
  pages = {64--71},
  publisher = {{ACM Press}},
  location = {{Marina Del Rey, CA, USA}},
  doi = {10.1145/3159652.3159731},
  url = {http://dl.acm.org/citation.cfm?doid=3159652.3159731},
  urldate = {2019-10-28},
  abstract = {Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural networks, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training framework with a graph-regularised objective, namely Neural Graph Machines, that can combine the power of neural networks and label propagation. This work generalises previous literature on graph-augmented training of neural networks, enabling it to be applied to multiple neural architectures (Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs, with a runtime that is linear in the number of edges. The proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks (multi-label classification on social graphs, news categorization, document classification and semantic intent classification), with multiple forms of graph inputs (including graphs with and without node-level features) and using different types of neural networks.},
  eventtitle = {The {{Eleventh ACM International Conference}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\E8LDIJEI\\Bui et al. - 2018 - Neural Graph Learning Training Neural Networks Us.pdf},
  isbn = {978-1-4503-5581-0},
  langid = {english}
}

@inproceedings{buiNeuralGraphLearning2018a,
  title = {Neural {{Graph Learning}}: {{Training Neural Networks Using Graphs}}},
  shorttitle = {Neural {{Graph Learning}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}} - {{WSDM}} '18},
  author = {Bui, Thang D. and Ravi, Sujith and Ramavajjala, Vivek},
  date = {2018},
  pages = {64--71},
  publisher = {{ACM Press}},
  location = {{Marina Del Rey, CA, USA}},
  doi = {10.1145/3159652.3159731},
  url = {http://dl.acm.org/citation.cfm?doid=3159652.3159731},
  urldate = {2019-10-24},
  abstract = {Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural networks, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training framework with a graph-regularised objective, namely Neural Graph Machines, that can combine the power of neural networks and label propagation. This work generalises previous literature on graph-augmented training of neural networks, enabling it to be applied to multiple neural architectures (Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs, with a runtime that is linear in the number of edges. The proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks (multi-label classification on social graphs, news categorization, document classification and semantic intent classification), with multiple forms of graph inputs (including graphs with and without node-level features) and using different types of neural networks.},
  eventtitle = {The {{Eleventh ACM International Conference}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XQG6PHFK\\Bui et al. - 2018 - Neural Graph Learning Training Neural Networks Us.pdf},
  isbn = {978-1-4503-5581-0},
  langid = {english}
}

@article{cakirConvolutionalRecurrentNeural2017,
  title = {Convolutional {{Recurrent Neural Networks}} for {{Polyphonic Sound Event Detection}}},
  author = {Çakır, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
  date = {2017-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  pages = {1291--1303},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2017.2690575},
  url = {http://arxiv.org/abs/1702.06286},
  abstract = {Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.},
  archivePrefix = {arXiv},
  eprint = {1702.06286},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FWBH4BCT\\Çakır et al. - 2017 - Convolutional Recurrent Neural Networks for Polyph.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SEA6SQPC\\1702.html},
  keywords = {Computer Science - Learning,Computer Science - Sound},
  number = {6}
}

@article{cancliniRobustLowComplexitySource2015,
  title = {A {{Robust}} and {{Low}}-{{Complexity Source Localization Algorithm}} for {{Asynchronous Distributed Microphone Networks}}},
  author = {Canclini, A. and Bestagini, P. and Antonacci, F. and Compagnoni, M. and Sarti, A. and Tubaro, S.},
  date = {2015-10},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {23},
  pages = {1563--1575},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2015.2439040},
  abstract = {In this paper, we propose a robust and low-complexity acoustic source localization technique based on time differences of arrival (TDOA), which addresses the scenario of distributed sensor networks in 3D environments. Network nodes are assumed to be unsynchronized, i.e., TDOAs between microphones belonging to different nodes are not available. We begin with showing how to select feasible TDOAs for each sensor node, exploiting both geometrical considerations and a characterization of the overall generalized cross correlation (GCC) shape. We then show how to localize sources in the space-range reference frame, where TDOA measurements have a clear geometrical interpretation that can be fruitfully used in the scenario of unsynchronized sensors. In this framework, in fact, the source corresponds to the apex of a hypercone passing through points described by the sole microphone positions and TDOA measurements. The localization problem is therefore approached as a hypercone fitting problem. Finally, in order to improve the robustness of the estimate, we include an outlier detection procedure based on the evaluation of the hypercone fitting residuals. A refinement of source location estimate is then performed ignoring the contributions coming from outlier measurements. A set of simulations shows the performance of individual blocks of the system, with particular focus on the effect of TDOA selection on source localization and refinement steps. Experiments on real data validate the localization algorithm in an everyday scenario, proving that good accuracy can be obtained while saving computational cost in comparison with state-of-the-art techniques.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PCNDVXDV\\7114217.html},
  keywords = {acoustic signal processing,acoustic source localization,asynchronous distributed microphone networks,Cost function,distributed acosutic sensor networks,distributed sensor networks,hypercone fitting problem,low-complexity source localization algorithm technique,microphone arrays,Microphones,outlier detection procedure,Position measurement,reverberation,sensor fusion,space-range reference frame,TDOA measurements,Three-dimensional displays,time difference of arrival (TDOA) selection,time differences of arrival,time-of-arrival estimation},
  number = {10}
}

@article{carabias-ortiMultichannelBlindSound2018,
  title = {Multichannel {{Blind Sound Source Separation Using Spatial Covariance Model With Level}} and {{Time Differences}} and {{Nonnegative Matrix Factorization}}},
  author = {Carabias-Orti, J. J. and Nikunen, J. and Virtanen, T. and Vera-Candeas, P.},
  date = {2018-09},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {1512--1527},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2830105},
  abstract = {This paper presents an algorithm for multichannel sound source separation using explicit modeling of level and time differences in source spatial covariance matrices (SCM). We propose a novel SCM model in which the spatial properties are modeled by the weighted sum of direction of arrival (DOA) kernels. DOA kernels are obtained as the combination of phase and level difference covariance matrices representing both time and level differences between microphones for a grid of predefined source directions. The proposed SCM model is combined with the NMF model for the magnitude spectrograms. Opposite to other SCM models in the literature, in this work, source localization is implicitly defined in the model and estimated during the signal factorization. Therefore, no localization preprocessing is required. Parameters are estimated using complex-valued nonnegative matrix factorization with both Euclidean distance and Itakura-Saito divergence. Separation performance of the proposed system is evaluated using the two-channel SiSEC development dataset and four channels signals recorded in a regular room with moderate reverberation. Finally, a comparison to other state-of-the-art methods is performed, showing better achieved separation performance in terms of SIR and perceptual measures.},
  keywords = {achieved separation performance,audio signal processing,blind source separation,complex-valued nonnegative matrix factorization,covariance matrices,Covariance matrices,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,direction-of-arrival kernels,DOA kernels,Euclidean distance,explicit modeling,interaural level difference,interaural time difference,Itakura-Saito divergence,Kernel,level difference covariance matrices,level differences,magnitude spectrograms,matrix decomposition,microphones,Microphones,multichannel blind sound source separation,Multichannel source separation,NMF model,non-negative matrix factorization,phase difference covariance matrices,predefined source directions,reverberation,SCM model,source localization,Source separation,source spatial covariance matrices,spatial covariance model,spatial properties,Spectrogram,time differences,Time-frequency analysis,two-channel SiSEC development dataset},
  number = {9}
}

@article{carabias-ortiMultichannelBlindSound2018a,
  title = {Multichannel {{Blind Sound Source Separation Using Spatial Covariance Model With Level}} and {{Time Differences}} and {{Nonnegative Matrix Factorization}}},
  author = {Carabias-Orti, Julio Jose and Nikunen, Joonas and Virtanen, Tuomas and Vera-Candeas, Pedro},
  date = {2018-09},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1512-1527\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2830105\}},
  abstract = {This paper presents an algorithm for multichannel sound source separation using explicit modeling of level and time differences in source spatial covariance matrices (SCM). We propose a novel SCM model in which the spatial properties are modeled by the weighted sum of direction of arrival (DOA) kernels. DOA kernels are obtained as the combination of phase and level difference covariance matrices representing both time and level differences between microphones for a grid of predefined source directions. The proposed SCM model is combined with the NMF model for the magnitude spectrograms. Opposite to other SCM models in the literature, in this work, source localization is implicitly defined in the model and estimated during the signal factorization. Therefore, no localization preprocessing is required. Parameters are estimated using complex-valued nonnegative matrix factorization with both Euclidean distance and Itakura-Saito divergence. Separation performance of the proposed system is evaluated using the two-channel SiSEC development dataset and four channels signals recorded in a regular room with moderate reverberation. Finally, a comparison to other state-of-the-art methods is performed, showing better achieved separation performance in terms of SIR and perceptual measures.},
  affiliation = {Carabias-Orti, JJ (Reprint Author), Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland. Carabias-Orti, Julio Jose; Nikunen, Joonas; Virtanen, Tuomas, Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland. Vera-Candeas, Pedro, Univ Jaen, Dept Telecommun Engn, Jaen 23071, Spain.},
  author-email = {carabiasjulio@gmail.com joonas.nikunen@tut.fi tuomas.virtanen@tut.fi pvera@ujaen.es},
  cited-references = {Arberet S, 2010, 2010 10th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA 2010), P1, DOI 10.1109/ISSPA.2010.5605570. Canadas-Quesada FJ, 2016, DIGIT SIGNAL PROCESS, V50, P240, DOI 10.1016/j.dsp.2016.01.004. Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250. DiBiase J. H., 2001, MICROPHONE ARRAYS SI. Emiya V, 2011, IEEE T AUDIO SPEECH, V19, P2046, DOI 10.1109/TASL.2011.2109381. Ewert S, 2011, INT CONF ACOUST SPEE, P385. Fevotte C., 2010, P 7 INT S COMP MUS M, P102. Fevotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771. FitzGerald D., 2005, IEE Irish Signals and Systems Conference 2005, P8, DOI 10.1049/cp:20050279. Gannot S, 2017, IEEE-ACM T AUDIO SPE, V25, P692, DOI 10.1109/TASLP.2016.2647702. Huang PS, 2012, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2012.6287816. IKRAM MZ, 2002, ACOUST SPEECH SIG PR, P881. Itoyama K., 2008, P INT C MUS INF RETR, P133. JOURJINE A, 2000, ACOUST SPEECH SIG PR, P2985. Kameoka H, 2009, INT CONF ACOUST SPEE, P3437, DOI 10.1109/ICASSP.2009.4960364. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. Marro C, 1998, IEEE T SPEECH AUDI P, V6, P240, DOI 10.1109/89.668818. Mitsufuji Y, 2013, INT CONF ACOUST SPEE, P71, DOI 10.1109/ICASSP.2013.6637611. Murata N., 2016, P IEEE INT C AC SPEE, P395. Nesta F, 2008, MACHINE LEARN SIGN P, P43, DOI 10.1109/MLSP.2008.4685453. Nikunen Joonas, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6677, DOI 10.1109/ICASSP.2014.6854892. Nikunen J, 2014, IEEE-ACM T AUDIO SPE, V22, P727, DOI 10.1109/TASLP.2014.2303576. Nugraha AA, 2016, EUR SIGNAL PR CONF, P1748, DOI 10.1109/EUSIPCO.2016.7760548. Nugraha AA, 2016, IEEE-ACM T AUDIO SPE, V24, P1652, DOI 10.1109/TASLP.2016.2580946. Ozerov A, 2010, IEEE T AUDIO SPEECH, V18, P550, DOI 10.1109/TASL.2009.2031510. Parry RM, 2006, LECT NOTES COMPUT SC, V3889, P666. Raczynski S., 2007, P INT C MUS INF RETR, P381. Rodriguez-Serrano FJ, 2016, INT CONF ACOUST SPEE, P61, DOI 10.1109/ICASSP.2016.7471637. Sawada H, 2007, IEEE T AUDIO SPEECH, V15, P1592, DOI 10.1109/TASL.2007.899218. Sawada H, 2013, IEEE T AUDIO SPEECH, V21, P971, DOI 10.1109/TASL.2013.2239990. Sawada H, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P153, DOI 10.1109/ASPAA.2011.6082275. Sawada H, 2011, IEEE T AUDIO SPEECH, V19, P516, DOI 10.1109/TASL.2010.2051355. Tashev I., 2009, SOUND CAPTURE PROCES. Thiemann J., 2013, P INT WORKSH MACH LE, P1. Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005. VINCENT E, 2007, PROC INT CONF IND, V4666, P552. Vincent E, 2012, SIGNAL PROCESS, V92, P1928, DOI 10.1016/j.sigpro.2011.10.007. Woodruff J., 2006, P INT C MUS INF RETR, P314. Yin JH, 2011, IEEE T SIGNAL PROCES, V59, P4489, DOI 10.1109/TSP.2011.2158425.},
  da = {2018-10-18},
  doc-delivery-number = {GH4KD},
  funding-acknowledgement = {Academy of Finland [290190]},
  funding-text = {This work was supported by the Academy of Finland project number 290190.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Multichannel source separation,direction of arrival estimation),interaural  time difference,interaural level difference,non-negative matrix  factorization,spatial covariance model},
  keywords-plus = {AUDIO SOURCE SEPARATION; PERMUTATION ALIGNMENT; SIGNAL SEPARATION; PERSPECTIVE; MIXTURES; ARRAYS},
  langid = {english},
  number = {9},
  number-of-cited-references = {39},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000433371500004},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@inproceedings{carlettaAMIMeetingCorpus2006,
  title = {The {{AMI Meeting Corpus}}: {{A Pre}}-Announcement},
  shorttitle = {The {{AMI Meeting Corpus}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Machine Learning}} for {{Multimodal Interaction}}},
  author = {Carletta, Jean and and {others}},
  date = {2006},
  pages = {28--39},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11677482_3},
  url = {http://dx.doi.org/10.1007/11677482_3},
  urldate = {2019-04-24},
  abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QKR4AE3R\\Carletta et al. - 2006 - The AMI Meeting Corpus A Pre-announcement.pdf},
  isbn = {978-3-540-32549-9},
  options = {useprefix=true},
  series = {{{MLMI}}'05},
  venue = {Edinburgh, UK}
}

@inproceedings{carlettaj.etal.AMIMeetingCorpus2006,
  title = {The {{AMI Meeting Corpus}}: {{A Pre}}-Announcement},
  shorttitle = {The {{AMI Meeting Corpus}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Machine Learning}} for {{Multimodal Interaction}}},
  author = {Carletta, J. et al.},
  date = {2006},
  pages = {28--39},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11677482_3},
  url = {http://dx.doi.org/10.1007/11677482_3},
  urldate = {2019-04-24},
  abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\V3FH7XJM\\Carletta et al. - 2006 - The AMI Meeting Corpus A Pre-announcement.pdf},
  isbn = {978-3-540-32549-9},
  series = {{{MLMI}}'05},
  venue = {Edinburgh, UK}
}

@inproceedings{carneal_acoustic_2007,
  title = {Acoustic {{Source Detection}}, {{Characterization}}, and {{Localization}} from {{Diffracting Microphone Array}}},
  author = {Carneal, James and Chapin, William L. and Johnson, Martin and Roginska, Agnieszka},
  date = {2007-03-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=13937},
  urldate = {2017-01-10},
  abstract = {The diffracting microphone array and node processor of a networked acoustic source localization system shall be demonstrated. The node processor is capable of detecting, classifying, and estimating a bearing on the top two signals above the noise in an acoustic environment utilizing less than a dozen microphone elements on a diffracting array.},
  eventtitle = {Audio {{Engineering Society Conference}}: 30th {{International Conference}}: {{Intelligent Audio Environments}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U4T6NIK3\\Acoustic Source Detection, Characterization, and Localization from Diffracting Microphone Array.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EKXT2ET6\\browse.html},
  langid = {english}
}

@article{carterExperimentsHandwritingNeural2016,
  title = {Experiments in {{Handwriting}} with a {{Neural Network}}},
  author = {Carter, Shan and Ha, David and Johnson, Ian and Olah, Chris},
  date = {2016-12-06},
  journaltitle = {Distill},
  url = {http://distill.pub/2016/handwriting/},
  urldate = {2017-01-12},
  abstract = {Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8KHJGZXQ\\handwriting.html}
}

@inproceedings{cassidyDynamicRangeCompression2004,
  title = {Dynamic Range Compression of Audio Signals Consistent with Recent Time-Varying Loudness Models},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Cassidy, R. J.},
  date = {2004-05},
  volume = {4},
  pages = {iv-213-iv-216 vol.4},
  doi = {10.1109/ICASSP.2004.1326801},
  abstract = {Dynamic range compression may be used to increase the volume of the softer passages of an audio signal relative to its louder portions, thus making the signal better suited to transmission through, or storage on, a given medium. The level-detection characteristics of typical contemporary dynamic range compressors are analyzed and investigated, thus revealing the shortcomings of such models in the light of knowledge about steady-state and time-varying loudness as perceived by the human auditory system. The design of an equal-loudness filter, desired to improve the steady-state properties of compressor level detection, is presented. Finally, the time-varying properties of the level detection scheme presented, configured via attack and release times, are tuned to provide optimal correspondence with a recently proposed model of time-varying loudness.},
  eventtitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9HQRGHQT\\Cassidy - 2004 - Dynamic range compression of audio signals consist.pdf;C\:\\Users\\sauli\\Zotero\\storage\\JJCIUSZG\\figures.html},
  keywords = {attack times,audio coding,audio signals,Auditory system,Compressors,Dynamic range,dynamic range compression,equal-loudness filter,filtering theory,Filters,hearing,human auditory system,Humans,IIR filter,IIR filters,level-detection characteristics,loudness,release time,signal detection,signal storage,signal transmission,Speech,Steady-state,steady-state loudness,Telephony,Time varying systems,time-varying loudness models,time-varying systems,Topology}
}

@online{center_for_history_and_new_media_zotero_????,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  url = {http://zotero.org/support/quick_start_guide}
}

@online{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  url = {http://zotero.org/support/quick_start_guide}
}

@article{cerezuelaescuderoRealtimeNeuroinspiredSound2018,
  title = {Real-Time Neuro-Inspired Sound Source Localization and Tracking Architecture Applied to a Robotic Platform},
  author = {Cerezuela Escudero, Elena and Perez Pena, Fernando and Paz Vicente, Rafael and Jimenez-Fernandez, Angel and Jimenez Moreno, Gabriel and Morgado-Estevez, Arturo},
  date = {2018-03-29},
  journaltitle = {NEUROCOMPUTING},
  volume = {283},
  pages = {\{129-139\}},
  publisher = {ELSEVIER SCIENCE BV},
  location = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
  issn = {0925-2312},
  doi = {\{10.1016/j.neucom.2017.12.041\}},
  abstract = {This paper proposes a real-time sound source localization and tracking architecture based on the ability of the mammalian auditory system using the interaural intensity difference. We used an innovative binaural Neuromorphic Auditory Sensor to obtain spike rates similar to those generated by the inner hair cells of the human auditory system. The design of the component that obtains the interaural intensity difference is inspired by the lateral superior olive. The spike stream that represents the IID is used to turn a robotic platform towards the sound source direction. The architecture was implemented on FPGA devices using general purpose FPGA resources and was tested with pure tones (1-kHz, 2.5-kHz and 5-kHz sounds) with an average error of 2.32 degrees. Our architecture demonstrates a potential practical application of sound localization for robots, and can be used to test paradigms for sound localization in the mammalian brain. (c) 2017 Elsevier B.V. All rights reserved.},
  affiliation = {Escudero, EC (Reprint Author), Univ Seville, Robot & Comp Technol Lab RTC, ETSI Informat, Avd Reina Mercedes S-N, E-41012 Seville, Spain. Cerezuela Escudero, Elena; Paz Vicente, Rafael; Jimenez-Fernandez, Angel; Jimenez Moreno, Gabriel, Univ Seville, Robot & Comp Technol Lab RTC, ETSI Informat, Avd Reina Mercedes S-N, E-41012 Seville, Spain. Perez Pena, Fernando; Morgado-Estevez, Arturo, Univ Cadiz, Appl Robot Res Lab, Fac Engn, Avda Univ 10, Cadiz 11519, Spain.},
  author-email = {ecerezuela@us.es},
  cited-references = {Birchfield S. T., P 2005 IEEE INT C AC, V4, piv. Cerezuela-Escudero E., P 12 INT WORK C ART, P179. Cerezuela-Escudero E., 2015, P INT JOINT C NEUR N, P1. Cerezuela-Escudero E, 2016, LECT NOTES COMPUT SC, V9887, P363, DOI 10.1007/978-3-319-44781-0_43. Dominguez-Morales JP, 2017, NEUROCOMPUTING, V237, P418, DOI 10.1016/j.neucom.2016.12.046. Dominguez-Morales M, 2011, LECT NOTES COMPUT SC, V6792, P389, DOI 10.1007/978-3-642-21738-8_50. FEDDERSEN WE, 1957, J ACOUST SOC AM, V29, P988, DOI 10.1121/1.1909356. Fernandez A., 2010, THESIS. Gomez-Rodriguez F, 2005, LECT NOTES COMPUT SC, V3512, P534. Grothe B, 2010, PHYSIOL REV, V90, P983, DOI 10.1152/physrev.00026.2009. Iwasa K., P INT JOINT C NEUR N, P902. JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495. Jimenez A., 2010, P 2010 INT JOINT C O, P1. Jimenez-Fernandez A., 2017, BINAURAL NEUROMORPHI, V28, P804. Jimenez-Fernandez A, 2012, SENSORS-BASEL, V12, P3831, DOI 10.3390/s120403831. Liu JD, 2010, NEUROCOMPUTING, V74, P129, DOI 10.1016/j.neucom.2009.10.030. Mackenzie J, 1997, IEEE SIGNAL PROC LET, V4, P39, DOI 10.1109/97.554467. McAlpine D, 2003, TRENDS NEUROSCI, V26, P347, DOI 10.1016/S0166-2236(03)00140-1. Nakadai K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1147. Okuno HG, 2003, LECT NOTES COMPUT SC, V2686, P118. Otani M, 2006, J ACOUST SOC AM, V119, P2589, DOI 10.1121/1.2191608. Park PKJ, 2013, IEEE ENG MED BIO, P5275, DOI 10.1109/EMBC.2013.6610739. Dominguez-Morales JP, 2016, LECT NOTES COMPUT SC, V9886, P45, DOI 10.1007/978-3-319-44778-0_6. Perez-Pena F, 2015, NEUROCOMPUTING, V149, P496, DOI 10.1016/j.neucom.2014.08.024. Rios-Navarro A, 2015, IEEE INT SYMP CIRC S, P1907, DOI 10.1109/ISCAS.2015.7169040. Seung Seob Yeom, 2007, 2007 International Conference on Control, Automation and Systems - ICCAS `07, P224. Simon HJ, 2005, J REHABIL RES DEV, V42, P117, DOI 10.1682/JRRD.2005.01.0021. Slaney M., 1993, TECHNICAL REPORT. Tamai Y, 2003, IEEE SENSOR, P1100, DOI 10.1109/ICSENS.2003.1279114. Tollin DJ, 2003, NEUROSCIENTIST, V9, P127, DOI 10.1177/1073858403252228. van Schaik A, 2009, INT CONF ACOUST SPEE, P2197, DOI 10.1109/ICASSP.2009.4960054. Wall JA, 2012, IEEE T NEUR NET LEAR, V23, P574, DOI 10.1109/TNNLS.2011.2178317. Wu W.-C., 2007, P INT C INT SYST APP, P1. YIN TCT, 2002, INTEGRATIVE FUNCTION, V15, P99.},
  da = {2018-10-18},
  doc-delivery-number = {FV9GT},
  eissn = {1872-8286},
  funding-acknowledgement = {Spanish grant COFNET [TEC2016-77785-P]; European Regional Development Fund},
  funding-text = {This work was supported by the Spanish grant (with support from the European Regional Development Fund) COFNET (TEC2016-77785-P).},
  journal-iso = {Neurocomputing},
  keywords = {(Sound localization,FPGA),Interaural intensity difference,Neuromorphic auditory sensor,Neurorobotics,Spike signal  processing},
  keywords-plus = {MODEL; MAMMALS},
  langid = {english},
  number-of-cited-references = {34},
  orcid-numbers = {Morgado-Estevez, Arturo/0000-0002-3639-3649 Jimenez Fernandez, Angel Francisco/0000-0003-3061-5922 Perez-Pena, Fernando/0000-0003-3586-2930 Cerezuela Escudero, Elena/0000-0003-0176-7863},
  research-areas = {Computer Science},
  researcherid-numbers = {Morgado-Estevez, Arturo/M-6750-2013},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000424896600013},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {15},
  web-of-science-categories = {Computer Science, Artificial Intelligence}
}

@article{chakrabartyBroadbandDOAEstimation2017,
  title = {Broadband {{DOA}} Estimation Using {{Convolutional}} Neural Networks Trained with Noise Signals},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A. P.},
  date = {2017-05-02},
  url = {http://arxiv.org/abs/1705.00919},
  urldate = {2018-10-18},
  abstract = {A convolution neural network (CNN) based classification method for broadband DOA estimation is proposed, where the phase component of the short-time Fourier transform coefficients of the received microphone signals are directly fed into the CNN and the features required for DOA estimation are learnt during training. Since only the phase component of the input is used, the CNN can be trained with synthesized noise signals, thereby making the preparation of the training data set easier compared to using speech signals. Through experimental evaluation, the ability of the proposed noise trained CNN framework to generalize to speech sources is demonstrated. In addition, the robustness of the system to noise, small perturbations in microphone positions, as well as its ability to adapt to different acoustic conditions is investigated using experiments with simulated and real data.},
  archivePrefix = {arXiv},
  eprint = {1705.00919},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N7GUPMQK\\Chakrabarty and Habets - 2017 - Broadband DOA estimation using Convolutional neura.pdf;C\:\\Users\\sauli\\Zotero\\storage\\X4NCCJK2\\1705.html},
  keywords = {Computer Science - Sound,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{chakrabartyBroadbandDoaEstimation2017,
  title = {Broadband Doa Estimation Using Convolutional Neural Networks Trained with Noise Signals},
  booktitle = {2017 {{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}} ({{WASPAA}})},
  author = {Chakrabarty, Soumitro and Habets, Emanuel A. P.},
  date = {2017-10},
  pages = {136--140},
  publisher = {{IEEE}},
  location = {{New Paltz, NY}},
  doi = {10.1109/WASPAA.2017.8170010},
  url = {http://ieeexplore.ieee.org/document/8170010/},
  urldate = {2019-08-19},
  eventtitle = {2017 {{IEEE Workshop}} on {{Applications}} of {{Signal Processing}} to {{Audio}} and {{Acoustics}} ({{WASPAA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7NVWNHM9\\Chakrabarty and Habets - 2017 - Broadband doa estimation using convolutional neura.pdf},
  isbn = {978-1-5386-1632-1},
  langid = {english}
}

@article{chakrabartyBroadbandDOAEstimation2017a,
  title = {Broadband {{DOA}} Estimation Using {{Convolutional}} Neural Networks Trained with Noise Signals},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A. P.},
  date = {2017-05-02},
  url = {http://arxiv.org/abs/1705.00919},
  urldate = {2019-08-19},
  abstract = {A convolution neural network (CNN) based classification method for broadband DOA estimation is proposed, where the phase component of the short-time Fourier transform coefficients of the received microphone signals are directly fed into the CNN and the features required for DOA estimation are learnt during training. Since only the phase component of the input is used, the CNN can be trained with synthesized noise signals, thereby making the preparation of the training data set easier compared to using speech signals. Through experimental evaluation, the ability of the proposed noise trained CNN framework to generalize to speech sources is demonstrated. In addition, the robustness of the system to noise, small perturbations in microphone positions, as well as its ability to adapt to different acoustic conditions is investigated using experiments with simulated and real data.},
  archivePrefix = {arXiv},
  eprint = {1705.00919},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HMXNJZ3F\\Chakrabarty_Habets_2017_Broadband DOA estimation using Convolutional neural networks trained with noise.pdf;C\:\\Users\\sauli\\Zotero\\storage\\XT9BMSGY\\1705.html},
  keywords = {COMMENTED,Computer Science - Sound,MUST READ,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chakrabartyMultiSpeakerDOAEstimation2019,
  title = {Multi-{{Speaker DOA Estimation Using Deep Convolutional Networks Trained}} with {{Noise Signals}}},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A. P.},
  date = {2019-03},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  shortjournal = {IEEE J. Sel. Top. Signal Process.},
  volume = {13},
  pages = {8--21},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2019.2901664},
  url = {http://arxiv.org/abs/1807.11722},
  urldate = {2019-08-21},
  abstract = {Supervised learning based methods for source localization, being data driven, can be adapted to different acoustic conditions via training and have been shown to be robust to adverse acoustic environments. In this paper, a convolutional neural network (CNN) based supervised learning method for estimating the direction-of-arrival (DOA) of multiple speakers is proposed. Multi-speaker DOA estimation is formulated as a multi-class multi-label classification problem, where the assignment of each DOA label to the input feature is treated as a separate binary classification problem. The phase component of the short-time Fourier transform (STFT) coefficients of the received microphone signals are directly fed into the CNN, and the features for DOA estimation are learnt during training. Utilizing the assumption of disjoint speaker activity in the STFT domain, a novel method is proposed to train the CNN with synthesized noise signals. Through experimental evaluation with both simulated and measured acoustic impulse responses, the ability of the proposed DOA estimation approach to adapt to unseen acoustic conditions and its robustness to unseen noise type is demonstrated. Through additional empirical investigation, it is also shown that with an array of M microphones our proposed framework yields the best localization performance with M-1 convolution layers. The ability of the proposed method to accurately localize speakers in a dynamic acoustic scenario with varying number of sources is also shown.},
  archivePrefix = {arXiv},
  eprint = {1807.11722},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FKN9P7P7\\Chakrabarty_Habets_2019_Multi-Speaker DOA Estimation Using Deep Convolutional Networks Trained with.pdf;C\:\\Users\\sauli\\Zotero\\storage\\LNZ9N54J\\1807.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  number = {1}
}

@article{chakrabartyMultiSpeakerLocalizationUsing,
  title = {Multi-{{Speaker Localization Using Convolutional Neural Network Trained}} with {{Noise}}},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A P},
  pages = {5},
  abstract = {The problem of multi-speaker localization is formulated as a multi-class multi-label classification problem, which is solved using a convolutional neural network (CNN) based source localization method. Utilizing the common assumption of disjoint speaker activities, we propose a novel method to train the CNN using synthesized noise signals. The proposed localization method is evaluated for two speakers and compared to a well-known steered response power method.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JSN2PCU4\\Chakrabarty and Habets - Multi-Speaker Localization Using Convolutional Neu.pdf},
  langid = {english}
}

@article{chakrabartyMultiSpeakerLocalizationUsinga,
  title = {Multi-{{Speaker Localization Using Convolutional Neural Network Trained}} with {{Noise}}},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A P},
  pages = {5},
  abstract = {The problem of multi-speaker localization is formulated as a multi-class multi-label classification problem, which is solved using a convolutional neural network (CNN) based source localization method. Utilizing the common assumption of disjoint speaker activities, we propose a novel method to train the CNN using synthesized noise signals. The proposed localization method is evaluated for two speakers and compared to a well-known steered response power method.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YKSDIBVY\\Chakrabarty and Habets - Multi-Speaker Localization Using Convolutional Neu.pdf},
  keywords = {*****,COMMENTED,MUST CITE: ANN SSL,MUST READ},
  langid = {english}
}

@article{chakrabartyMultiSpeakerLocalizationUsingb,
  title = {Multi-{{Speaker Localization Using Convolutional Neural Network Trained}} with {{Noise}}},
  author = {Chakrabarty, Soumitro and Habets, Emanuël A P},
  pages = {5},
  abstract = {The problem of multi-speaker localization is formulated as a multi-class multi-label classification problem, which is solved using a convolutional neural network (CNN) based source localization method. Utilizing the common assumption of disjoint speaker activities, we propose a novel method to train the CNN using synthesized noise signals. The proposed localization method is evaluated for two speakers and compared to a well-known steered response power method.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\26LFVX7D\\Chakrabarty and Habets - Multi-Speaker Localization Using Convolutional Neu.pdf},
  langid = {english}
}

@article{champagnePerformanceTimedelayEstimation1996,
  title = {Performance of Time-Delay Estimation in the Presence of Room Reverberation},
  author = {Champagne, B. and Bedard, S. and Stephenne, A.},
  date = {1996-03},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  shortjournal = {IEEE Trans. Speech Audio Process.},
  volume = {4},
  pages = {148--152},
  issn = {10636676},
  doi = {10.1109/89.486067},
  url = {http://ieeexplore.ieee.org/document/486067/},
  urldate = {2019-08-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JGX6GQ97\\Champagne et al. - 1996 - Performance of time-delay estimation in the presen.pdf},
  langid = {english},
  number = {2}
}

@article{chandler-wildeBoundaryElementMethods2007,
  title = {Boundary Element Methods for Acoustics},
  author = {Chandler-Wilde, Simon and Steve, L.},
  date = {2007},
  journaltitle = {Dept. of Math., Univ. of Reading},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5VEK7VHC\\Chandler-Wilde and Steve - 2007 - Boundary element methods for acoustics.pdf}
}

@inproceedings{chen_binaural_2007,
  title = {Binaural {{Sound Source Localization Based On Steered Beamformer With Spherical Scatterer}}},
  booktitle = {Audio {{Engineering Society Conference}}: 30th {{International Conference}}: {{Intelligent Audio Environments}}},
  author = {Chen, Xun and Hao, Xiaohui and Wu, Rongbin and Wu, Xihong and Zhao, Shuo},
  date = {2007-03-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=13912},
  urldate = {2017-01-10},
  abstract = {Inspired by human sound localization, this paper introduces a novel approach to achieving binaural sound source localization in the frontal azimuthal half-plane based on steered beamformer. Instead of using HRTFs, a rigid sphere, whose transfer functions can be calculated accurately, is introduced to simulate the head effect. Sub-band beamformer using both the time cue ITD and the intensity cue IID is designed to process sound scattered by the rigid sphere. In the multi-band processing, a...},
  eventtitle = {Audio {{Engineering Society Conference}}: 30th {{International Conference}}: {{Intelligent Audio Environments}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U4MFCK9X\\Binaural Sound Source Localization Based on Steered Beamformer with Spherica Scatterer.pdf;C\:\\Users\\sauli\\Zotero\\storage\\VWV5ZRRB\\browse.html},
  langid = {english}
}

@inproceedings{chenAcousticSourceLocalization2009,
  title = {Acoustic Source Localization Using {{LS}}-{{SVMs}} without Calibration of Microphone Arrays},
  booktitle = {2009 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Chen, Huawei and Ser, Wee},
  date = {2009-05},
  pages = {1863--1866},
  doi = {10.1109/ISCAS.2009.5118142},
  abstract = {Common assumptions of the conventional approaches to acoustic source localization are usually that the microphones used are ideal and that the locations of the microphones are also known a priori, which usually may not hold in practice. Therefore, the microphone arrays need to be calibrated carefully before use. However, it is not an easy task to calibrate microphone arrays perfectly. In this paper, we proposed an algorithm for acoustic source localization based on the least-squares support vector machines (LS-SVMs). The advantage of the proposed algorithm is that it requires no calibration of microphone arrays. The performance and effectiveness of the proposed method is demonstrated by simulation results and the real-data experiments.},
  eventtitle = {2009 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6BGQHE3W\\Chen and Ser - 2009 - Acoustic source localization using LS-SVMs without.pdf;C\:\\Users\\sauli\\Zotero\\storage\\MBXX5PBB\\5118142.html},
  keywords = {Acoustic applications,acoustic arrays,acoustic signal processing,acoustic source localization,Calibration,least squares approximations,least-squares support vector machines,LS-SVM,machine learning,microphone arrays,physics computing,Position measurement,Signal processing algorithms,support vector machines,Teleconferencing}
}

@article{chenAdaptiveBeamformingRecursive2008,
  title = {Adaptive {{Beamforming}} and {{Recursive DOA Estimation Using Frequency}}-{{Invariant Uniform Concentric Spherical Arrays}}},
  author = {Chen, H. H. and Chan, S. C. and Zhang, Z. G. and Ho, K. L.},
  date = {2008-11},
  journaltitle = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {55},
  pages = {3077--3089},
  issn = {1549-8328},
  doi = {10.1109/TCSI.2008.924126},
  abstract = {This paper proposes recursive adaptive beamforming and broadband 2D direction-of-arrival (DOA) estimation algorithms for uniform concentric spherical arrays (UCSAs) having nearly frequency-invariant (FI) characteristics. The basic principle of the FI-UCSA is to transform the received signals to the phase mode and remove the frequency dependency of individual phase modes through a digital beamforming network. Hence, the far-field pattern of the array is determined by a set of weights. Thanks to the FI characteristic, traditional narrowband adaptive beamforming algorithms such as minimum variance beamforming and the generalized sidelobe canceller method can be applied to the FI-UCSA. Simulation results show that the proposed adaptive FI-UCSA beamformer achieves a lower steady-state error and converges faster than the conventional tapped-delay line approach while requiring fewer adaptive coefficients. A new broadband 2-D DOA estimation algorithm using ESPRIT techniques for FI-UCSA is proposed to recursively estimate the DOAs of the moving targets. Simulation results show that the proposed DOA estimation algorithm achieves a satisfactory performance for slowly varying sources at low arithmetic complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9AJ83H4W\\Chen et al. - 2008 - Adaptive Beamforming and Recursive DOA Estimation .pdf;C\:\\Users\\sauli\\Zotero\\storage\\KNIFZXR5\\4498388.html},
  keywords = {Array processing,array signal processing,broadband 2-D direction-of-arrival (DOA) estimation,broadband 2-D DOA estimation,broadband 2D direction-of-arrival estimation algorithm,broadband adaptive beamforming,digital beamforming network,direction-of-arrival estimation,frequency invariant,frequency-invariant (FI),frequency-invariant uniform concentric spherical arrays,generalized sidelobe canceller method,low arithmetic complexity,minimum variance beamforming,nearly frequency-invariant,recursive adaptive beamforming,recursive DOA estimation,recursive estimation,steady-state error,subspace tracking,tapped-delay line approach,uniform concentric spherical array (UCSA),uniform concentric spherical arrays},
  number = {10}
}

@inproceedings{chenMultiplanesGeometricApproach2018,
  title = {A {{Multiplanes Geometric Approach}} for {{Sound Source Localization}} with {{TDOA}}},
  booktitle = {2018 {{International Conference}} on {{System Science}} and {{Engineering}} ({{ICSSE}})},
  author = {Chen, J. and Wang, J. and Sun, T.},
  date = {2018-06},
  pages = {1--5},
  issn = {2325-0925},
  doi = {10.1109/ICSSE.2018.8520107},
  abstract = {This study mitigates the effect of sampling errors that comes from sampling rate in reality when localizing the sound source. In general, the sampling errors may make sound source localization that uses multi-hyperboloids method, for example, time difference of arrival (TDOA), have wrong sound source estimation or do not work. Due to alleviate the wrong source estimation, a geometric approach that transfers from multi-hyperboloids to multiplanes for sound source localization is proposed. This study also added a discriminant for check of different TDOAs conditions. Because of the easy realization for embedded systems, the closely spaced microphone array was used in this study. From experiments, the propose method successful improve the accuracy and validity of the source estimation.},
  keywords = {Estimation,Mathematical model,Microphone arrays,Position measurement,Sensors,Shape}
}

@article{childersCepstrumGuideProcessing1977,
  title = {The Cepstrum: {{A}} Guide to Processing},
  shorttitle = {The Cepstrum},
  author = {Childers, D. G. and Skinner, D. P. and Kemerait, R. C.},
  date = {1977-10},
  journaltitle = {Proceedings of the IEEE},
  volume = {65},
  pages = {1428--1443},
  issn = {0018-9219},
  doi = {10.1109/PROC.1977.10747},
  abstract = {This paper is a pragmatic tutorial review of the cepstrum literature focusing on data processing. The power, complex, and phase cepstra are shown to be easily related to one another. Problems associated with phase unwrapping, linear phase components, spectrum notching, aliasing, oversampling, and extending the data sequence with zeros are discussed. The advantages and disadvantages of windowing the sampled data sequence, the log spectrum, and the complex cepstrum are presented. The influence of noise upon the data processing procedures is discussed throughout the paper, but is not thoroughly analyzed. The effects of various forms of liftering the cepstrum are described. The results obtained by applying whitening and trend removal techniques to the spectrum prior to the calculation of the cepstrum are discussed. We have attempted to synthesize the results, procedures, and information peculiar to the many fields that are finding cepstrum analysis useful. In particular we discuss the interpretation and processing of data in such areas as speech, seismology, and hydroacoustics. But we must caution the reader that the paper is heavily influenced by our own experiences; specific procedures that have been found useful in one field should not be considered as totally general to other fields. It is hoped that this review will be of value to those familiar with the field and reduce the time required for those wishing to become so.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6IR7PEA9\\1455016.html},
  keywords = {Cepstral analysis,Cepstrum,Data analysis,Data processing,Frequency estimation,Signal analysis,signal processing,Sonar detection,Speech synthesis,Wavelet analysis},
  number = {10}
}

@article{Childerscepstrumguideprocessing1977,
  title = {The {{Cepstrum}}: {{A Guide}} to {{Processing}}},
  shorttitle = {The {{Cepstrum}}},
  author = {Childers, D. G. and Skinner, D. P. and Kemerait, R. C.},
  date = {1977-10},
  journaltitle = {Proceedings of the IEEE},
  volume = {65},
  pages = {1428--1443},
  issn = {0018-9219},
  doi = {10.1109/PROC.1977.10747},
  abstract = {This paper is a pragmatic tutorial review of the cepstrum literature focusing on data processing. The power, complex, and phase cepstra are shown to be easily related to one another. Problems associated with phase unwrapping, linear phase components, spectrum notching, aliasing, oversampling, and extending the data sequence with zeros are discussed. The advantages and disadvantages of windowing the sampled data sequence, the log spectrum, and the complex cepstrum are presented. The influence of noise upon the data processing procedures is discussed throughout the paper, but is not thoroughly analyzed. The effects of various forms of liftering the cepstrum are described. The results obtained by applying whitening and trend removal techniques to the spectrum prior to the calculation of the cepstrum are discussed. We have attempted to synthesize the results, procedures, and information peculiar to the many fields that are finding cepstrum analysis useful. In particular we discuss the interpretation and processing of data in such areas as speech, seismology, and hydroacoustics. But we must caution the reader that the paper is heavily influenced by our own experiences; specific procedures that have been found useful in one field should not be considered as totally general to other fields. It is hoped that this review will be of value to those familiar with the field and reduce the time required for those wishing to become so.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7A3JKVYI\\1455016.html},
  keywords = {Cepstral analysis,Cepstrum,Data analysis,Data processing,Frequency estimation,Signal analysis,signal processing,Sonar detection,Speech synthesis,Wavelet analysis},
  number = {10}
}

@article{choiKapreOnGPUAudio2017,
  title = {Kapre: {{On}}-{{GPU Audio Preprocessing Layers}} for a {{Quick Implementation}} of {{Deep Neural Network Models}} with {{Keras}}},
  shorttitle = {Kapre},
  author = {Choi, Keunwoo and Joo, Deokjin and Kim, Juho},
  date = {2017-06-19},
  url = {http://arxiv.org/abs/1706.05781},
  urldate = {2018-04-02},
  abstract = {We introduce Kapre, Keras layers for audio and music signal preprocessing. Music research using deep neural networks requires a heavy and tedious preprocessing stage, for which audio processing parameters are often ignored in parameter optimisation. To solve this problem, Kapre implements time-frequency conversions, normalisation, and data augmentation as Keras layers. We report simple benchmark results, showing real-time on-GPU preprocessing adds a reasonable amount of computation.},
  archivePrefix = {arXiv},
  eprint = {1706.05781},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RYXNRFHK\\Choi et al. - 2017 - Kapre On-GPU Audio Preprocessing Layers for a Qui.pdf;C\:\\Users\\sauli\\Zotero\\storage\\H75IFCJ7\\1706.html},
  keywords = {Computer Science - Learning,Computer Science - Multimedia,Computer Science - Sound},
  primaryClass = {cs}
}

@incollection{christensen-dalsgaardDirectionalHearingNonmammalian2005,
  title = {Directional {{Hearing}} in {{Nonmammalian Tetrapods}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Christensen-Dalsgaard, Jakob},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {67--123},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_4},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_4},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KKR74HBJ\\0-387-28863-5_4.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{Christensen-DalsgaardDirectionalHearingNonmammalian2005,
  title = {Directional {{Hearing}} in {{Nonmammalian Tetrapods}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Christensen-Dalsgaard, Jakob},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {67--123},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_4},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8NB2V9LI\\0-387-28863-5_4.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@inproceedings{chun-guangliSupervisedIsomapExplicit2006,
  title = {Supervised {{Isomap}} with {{Explicit Mapping}}},
  booktitle = {First {{International Conference}} on {{Innovative Computing}}, {{Information}} and {{Control}} - {{Volume I}} ({{ICICIC}}'06)},
  author = {Chun-Guang Li and Jun Guo},
  date = {2006-08},
  volume = {3},
  pages = {345--348},
  issn = {null},
  doi = {10.1109/ICICIC.2006.530},
  abstract = {Isomap is one of the recently proposed manifold learning algorithms for nonlinear dimensionality reduction. However, Isomap not only suffers from a deficiency of no explicit mapping function, which is from high dimensional space to low dimensional space, but also does not employ the class information. In this paper, a supervised version of Isomap with explicit mapping, called SE-Isomap, is proposed. In SE-Isomap, geodesic distance matrix is calculated with respect to the class label information and multidimensional scaling (MDS) with explicit transformation is adopted instead of classical MDS used in Isomap. Thanks to the existence of explicit mapping and the use of class label information, SE-Isomap can be more easily used in pattern recognition than the original ones. Experimental results on two benchmark data sets demonstrated the performance of the presented method},
  eventtitle = {First {{International Conference}} on {{Innovative Computing}}, {{Information}} and {{Control}} - {{Volume I}} ({{ICICIC}}'06)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KNQBAYJJ\\Chun-Guang Li and Jun Guo - 2006 - Supervised Isomap with Explicit Mapping.pdf;C\:\\Users\\sauli\\Zotero\\storage\\KEJCMGIQ\\1692185.html},
  keywords = {class label information,differential geometry,Euclidean distance,explicit mapping,geodesic distance matrix,learning (artificial intelligence),Linear discriminant analysis,manifold learning algorithm,matrix algebra,multidimensional scaling,Multidimensional systems,nonlinear dimensionality reduction,pattern recognition,Pattern recognition,SE-Isomap,supervised Isomap}
}

@inproceedings{chun-guangliSupervisedIsomapExplicit2006a,
  title = {Supervised {{Isomap}} with {{Explicit Mapping}}},
  booktitle = {First {{International Conference}} on {{Innovative Computing}}, {{Information}} and {{Control}} - {{Volume I}} ({{ICICIC}}'06)},
  author = {Chun-Guang Li and Jun Guo},
  date = {2006-08},
  volume = {3},
  pages = {345--348},
  doi = {10.1109/ICICIC.2006.530},
  abstract = {Isomap is one of the recently proposed manifold learning algorithms for nonlinear dimensionality reduction. However, Isomap not only suffers from a deficiency of no explicit mapping function, which is from high dimensional space to low dimensional space, but also does not employ the class information. In this paper, a supervised version of Isomap with explicit mapping, called SE-Isomap, is proposed. In SE-Isomap, geodesic distance matrix is calculated with respect to the class label information and multidimensional scaling (MDS) with explicit transformation is adopted instead of classical MDS used in Isomap. Thanks to the existence of explicit mapping and the use of class label information, SE-Isomap can be more easily used in pattern recognition than the original ones. Experimental results on two benchmark data sets demonstrated the performance of the presented method},
  eventtitle = {First {{International Conference}} on {{Innovative Computing}}, {{Information}} and {{Control}} - {{Volume I}} ({{ICICIC}}'06)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HB9YZRRV\\Chun-Guang Li and Jun Guo - 2006 - Supervised Isomap with Explicit Mapping.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ST6YTPAD\\1692185.html},
  keywords = {class label information,differential geometry,Euclidean distance,explicit mapping,geodesic distance matrix,learning (artificial intelligence),Linear discriminant analysis,manifold learning algorithm,matrix algebra,multidimensional scaling,Multidimensional systems,nonlinear dimensionality reduction,pattern recognition,Pattern recognition,SE-Isomap,supervised Isomap}
}

@article{claraUS72Inventors,
  title = {({{US}}) (72) {{Inventors}}: {{YiWu}}, {{San Jose}}, {{CA}} ({{US}});{{Gabriel}}},
  author = {Clara, Santa and Choubassi, El and Jose, San},
  pages = {15},
  abstract = {Systems, apparatuses and methods to provide image data , augmented with related data , to be displayed on a mobile computing device are disclosed . An example mobile device includes a camera to provide images of a scene from different angles to a server, at least one sensor to sense a position and an orientation of the camera, and a screen to present augmented reality data over the scene based on the position and the orientation of the camera and on a three dimensional representation of the scene based on the images.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\45CKYHW7\\Clara et al. - (US) (72) Inventors YiWu, San Jose, CA (US)\;Gabri.pdf},
  langid = {english}
}

@inproceedings{claudioMultisourceLocalizationReverberant2000,
  title = {Multi-Source Localization in Reverberant Environments by {{ROOT}}-{{MUSIC}} and Clustering},
  booktitle = {2000 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00CH37100}})},
  author = {Claudio, E. D. Di and Parisi, R. and Orlandi, G.},
  date = {2000},
  volume = {2},
  pages = {II921-II924 vol.2},
  doi = {10.1109/ICASSP.2000.859111},
  abstract = {Localization of acoustic sources in reverberant environments by microphone arrays remains a challenging task in audio signal processing. As a matter of fact, most assumptions of commonly adopted models are not met in real applications. Moreover, in practical systems it is not convenient or possible to employ sophisticated and costly architectures, that require precise synchronization and fast data shuffling among sensors. In this paper, a new robust multi-step procedure for speaker localization in reverberant rooms is introduced and described. The new approach is based on a disturbed harmonics model of time delays in the frequency domain and employs the well-known ROOT-MUSIC algorithm, after a preliminary distributed processing of the received signals. Candidate source positions are then estimated by clustering of raw TDOA estimates. Main features of the proposed approach, compared to previous solutions, are the capability of tracking multiple speakers and the high accuracy of the closed form TDOA estimator},
  eventtitle = {2000 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00CH37100}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DF834TU9\\Claudio et al. - 2000 - Multi-source localization in reverberant environme.pdf;C\:\\Users\\sauli\\Zotero\\storage\\GXGT3IRF\\859111.html},
  keywords = {acoustic arrays,acoustic signal processing,acoustic sources,acoustic transducer arrays,architectural acoustics,array signal processing,audio signal processing,clustering,Clustering algorithms,Delay effects,delays,direction-of-arrival estimation,distributed processing,disturbed harmonics model,frequency domain,Frequency domain analysis,Frequency synchronization,microphone arrays,Microphones,multi-source localization,multi-step procedure,pattern clustering,reverberant environments,Robustness,ROOT-MUSIC,Sensor systems,speaker localization,TDOA estimates,time delay of arrival,time delays}
}

@inproceedings{cobos_localization_2011,
  title = {Localization of {{Multiple Speech Sources Using Distributed Microphones}}},
  author = {Cobos, Maximo and Marti, Amparo and Lopez, Jose J.},
  date = {2011-05-13},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15794},
  urldate = {2017-01-09},
  abstract = {Source localization is an important task in many speech processing systems. There are many microphone array techniques intended to provide accurate source localization, but their performance is severely affected by noise and reverberation. The Steered-Response Power Phase Transform (SRP-PHAT) algorithm has been shown to perform very robustly in adverse acoustic environments; however, its computational cost can be an issue. Recently, the authors presented a modified version of the SRP-PHAT...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q8FX3WX4\\Localization of Multiple Speech Sources Using Distributed Microphones.pdf;C\:\\Users\\sauli\\Zotero\\storage\\7PHRE5ZI\\browse.html}
}

@inproceedings{cobosLocalizationMultipleSpeech2011,
  title = {Localization of {{Multiple Speech Sources Using Distributed Microphones}}},
  booktitle = {Audio {{Engineering Society Convention}} 130},
  author = {Cobos, Maximo and Marti, Amparo and Lopez, Jose J.},
  date = {2011-05-13},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15794},
  urldate = {2017-01-09},
  abstract = {Source localization is an important task in many speech processing systems. There are many microphone array techniques intended to provide accurate source localization, but their performance is severely affected by noise and reverberation. The Steered-Response Power Phase Transform (SRP-PHAT) algorithm has been shown to perform very robustly in adverse acoustic environments; however, its computational cost can be an issue. Recently, the authors presented a modified version of the SRP-PHAT...},
  eventtitle = {Audio {{Engineering Society Convention}} 130},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BTTK5PIR\\Localization of Multiple Speech Sources Using Distributed Microphones.pdf;C\:\\Users\\sauli\\Zotero\\storage\\CGQUQNZB\\browse.html},
  langid = {english}
}

@article{cobosSurveySoundSource2017,
  title = {A {{Survey}} of {{Sound Source Localization Methods}} in {{Wireless Acoustic Sensor Networks}}},
  author = {Cobos, Maximo and Antonacci, Fabio and Alexandridis, Anastasios and Mouchtaris, Athanasios and Lee, Bowon},
  date = {2017},
  journaltitle = {Wireless Communications and Mobile Computing},
  shortjournal = {Wireless Communications and Mobile Computing},
  volume = {2017},
  pages = {1--24},
  issn = {1530-8669, 1530-8677},
  doi = {10.1155/2017/3956282},
  url = {https://www.hindawi.com/journals/wcmc/2017/3956282/},
  urldate = {2019-08-16},
  abstract = {Wireless acoustic sensor networks (WASNs) are formed by a distributed group of acoustic-sensing devices featuring audio playing and recording capabilities. Current mobile computing platforms offer great possibilities for the design of audio-related applications involving acoustic-sensing nodes. In this context, acoustic source localization is one of the application domains that have attracted the most attention of the research community along the last decades. In general terms, the localization of acoustic sources can be achieved by studying energy and temporal and/or directional features from the incoming sound at different microphones and using a suitable model that relates those features with the spatial location of the source (or sources) of interest. This paper reviews common approaches for source localization in WASNs that are focused on different types of acoustic features, namely, the energy of the incoming signals, their time of arrival (TOA) or time difference of arrival (TDOA), the direction of arrival (DOA), and the steered response power (SRP) resulting from combining multiple microphone signals. Additionally, we discuss methods not only aimed at localizing acoustic sources but also designed to locate the nodes themselves in the network. Finally, we discuss current challenges and frontiers in this field.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R3TTKJPE\\Cobos et al. - 2017 - A Survey of Sound Source Localization Methods in W.pdf},
  langid = {english}
}

@online{cobosSurveySoundSource2017a,
  title = {A {{Survey}} of {{Sound Source Localization Methods}} in {{Wireless Acoustic Sensor Networks}}},
  author = {Cobos, Maximo and Antonacci, Fabio and Alexandridis, Anastasios and Mouchtaris, Athanasios and Lee, Bowon},
  date = {2017},
  journaltitle = {Wireless Communications and Mobile Computing},
  volume = {2017},
  pages = {e3956282},
  publisher = {{Hindawi}},
  issn = {1530-8669},
  doi = {10.1155/2017/3956282},
  url = {https://www.hindawi.com/journals/wcmc/2017/3956282/},
  urldate = {2020-03-12},
  abstract = {Wireless acoustic sensor networks (WASNs) are formed by a distributed group of acoustic-sensing devices featuring audio playing and recording capabilities. Current mobile computing platforms offer great possibilities for the design of audio-related applications involving acoustic-sensing nodes. In this context, acoustic source localization is one of the application domains that have attracted the most attention of the research community along the last decades. In general terms, the localization of acoustic sources can be achieved by studying energy and temporal and/or directional features from the incoming sound at different microphones and using a suitable model that relates those features with the spatial location of the source (or sources) of interest. This paper reviews common approaches for source localization in WASNs that are focused on different types of acoustic features, namely, the energy of the incoming signals, their time of arrival (TOA) or time difference of arrival (TDOA), the direction of arrival (DOA), and the steered response power (SRP) resulting from combining multiple microphone signals. Additionally, we discuss methods not only aimed at localizing acoustic sources but also designed to locate the nodes themselves in the network. Finally, we discuss current challenges and frontiers in this field.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3S6KCSHN\\Cobos et al. - 2017 - A Survey of Sound Source Localization Methods in W.pdf;C\:\\Users\\sauli\\Zotero\\storage\\AH3LMTFE\\3956282.html},
  langid = {english},
  type = {Review Article}
}

@online{CobraNetFAQCobraNet,
  title = {{{CobraNet FAQ}} | {{CobraNet HOME}}},
  url = {http://www.cobranet.info/support/faq#Q24},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ADSGGJI8\\faq.html}
}

@inproceedings{cohen-mcfarlaneDesignSystemMeasure2018,
  title = {Design of a {{System}} to {{Measure Spatial Sound Localization Abilities}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Medical Measurements}} and {{Applications}} ({{MeMeA}})},
  author = {Cohen-McFarlane, M. and Goubran, R. and Knoefel, F.},
  date = {2018-06},
  pages = {1--6},
  doi = {10.1109/MeMeA.2018.8438704},
  abstract = {Hearing ability declines with age. Specifically, it has been shown that sound localization ability becomes increasingly unreliable. This paper describes the design of a system that simulates spatial sound sources to be presented to the user via headphones. Simulation of spatial sound sources are calculated using four increasingly complex methods; (1) Phase adjustment simulation, (2) Amplitude adjustment simulation, (3) Amplitude + Phase adjustment simulation, and (4) Head Related Transfer Function (HRTF) adjustment simulation. The preliminary validation experiment is presented in order to evaluate if the system’s simulated sources presented via noise cancelling headphones is comparable to previous work using simulated sources presented via a loudspeaker array surrounding the user. Eight participants were asked to report the perceived direction of the source for each simulation method. Overall the amplitude simulation performed the best (47.5\% accuracy within a ±30° window), however a high accuracy is not needed for these simulation methods to be relevant. Implementing these methods to differentiate between population groups is the main goal. A secondary repeatability experiment was done using the amplitude simulation, which suggests that the method is suitable give the ±30° window constraint. Future implementations are proposed to evaluate if the user interface and accompanying user feedback is able to identify older adults from younger adults. This may lead to a measure of decreasing hearing abilities associated the aging process.},
  keywords = {Auditory system,Ear,Headphones,Loudspeakers,Mathematical model,Matlab,Transfer functions}
}

@incollection{colburnModelsSoundLocalization2005,
  title = {Models of {{Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Colburn, H. Steven and Kulkarni, Abhijit},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {272--316},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_8},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_8},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G942GTAS\\0-387-28863-5_8.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{ColburnModelsSoundLocalization2005,
  title = {Models of {{Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Colburn, H. Steven and Kulkarni, Abhijit},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {272--316},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_8},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4KQN29GX\\0-387-28863-5_8.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@online{CollectedPapersAcoustics,
  title = {Collected Papers on Acoustics : {{Sabine}}, {{Wallace Clement}}, 1868-1919 : {{Free Download}}, {{Borrow}}, and {{Streaming}} : {{Internet Archive}}},
  url = {https://archive.org/details/collectedpaperso00sabi/page/42},
  urldate = {2019-06-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IC6VNAUM\\42.html}
}

@inproceedings{comanducciRaySpaceTransform2018,
  title = {Ray {{Space Transform Interpolation}} with {{Convolutional Autoencoder}}},
  booktitle = {2018 16th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Comanducci, L. and Borra, F. and Bestagini, P. and Antonacci, F. and Sarti, A. and Tubaro, S.},
  date = {2018-09},
  pages = {261--265},
  doi = {10.1109/IWAENC.2018.8521397},
  abstract = {In this paper we propose an algorithm for the reconstruction of the Ray Space Transform (RST) through the use of neural networks. In particular, our aim is to reconstruct the magnitude of the RST acquired from a linear microphone array, as if the array were composed by a larger amount of microphones. This is useful for applications that need a higher RST resolution when only a limited amount of microphones can be used due to practical constraints or physical limitations. The proposed solution leverages recent advancements in deep learning as it is based on a fully convolutional autoencoder. To validate our method, we show through a simulative campaign that it is possible to improve sound source localization using the reconstructed RST compared to the use of the original RST.},
  keywords = {Acoustics,convolutional neural networks,deep learning,Image reconstruction,Interpolation,Microphone arrays,Ray space,source localization,Training,Transforms}
}

@online{CombiningCNNRNN,
  title = {Combining {{CNN}} and {{RNN}} for Spoken Language Identification · {{YerevaNN}}},
  url = {https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/},
  urldate = {2018-04-02},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G4RHWSFT\\combining-cnn-and-rnn-for-spoken-language-identification.html}
}

@online{ConstantDirectivityBeamforming,
  title = {Constant {{Directivity Beamforming}} | {{SpringerLink}}},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-04619-7_1},
  urldate = {2019-08-05},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W8Y395AI\\978-3-662-04619-7_1.html}
}

@online{ConvolutionalAutoencoderGoogle,
  title = {Convolutional Autoencoder - „{{Google}}“ Paieška},
  url = {https://www.google.com/search?q=convolutional+autoencoder&client=firefox-b&biw=1184&bih=591&noj=1&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi4hdHq75vSAhUBqCwKHVOiC3sQ_AUICCgB},
  urldate = {2017-02-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\854MERBB\\search.html}
}

@article{convolutionalautoencoderGoogle,
  title = {Convolutional {{Autoencoder}} - ,,\{\{\vphantom{\}\}}{{Google}}\vphantom\{\}\vphantom\{\}`` {{Paieška}}},
  url = {https://www.google.com/search?q=convolutional+autoencoder&client=firefox-b&biw=1184&bih=591&noj=1&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi4hdHq75vSAhUBqCwKHVOiC3sQ_AUICCgB},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6LCM9Q25\\search.html}
}

@inproceedings{coorsLearningTransformationInvariant2018,
  title = {Learning {{Transformation Invariant Representations}} with {{Weak Supervision}}:},
  shorttitle = {Learning {{Transformation Invariant Representations}} with {{Weak Supervision}}},
  booktitle = {Proceedings of the 13th {{International Joint Conference}} on {{Computer Vision}}, {{Imaging}} and {{Computer Graphics Theory}} and {{Applications}}},
  author = {Coors, Benjamin and Condurache, Alexandru and Mertins, Alfred and Geiger, Andreas},
  date = {2018},
  pages = {64--72},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {{Funchal, Madeira, Portugal}},
  doi = {10.5220/0006549000640072},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006549000640072},
  urldate = {2019-12-11},
  abstract = {Deep convolutional neural networks are the current state-of-the-art solution to many computer vision tasks.},
  eventtitle = {International {{Conference}} on {{Computer Vision Theory}} and {{Applications}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DX7J88FZ\\Coors et al. - 2018 - Learning Transformation Invariant Representations .pdf},
  isbn = {978-989-758-290-5},
  langid = {english}
}

@online{corporationRolandProSolutions,
  title = {Roland {{Pro A}}/{{V}} - {{Solutions}} - {{V}}-{{Mixing System Overview}} - {{REAC Benfits}} and {{Power}}},
  author = {Corporation, Roland},
  journaltitle = {Roland Pro A/V},
  url = {https://proav.roland.com/global/solutions/v-mixing_system/reac_overview/},
  urldate = {2017-06-13},
  abstract = {REAC Benfits and Power},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AFUVMI5Q\\reac_overview.html}
}

@online{corpWhatLivewire,
  title = {What Is {{Livewire}}+?},
  author = {Corp, T. L. S.},
  url = {http://success.telosalliance.com/livewire-plus},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ATFHIXNQ\\livewire-plus.html}
}

@article{coteliMultipleSoundSource2018,
  title = {Multiple {{Sound Source Localization With Steered Response Power Density}} and {{Hierarchical Grid Refinement}}},
  author = {Çöteli, M. B. and Olgun, O. and Hacıhabiboğlu, H.},
  date = {2018-11},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {2215--2229},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2858932},
  abstract = {Estimation of the direction-of-arrival (DOA) of sound sources is an important step in sound field analysis. Rigid spherical microphone arrays allow the calculation of a compact spherical harmonic representation of the sound field. The standard method for analyzing sound fields recorded using such arrays is steered response power (SRP) maps wherein the source DOA can be estimated as the steering direction that maximizes the output power of a maximally directive beam. This approach is computationally costly since it requires steering the beam in all possible directions. This paper presents an extension to SRP called steered response power density (SRPD) and an associated, signal-adaptive search method called hierarchical grid refinement for reducing the number of steering directions needed for DOA estimation. The proposed method can localize near-coherent as well as incoherent sources while jointly providing the number of prominent sources in the scene. It is shown to be robust to reverberation and additive white noise. An evaluation of the proposed method using simulations and real recordings under highly reverberant conditions as well as a comparison with the state-of-the-art methods are presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PDTNG8BP\\Çöteli et al. - 2018 - Multiple Sound Source Localization With Steered Re.pdf;C\:\\Users\\sauli\\Zotero\\storage\\HVC3CWGB\\8418732.html},
  keywords = {acoustic field,acoustic signal processing,array signal processing,associated signal-adaptive search method,compact spherical harmonic representation,Computational efficiency,direction-of-arrival estimation,Direction-of-arrival estimation,DOA estimation,Estimation,Harmonic analysis,hierarchical grid refinement,microphone arrays,Microphone arrays,multiple sound source localization,reverberation,rigid spherical microphone arrays,sound field analysis,Source localization,Speech processing,steered response power,steered response power density},
  number = {11}
}

@article{coteliMultipleSoundSource2018a,
  title = {Multiple {{Sound Source Localization With Steered Response Power Density}} and {{Hierarchical Grid Refinement}}},
  author = {Çöteli, M. B. and Olgun, O. and Hacıhabiboğlu, H.},
  date = {2018-11},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {2215--2229},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2858932},
  abstract = {Estimation of the direction-of-arrival (DOA) of sound sources is an important step in sound field analysis. Rigid spherical microphone arrays allow the calculation of a compact spherical harmonic representation of the sound field. The standard method for analyzing sound fields recorded using such arrays is steered response power (SRP) maps wherein the source DOA can be estimated as the steering direction that maximizes the output power of a maximally directive beam. This approach is computationally costly since it requires steering the beam in all possible directions. This paper presents an extension to SRP called steered response power density (SRPD) and an associated, signal-adaptive search method called hierarchical grid refinement for reducing the number of steering directions needed for DOA estimation. The proposed method can localize near-coherent as well as incoherent sources while jointly providing the number of prominent sources in the scene. It is shown to be robust to reverberation and additive white noise. An evaluation of the proposed method using simulations and real recordings under highly reverberant conditions as well as a comparison with the state-of-the-art methods are presented.},
  keywords = {acoustic field,acoustic signal processing,array signal processing,associated signal-adaptive search method,compact spherical harmonic representation,Computational efficiency,direction-of-arrival estimation,Direction-of-arrival estimation,DOA estimation,Estimation,Harmonic analysis,hierarchical grid refinement,microphone arrays,Microphone arrays,multiple sound source localization,reverberation,rigid spherical microphone arrays,sound field analysis,Source localization,Speech processing,steered response power,steered response power density},
  number = {11}
}

@article{coteliMultipleSoundSource2018b,
  title = {Multiple {{Sound Source Localization With Steered Response Power Density}} and {{Hierarchical Grid Refinement}}},
  author = {Coteli, Mert Burkay and Olgun, Orhun and Hacihabiboglu, Huseyin},
  date = {2018-11},
  journaltitle = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  volume = {26},
  pages = {2215--2229},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2858932},
  url = {https://doi.org/10.1109/TASLP.2018.2858932},
  urldate = {2018-10-19},
  abstract = {Estimation of the direction-of-arrival DOA of sound sources is an important step in sound field analysis. Rigid spherical microphone arrays allow the calculation of a compact spherical harmonic representation of the sound field. The standard method for analyzing sound fields recorded using such arrays is steered response power SRP maps wherein the source DOA can be estimated as the steering direction that maximizes the output power of a maximally directive beam. This approach is computationally costly since it requires steering the beam in all possible directions. This paper presents an extension to SRP called steered response power density SRPD and an associated, signal-adaptive search method called hierarchical grid refinement for reducing the number of steering directions needed for DOA estimation. The proposed method can localize near-coherent as well as incoherent sources while jointly providing the number of prominent sources in the scene. It is shown to be robust to reverberation and additive white noise. An evaluation of the proposed method using simulations and real recordings under highly reverberant conditions as well as a comparison with the state-of-the-art methods are presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6BUWMDZ3\\Coteli et al. - 2018 - Multiple Sound Source Localization With Steered Re.pdf},
  number = {11}
}

@article{coteliMultipleSoundSource2018c,
  title = {Multiple {{Sound Source Localization With Steered Response Power Density}} and {{Hierarchical Grid Refinement}}},
  author = {Coteli, Mert Burkay and Olgun, Orhun and Hacihabiboglu, Huseyin},
  date = {2018-11},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{2215-2229\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2858932\}},
  abstract = {Estimation of the direction-of-arrival (DOA) of sound sources is an important step in sound field analysis. Rigid spherical microphone arrays allow the calculation of a compact spherical harmonic representation of the sound field. The standard method for analyzing sound fields recorded using such arrays is steered response power (SRP) maps wherein the source DOA can be estimated as the steering direction that maximizes the output power of a maximally directive beam. This approach is computationally costly since it requires steering the beam in all possible directions. This paper presents an extension to SRP called steered response power density (SRPD) and an associated, signal-adaptive search method called hierarchical grid refinement for reducing the number of steering directions needed for DOA estimation. The proposed method can localize near-coherent as well as incoherent sources while jointly providing the number of prominent sources in the scene. It is shown to be robust to reverberation and additive white noise. An evaluation of the proposed method using simulations and real recordings under highly reverberant conditions as well as a comparison with the state-of-the-art methods are presented.},
  affiliation = {Hacihabiboglu, H (Reprint Author), Middle East Tech Univ, Grad Sch Informat, Spatial Audio Res Grp SPARG, TR-06800 Ankara, Turkey. Coteli, Mert Burkay; Olgun, Orhun; Hacihabiboglu, Huseyin, Middle East Tech Univ, Grad Sch Informat, Spatial Audio Res Grp SPARG, TR-06800 Ankara, Turkey.},
  author-email = {mert.coteli@metu.edu.tr orhun.olgun@metu.edu.tr hhuseyin@metu.edu.tr},
  cited-references = {Arfken G.B., 2005, MATH METHODS PHYS IN. Bang and Olufsen, 1992, MUS ARCH. BATTY M, 1974, GEOGR ANAL, V6, P1. Bock S., 2013, P 16 INT C DIG AUD E. Bradley Derek, 2007, Journal of Graphics Tools, V12, P13. Brandstein M., 2013, MICROPHONE ARRAYS SI. Brent R. P., 1976, MULTIPLE PRECISION Z, P151. Brookes M., 1997, VOICEBOX SPEECH PROC, V47. Cobos M, 2011, IEEE SIGNAL PROC LET, V18, P71, DOI 10.1109/LSP.2010.2091502. Do H., 2007, P IEEE WORKSH APPL S, P295. Do H, 2007, INT CONF ACOUST SPEE, P121. Evers C, 2014, 2014 14TH INTERNATIONAL WORKSHOP ON ACOUSTIC SIGNAL ENHANCEMENT (IWAENC), P258, DOI 10.1109/IWAENC.2014.6954298. Fahy F. J., 1995, SOUND INTENSITY. Farina A., 2000, P 108 AUD ENG SOC CO. Gorski KM, 2005, ASTROPHYS J, V622, P759, DOI 10.1086/427976. Hafezi S, 2017, IEEE-ACM T AUDIO SPE, V25, P1956, DOI 10.1109/TASLP.2017.2736067. Hafezi S, 2017, 2017 HANDS-FREE SPEECH COMMUNICATIONS AND MICROPHONE ARRAYS (HSCMA 2017), P81, DOI 10.1109/HSCMA.2017.7895566. Hafezi S, 2016, EUR SIGNAL PR CONF, P602, DOI 10.1109/EUSIPCO.2016.7760319. Hafezi S, 2016, INT CONF ACOUST SPEE, P415, DOI 10.1109/ICASSP.2016.7471708. Horn R. A., 1990, MATRIX ANAL. International Telecommunications Union, 1993, ITU T REC. Jarrett D. P., 2016, THEORY APPL SPHERICA. Jarrett DP, 2010, EUR SIGNAL PR CONF, P442. Khaykin Dima, 2009, 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), P221, DOI 10.1109/ASPAA.2009.5346492. Khaykin D, 2012, J ACOUST SOC AM, V132, P261, DOI 10.1121/1.4726012. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Kuhn HW, 1955, NAV RES LOG, V2, P83, DOI [10.1002/nay.3800020109, 10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]. Li ZY, 2007, IEEE T AUDIO SPEECH, V15, P702, DOI 10.1109/TASL.2006.876764. Li ZY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL IV, PROCEEDINGS, P41. Lima Markus V. S., 2015, IEEE Signal Processing Letters, V22, P1098, DOI 10.1109/LSP.2014.2385864. Marti A, 2013, J ACOUST SOC AM, V134, P2627, DOI 10.1121/1.4820885. Meyer J, 2004, AUDIO SIGNAL PROCESSING: FOR NEXT-GENERATION MULTIMEDIA COMMUNICATION SYSTEMS, P67, DOI 10.1007/1-4020-7769-6_3. Moore AH, 2017, IEEE-ACM T AUDIO SPE, V25, P178, DOI 10.1109/TASLP.2016.2613280. Moore AH, 2015, EUR SIGNAL PR CONF, P2296, DOI 10.1109/EUSIPCO.2015.7362794. Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1. Nadiri O, 2014, IEEE-ACM T AUDIO SPE, V22, P1494, DOI 10.1109/TASLP.2014.2337846. Nunes LO, 2014, IEEE T SIGNAL PROCES, V62, P5171, DOI 10.1109/TSP.2014.2336636. Patynen J, 2008, ACTA ACUST UNITED AC, V94, P856, DOI 10.3813/AAA.918104. Pavlidi D, 2016, INT CONF ACOUST SPEE, P96, DOI 10.1109/ICASSP.2016.7471644. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Rafaely B, 2004, J ACOUST SOC AM, V116, P2149, DOI 10.1121/1.1792643. Rafaely B., 2015, FUNDAMENTALS SPHERIC. Rafaely B, 2007, IEEE T SIGNAL PROCES, V55, P1003, DOI 10.1109/TSP.2006.888896. Rafaely B, 2018, SIGNAL PROCESS, V143, P42, DOI 10.1016/j.sigpro.2017.08.010. Rafaely B, 2017, INT CONF ACOUST SPEE, P6120, DOI 10.1109/ICASSP.2017.7953332. Rafaely B, 2010, SPRINGER TOP SIGN PR, V3, P281, DOI 10.1007/978-3-642-11130-3_11. RICKARD S, 2002, ACOUST SPEECH SIG PR, P529. Salvati D, 2017, J ACOUST SOC AM, V141, P586, DOI 10.1121/1.4974289. SAMET H, 1981, J ACM, V28, P487, DOI 10.1145/322261.322267. Soille P., 2004, MORPHOLOGICAL IMAGE. Sun HH, 2012, J ACOUST SOC AM, V131, P2828, DOI 10.1121/1.3688476. Sun HH, 2011, INT CONF ACOUST SPEE, P117. Teutsch H, 2008, INT CONF ACOUST SPEE, P5276, DOI 10.1109/ICASSP.2008.4518850. Williams E. G., 1999, FOURIER ACOUSTICS. Yook D, 2016, IEEE T CYBERNETICS, V46, P20, DOI 10.1109/TCYB.2015.2391252.},
  da = {2018-10-18},
  doc-delivery-number = {GR9DQ},
  funding-acknowledgement = {Turkish Scientific and Technological Research Council (TUBITAK) [113E513]},
  funding-text = {This work was supported by the Turkish Scientific and Technological Research Council (TUBITAK) under Research Grant 113E513 “Spatial Audio Reproduction Using Analysis-based Synthesis Methods.”},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Source localization,direction-of-arrival estimation),hierarchical grid refinement,PHAT,rigid spherical microphone arrays,SRP,steered response  power},
  keywords-plus = {SPHERICAL MICROPHONE ARRAYS; ACOUSTIC SOURCE LOCALIZATION; OF-ARRIVAL ESTIMATION; PATH DOMINANCE TEST; HARMONIC DOMAIN; FIELD; DECOMPOSITION; ALGORITHM; VECTORS},
  langid = {english},
  number = {11},
  number-of-cited-references = {55},
  orcid-numbers = {Hacihabiboglu, Huseyin/0000-0002-5399-0162},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000443046300004},
  usage-count-last-180-days = {5},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@inproceedings{crammerRoomImpulseResponse2006,
  title = {Room {{Impulse Response Estimation}} Using {{Sparse Online Prediction}} and {{Absolute Loss}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Acoustics Speed}} and {{Signal Processing Proceedings}}},
  author = {Crammer, K. and Lee, D.D.},
  date = {2006},
  volume = {3},
  pages = {III-748-III-751},
  publisher = {{IEEE}},
  location = {{Toulouse, France}},
  doi = {10.1109/ICASSP.2006.1660762},
  url = {http://ieeexplore.ieee.org/document/1660762/},
  urldate = {2018-10-09},
  abstract = {The need to accurately and efficiently estimate room impulse responses arises in many acoustic signal processing applications. In this work, we present a general family of algorithms which contain the conventional normalized least mean squares (NLMS) algorithm as a special case. Specific members of this family yield estimates which are robust both to different noise models and choice of parameters. We demonstrate the merits of our approach to accurately estimate sparse room impulse responses in simulations with speech signals.},
  eventtitle = {2006 {{IEEE International Conference}} on {{Acoustics Speed}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\A28FXTLX\\Crammer and Lee - 2006 - Room Impulse Response Estimation using Sparse Onli.pdf},
  isbn = {978-1-4244-0469-8},
  langid = {english}
}

@online{CrewsafeOperationManual,
  title = {{$<\%$}{{Crewsafe Operation Manual}}\%{$>$} - {{Audio}}-over-{{IP}}-{{Instant}}-{{Expert}}-{{Guide}}-v1.Pdf},
  url = {http://www.tieline.com/Downloads/Audio-over-IP-Instant-Expert-Guide-v1.pdf},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\THSH4PX3\\Audio-over-IP-Instant-Expert-Guide-v1.html}
}

@article{croccoAudioTrackingNoisy2018,
  title = {Audio {{Tracking}} in {{Noisy Environments}} by {{Acoustic Map}} and {{Spectral Signature}}},
  author = {Crocco, Marco and Martelli, Samuele and Trucco, Andrea and Zunino, Andrea and Murino, Vittorio},
  date = {2018-05},
  journaltitle = {IEEE TRANSACTIONS ON CYBERNETICS},
  volume = {48},
  pages = {\{1619-1632\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2168-2267},
  doi = {\{10.1109/TCYB.2017.2711497\}},
  abstract = {A novel method is proposed for generic target tracking by audio measurements from a microphone array. To cope with noisy environments characterized by persistent and high energy interfering sources, a classification map (CM) based on spectral signatures is calculated by means of a machine learning algorithm. Next, the CM is combined with the acoustic map, describing the spatial distribution of sound energy, in order to obtain a cleaned joint map in which contributions from the disturbing sources are removed. A likelihood function is derived from this map and fed to a particle filter yielding the target location estimation on the acoustic image. The method is tested on two real environments, addressing both speaker and vehicle tracking. The comparison with a couple of trackers, relying on the acoustic map only, shows a sharp improvement in performance, paving the way to the application of audio tracking in real challenging environments.},
  affiliation = {Zunino, A (Reprint Author), Ist Italiano Tecnol, Pattern Anal & Comp Vis, I-16163 Genoa, Italy. Crocco, Marco; Martelli, Samuele; Trucco, Andrea; Zunino, Andrea; Murino, Vittorio, Ist Italiano Tecnol, Pattern Anal & Comp Vis, I-16163 Genoa, Italy. Trucco, Andrea; Zunino, Andrea, Univ Genoa, Dipartimento Ingn Navale Elettr Elettron & Teleco, I-16145 Genoa, Italy. Murino, Vittorio, Univ Verona, Comp Sci Dept, I-37134 Verona, Italy.},
  author-email = {marco.crocco@iit.it samuele.martelli@iit.it trucco@dibe.unige.it andrea.zunino@iit.it vittorio.murino@iit.it},
  cited-references = {Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374. Asaei Afsaneh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1439, DOI 10.1109/ICASSP.2014.6853835. Bishop C. M., 2006, PATTERN RECOGNITION. Brandstein MS, 1997, COMPUT SPEECH LANG, V11, P91, DOI 10.1006/csla.1996.0024. Cevher V, 2007, IEEE T SIGNAL PROCES, V55, P2810, DOI 10.1109/TSP.2007.893962. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. Chen C.-E., 2006, P IEEE INT C AC SPEE, V4, P4. Chen YQ, 2001, PROC CVPR IEEE, P543. Clavel C., 2005, P IEEE INT C MULT EX, P1306. Crocco M, 2014, IEEE-ACM T AUDIO SPE, V22, P800, DOI 10.1109/TASLP.2014.2304635. Fallon MF, 2010, IEEE T AUDIO SPEECH, V18, P1228, DOI 10.1109/TASL.2009.2031781. GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015. Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830. Hartley R. I., 2004, MULTIPLE VIEW GEOMET. Huang YT, 2011, IEEE SIGNAL PROC MAG, V28, P20, DOI 10.1109/MSP.2010.938754. Kemp T, 2000, INT CONF ACOUST SPEE, P1423, DOI 10.1109/ICASSP.2000.861862. Kepesi M, 2007, INT WORK CONTENT MUL, P303. Kunlai Xiong, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2233, DOI 10.1109/ICASSP.2014.6853996. Lehmann E. A., 2007, EURASIP J APPL SIG P, V2007, P28. Martin A, 2001, INT CONF ACOUST SPEE, P237, DOI 10.1109/ICASSP.2001.940811. May T, 2012, IEEE T AUDIO SPEECH, V20, P2016, DOI 10.1109/TASL.2012.2193391. Mitrovic D, 2010, ADV COMPUT, V78, P71, DOI 10.1016/S0065-2458(10)78003-7. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Pham QC, 2010, INT C IM PROC THEOR, P47. Platt J.C, 1999, ADV LARGE MARGIN CLA, P61. Plinge Axel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P614, DOI 10.1109/ICASSP.2014.6853669. Pucher Michael, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P837, DOI 10.1109/ITSC.2010.5625035. Rabaoui A, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P117, DOI 10.1109/AVSS.2007.4425296. Salmond DJ, 2001, P AMER CONTR CONF, P3755, DOI 10.1109/ACC.2001.946220. Savran A, 2015, IEEE T CYBERNETICS, V45, P1927, DOI 10.1109/TCYB.2014.2362101. Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88. Srinivas U, 2015, IEEE T CYBERNETICS, V45, P576, DOI 10.1109/TCYB.2014.2331284. Stewart D, 2014, IEEE T CYBERNETICS, V44, P175, DOI 10.1109/TCYB.2013.2250954. Talantzis F, 2009, IEEE T SYST MAN CY B, V39, P7, DOI 10.1109/TSMCB.2008.2009558. Trucco A, 2003, IEEE T SYST MAN CY B, V33, P687, DOI [10.1109/TSMCB.2003.814300, 10.1109/TSMCB..2003.814300]. Valenzise G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P21, DOI 10.1109/AVSS.2007.4425280. Van Trees H.L., 2002, DETECTION ESTIMATION. Vermaak J, 2002, LECT NOTES COMPUT SC, V2350, P645. Ward DB, 2003, IEEE T SPEECH AUDI P, V11, P826, DOI 10.1109/TSA.2003.818112. Wu K, 2013, INT CONF ACOUST SPEE, P365, DOI 10.1109/ICASSP.2013.6637670. Zhang C, 2008, INT CONF ACOUST SPEE, P2565. Zhang MH, 2013, NEUROCOMPUTING, V122, P163, DOI 10.1016/j.neucom.2013.05.041.},
  da = {2018-10-18},
  doc-delivery-number = {GB7II},
  eissn = {2168-2275},
  journal-iso = {IEEE T. Cybern.},
  keywords = {(Acoustical imaging,audio classification,audio source localization,audio tracking,particle filter,spectral signature,tracking before  detect)},
  keywords-plus = {SOURCE LOCALIZATION; TUTORIAL; SIGNALS; FUSION; DETECT; ARRAYS; VIDEO},
  langid = {english},
  number = {5},
  number-of-cited-references = {42},
  orcid-numbers = {Trucco, Andrea/0000-0003-1189-6191},
  research-areas = {Automation & Control Systems; Computer Science},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000429247700022},
  usage-count-last-180-days = {10},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics}
}

@article{croccoUncalibrated3DRoom2016,
  title = {Uncalibrated {{3D Room Reconstruction}} from {{Sound}}},
  author = {Crocco, Marco and Trucco, Andrea and Del Bue, Alessio},
  date = {2016-06-20},
  url = {http://arxiv.org/abs/1606.06258},
  urldate = {2020-02-21},
  abstract = {This paper presents a method to reconstruct the 3D structure of generic convex rooms from sound signals. Differently from most of the previous approaches, the method is fully uncalibrated in the sense that no knowledge about the microphones and sources position is needed. Moreover, we demonstrate that it is possible to bypass the well known echo labeling problem, allowing to reconstruct the room shape in a reasonable computation time without the need of additional hypotheses on the echoes order of arrival. Finally, the method is intrinsically robust to outliers and missing data in the echoes detection, allowing to work also in low SNR conditions. The proposed pipeline formalises the problem in different steps such as time of arrival estimation, microphones and sources localization and walls estimation. After providing a solution to these different problems we present a global optimization approach that links together all the problems in a single optimization function. The accuracy and robustness of the method is assessed on a wide set of simulated setups and in a challenging real scenario. Moreover we make freely available for a challenging dataset for 3D room reconstruction with accurate ground truth in a real scenario.},
  archivePrefix = {arXiv},
  eprint = {1606.06258},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YNXEDX88\\Crocco et al. - 2016 - Uncalibrated 3D Room Reconstruction from Sound.pdf;C\:\\Users\\sauli\\Zotero\\storage\\XG4CXIYP\\1606.html},
  keywords = {Computer Science - Sound,I.5.4},
  primaryClass = {cs}
}

@article{croccoUncalibrated3DRoom2017,
  title = {Uncalibrated {{3D}} Room Geometry Estimation from Sound Impulse Responses},
  author = {Crocco, Marco and Trucco, Andrea and Del Bue, Alessio},
  date = {2017-12-01},
  journaltitle = {Journal of the Franklin Institute},
  shortjournal = {Journal of the Franklin Institute},
  volume = {354},
  pages = {8678--8709},
  issn = {0016-0032},
  doi = {10.1016/j.jfranklin.2017.10.024},
  url = {http://www.sciencedirect.com/science/article/pii/S0016003217305458},
  urldate = {2020-02-21},
  abstract = {This paper presents a method to reconstruct the 3D structure of generic convex rooms from sound signals. Differently from most of the previous approaches, the method is fully uncalibrated in the sense that no knowledge about the microphones and sources position is needed. Moreover, we demonstrate that it is possible to bypass the well known echo labelling problem, allowing to reconstruct the room shape in a reasonable computation time without the need of additional hypotheses on the echoes order of arrival. Finally, the method is intrinsically robust to outliers and missing data in the echoes detection, allowing to work also in low SNR conditions. The proposed pipeline formalizes the problem in different steps such as TOA estimation, microphones and sources localization and walls estimation. After providing a solution to these different problems, we present a global optimization approach that links together all the problems in a single optimization function. The accuracy and robustness of the method is assessed on a wide set of simulated setups and in a challenging real scenario. Compared to previous approaches, the method shows relevant improvements in terms of precision and scalability. Moreover, we make freely available for a challenging dataset for 3D room reconstruction with accurate ground truth in a real scenario.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TH5ASHT6\\Crocco et al. - 2017 - Uncalibrated 3D room geometry estimation from soun.pdf;C\:\\Users\\sauli\\Zotero\\storage\\RIATEHE6\\S0016003217305458.html},
  langid = {english},
  number = {18}
}

@online{CS7960ProjectJames,
  title = {{{CS7960}} - {{Project}} 6 | {{James Fishbaugh}}},
  url = {http://www.cs.utah.edu/~jfishbau/advimproc/project6/},
  urldate = {2019-11-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UD2PVVA8\\project6.html}
}

@article{cuiApproximateClosedFormTDOABased2018,
  title = {Approximate {{Closed}}-{{Form TDOA}}-{{Based Estimator}} for {{Acoustic Direction Finding}} via {{Constrained Optimization}}},
  author = {Cui, Xunxue and Yu, Kegen and Lu, Songsheng},
  date = {2018-04-15},
  journaltitle = {IEEE SENSORS JOURNAL},
  volume = {18},
  pages = {\{3360-3371\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {1530-437X},
  doi = {\{10.1109/JSEN.2018.2803150\}},
  abstract = {This paper focuses on estimating the azimuth and elevation of a sound emitter in 3-D space based on time-difference-of-arrival (TDOA) measurements with an array of acoustic sensors. The TDOA-based direction finding problem is significant because in a range of scenarios the source only emits a transient signal and only TDOA measurements can be used to find the direction. The linear least squares estimator provides a suboptimal solution, since there is nontrivial information loss in the linearization of the nonlinear observation equation. To avoid the information loss, the Lagrange multiplier method is usually used to realize the constrained optimization, but the computational complexity is rather high. This paper proposes a constrained least squares estimator to deal with the direction finding problem. The proposed method makes use of both Lagrange multiplier and quadratic constraints to form the cost function. The resultant estimator is shown to be approximate closed-form so that the computational complexity is reduced greatly, but contributes little under a small noise level. Mathematical formula is derived to evaluate the theoretical accuracy of the proposed estimator in terms of mean square error. Both simulation and field experimental results demonstrate that the proposed estimator can outperform the traditional linear and nonlinear estimators.},
  affiliation = {Cui, XX (Reprint Author), Anhui Univ Sci & Technol, Huainan 232001, Peoples R China. Cui, Xunxue, Anhui Univ Sci & Technol, Huainan 232001, Peoples R China. Yu, Kegen, China Univ Min & Technol, Sch Environm Sci & Spatial Informat, Xuzhou 221116, Peoples R China. Yu, Kegen, Wuhan Univ, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Hubei, Peoples R China. Yu, Kegen, Wuhan Univ, Sch Geodesy & Geomat, Wuhan 430079, Hubei, Peoples R China. Lu, Songsheng, New Star Inst Appl Technol, Hefei 230031, Anhui, Peoples R China.},
  author-email = {xxcui@tsinghua.org.cn kegen.yu@foxmail.com l54ong@163.com},
  cited-references = {ABATZOGLOU TJ, 1991, IEEE T SIGNAL PROCES, V39, P1070, DOI 10.1109/78.80955. Berdugo B, 1999, J ACOUST SOC AM, V105, P3355, DOI 10.1121/1.424664. Bertsekas D. P., 1999, NONLINEAR PROGRAMMIN. Biguesh M, 2016, IEEE SENS J, V16, P6961, DOI 10.1109/JSEN.2016.2592950. Bishop AN, 2009, IEEE T AERO ELEC SYS, V45, P308, DOI 10.1109/TAES.2009.4805281. Bishop AN, 2008, IEEE J OCEANIC ENG, V33, P289, DOI 10.1109/JOE.2008.926960. Canclini A, 2015, IEEE-ACM T AUDIO SPE, V23, P1563, DOI 10.1109/TASLP.2015.2439040. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. Cui XX, 2016, IEEE T INSTRUM MEAS, V65, P2442, DOI 10.1109/TIM.2016.2583224. Cui XX, 2015, IEEE T INSTRUM MEAS, V64, P2347, DOI 10.1109/TIM.2015.2415051. Ferguson BG, 2002, J ACOUST SOC AM, V111, P104, DOI 10.1121/1.1402618. Gergely V, 2011, IEEE T INSTRUM MEAS, V60, P1820, DOI 10.1109/TIM.2011.2108074. Ho KC, 2006, IEEE T SIGNAL PROCES, V54, P809, DOI 10.1109/TSP.2005.861776. Horn R. A., 2012, MATRIX ANAL. Huang YT, 2001, IEEE T SPEECH AUDI P, V9, P943, DOI 10.1109/89.966097. James M. D., 2008, THESIS. Lemmerling P, 1996, IEEE T SIGNAL PROCES, V44, P2908, DOI 10.1109/78.542454. Lin LX, 2013, SIGNAL PROCESS, V93, P2872, DOI 10.1016/j.sigpro.2013.04.004. Lo KW, 2014, 2014 IEEE WORKSHOP ON STATISTICAL SIGNAL PROCESSING (SSP), P193, DOI 10.1109/SSP.2014.6884608. Sinha P., 2015, P 21 NAT C COMM NCC, P1. [王鼎 Wang Ding], 2015, [中国科学. 信息科学, Scientia Sinica Informationis], V45, P1466. Xu B, 2013, IEEE T PARALL DISTR, V24, P1567, DOI 10.1109/TPDS.2012.248. Xu LF, 2013, IEEE T SIGNAL PROCES, V61, P2927, DOI 10.1109/TSP.2013.2255045. Yang K, 2010, IEEE T VEH TECHNOL, V59, P1558, DOI 10.1109/TVT.2009.2037509. Yu K., 2009, GROUND BASED WIRELES.},
  da = {2018-10-18},
  doc-delivery-number = {GA8JJ},
  eissn = {1558-1748},
  funding-acknowledgement = {National Natural Science Foundation of China [61672532]},
  funding-text = {This work was supported by the National Natural Science Foundation of China under Grant 61672532. The associate editor coordinating the review of this paper and approving it for publication was Prof. Kazuaki Sawada. (Corresponding author: Xunxue Cui.)},
  journal-iso = {IEEE Sens. J.},
  keywords = {(Direction finding,constrained least  squares,Lagrange multiplier),time-difference-of-arrival},
  keywords-plus = {TOTAL LEAST-SQUARES; SOURCE LOCALIZATION; MICROPHONE NETWORKS; TIME DELAYS; ALGORITHM},
  langid = {english},
  number = {8},
  number-of-cited-references = {25},
  orcid-numbers = {Cui, Xunxue/0000-0002-3150-0388},
  research-areas = {Engineering; Instruments & Instrumentation; Physics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000428585900032},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Engineering, Electrical & Electronic; Instruments & Instrumentation; Physics, Applied}
}

@article{czyzewskiAutomaticIdentificationSound2003,
  title = {Automatic {{Identification}} of {{Sound Source Position Employing Neural Networks}} and {{Rough Sets}}},
  author = {Czyzewski, Andrzej},
  date = {2003-03},
  journaltitle = {Pattern Recogn. Lett.},
  volume = {24},
  pages = {921--933},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(02)00204-0},
  url = {http://dx.doi.org/10.1016/S0167-8655(02)00204-0},
  urldate = {2017-05-03},
  abstract = {Methods for the identification of direction of the incoming acoustical signal in the presence of noise and reverberation are investigated. Since the problem is a non-deterministic one, thus applications of two learning algorithms, namely neural networks and rough sets are developed to solve it. Consequently, two sets of parameters have been formulated in order to discern target source from unwanted sound source position and then processed by learning algorithms. The applied feature extraction methods are discussed, training processes are described and obtained sound source localizing results are demonstrated and compared.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HCG352CG\\Czyzewski - 2003 - Automatic Identification of Sound Source Position .pdf},
  keywords = {Beamforming,Neural networks,rough sets,signal processing},
  number = {6}
}

@article{CzyzewskiAutomaticIdentificationSound2003,
  title = {Automatic {{Identification}} of {{Sound Source Position Employing Neural Networks}} and {{Rough Sets}}},
  author = {Czyzewski, Andrzej},
  date = {2003-03},
  journaltitle = {Pattern Recogn. Lett.},
  volume = {24},
  pages = {921--933},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(02)00204-0},
  abstract = {Methods for the identification of direction of the incoming acoustical signal in the presence of noise and reverberation are investigated. Since the problem is a non-deterministic one, thus applications of two learning algorithms, namely neural networks and rough sets are developed to solve it. Consequently, two sets of parameters have been formulated in order to discern target source from unwanted sound source position and then processed by learning algorithms. The applied feature extraction methods are discussed, training processes are described and obtained sound source localizing results are demonstrated and compared.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BDFKLLWU\\Czyzewski - 2003 - Automatic Identification of Sound Source Position .pdf},
  keywords = {Beamforming,Neural networks,rough sets,signal processing},
  number = {6}
}

@article{dandachContinuousTimeLinear2009,
  title = {A Continuous Time Linear Adaptive Source Localization Algorithm, Robust to Persistent Drift},
  author = {Dandach, Sandra H. and Fidan, Barış and Dasgupta, Soura and Anderson, Brian D. O.},
  date = {2009-01},
  journaltitle = {Systems \& Control Letters},
  shortjournal = {Systems \& Control Letters},
  volume = {58},
  pages = {7--16},
  issn = {0167-6911},
  doi = {10.1016/j.sysconle.2008.07.008},
  url = {http://www.sciencedirect.com/science/article/pii/S016769110800114X},
  urldate = {2017-01-12},
  abstract = {The problem of source localization has assumed increased importance in recent years. In this paper, we formulate a continuous time adaptive localization algorithm, that permits a mobile agent to estimate the location of a stationary source, using only the measured distance from the source. The algorithm is shown to be exponentially asymptotically stable, under a persistent excitation condition that has an appealing interpretation. We quantify the fact that exponential asymptotic stability endows the algorithm with the ability to track slow, bounded but potentially persistent and nontrivial drift in the source.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R6PFRZI2\\Dandach et al. - 2009 - A continuous time linear adaptive source localizat.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BJSV4ZNZ\\S016769110800114X.html},
  keywords = {Adaptive,Continuous time,Estimation,Persistent excitation,Source localization},
  number = {1}
}

@inproceedings{dangMultipleSoundSource2018,
  title = {Multiple {{Sound Source Localization Based}} on a {{Multi}}-{{Dimensional Assignment Model}}},
  booktitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Dang, X. and Zhu, H. and Cheng, Q.},
  date = {2018-07},
  pages = {1732--1737},
  doi = {10.23919/ICIF.2018.8455616},
  abstract = {In this paper, we address the multiple sound source localization problem using time differences of arrival (TDOAs) of sound sources to a microphone array. Typically, TDOAs are estimated based on the peak extraction of the generalized crosscorrelation function. In multi-source cases, for any given microphone pair, it is hard to tell the correspondence between the sound sources and the extracted peaks. In this work, we develop a novel localization approach based on data association which combines multiple TDOAs from the same source across different microphone pairs. Firstly, the generalized cross correlation-phase transform (GCC-PHAT) function is evaluated and multiple peaks of the GCC function indicating candidate TDOAs are extracted for each pair of microphones. Next, we employ the multi-dimensional assignment algorithm to associate multiple TDOAs from the same source. Finally, multiple sound source localization is carried out based on the obtained TDOA associations across different microphone pairs. Experimental results show the proposed method achieves superior performance for multiple sound source localization compared to the competing algorithm, especially in noisy environments.},
  keywords = {5G mobile communication,acoustic signal processing,CC-PHAT function,data association,GCC-PHAT,generalized cross correlation-phase transform,localization,microphone array,microphone arrays,microphone pairs,multidimensional assignment,multidimensional assignment model,multiple sound source localization problem,time difference of arrival,time differences of arrival}
}

@online{DanteFAQs,
  title = {Dante {{FAQs}}},
  journaltitle = {Audinate},
  url = {https://www.audinate.com/resources/faqs},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\38SNXKQG\\faqs.html}
}

@article{datumArtificialNeuralNetwork1996,
  title = {An Artificial Neural Network for Sound Localization Using Binaural Cues},
  author = {Datum, M. S. and Palmieri, F. and Moiseff, A.},
  date = {1996-07},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {100},
  pages = {372--383},
  issn = {0001-4966},
  abstract = {A three-layer neural network is used to estimate the direction of a sound source from the signals detected by two directional, spatially separate receivers. Although the implemented system does not require any specific knowledge about acoustical parameters or propagation properties, a model of the acoustical environment is used to generate simulated data for training the network. The neural network is trained according to the multiple extended Kalman algorithm (MEKA), which provides fast convergence and does not require intervention for adjustment of the learning parameters. Lower bounds on estimation are computed and compared with simulations using the neural network.},
  eprint = {8675834},
  eprinttype = {pmid},
  keywords = {Acoustics,Algorithms,Learning,Models; Biological,Models; Theoretical,Neural Networks (Computer),Noise,Sound localization},
  langid = {english},
  number = {1}
}

@article{Datumartificialneuralnetwork1996,
  title = {An {{Artificial Neural Network}} for {{Sound Localization Using Binaural Cues}}},
  author = {Datum, M. S. and Palmieri, F. and Moiseff, A.},
  date = {1996-07},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {100},
  pages = {372--383},
  issn = {0001-4966},
  abstract = {A three-layer neural network is used to estimate the direction of a sound source from the signals detected by two directional, spatially separate receivers. Although the implemented system does not require any specific knowledge about acoustical parameters or propagation properties, a model of the acoustical environment is used to generate simulated data for training the network. The neural network is trained according to the multiple extended Kalman algorithm (MEKA), which provides fast convergence and does not require intervention for adjustment of the learning parameters. Lower bounds on estimation are computed and compared with simulations using the neural network.},
  eprint = {8675834},
  eprinttype = {pmid},
  keywords = {Acoustics,Algorithms,Biological,Learning,Models,Neural Networks (Computer),Noise,Sound localization,Theoretical},
  langid = {english},
  number = {1}
}

@article{datumArtificialNeuralNetwork1996a,
  title = {An Artificial Neural Network for Sound Localization Using Binaural Cues},
  author = {Datum, Michael S. and Palmieri, Francesco and Moiseff, Andrew},
  date = {1996-07-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {100},
  pages = {372--383},
  issn = {0001-4966},
  doi = {10.1121/1.415854},
  url = {https://asa.scitation.org/doi/10.1121/1.415854},
  urldate = {2018-11-14},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FK25J73T\\1.html},
  keywords = {no free pdf},
  number = {1}
}

@book{davisSoundSystemEngineering2006,
  title = {Sound {{System Engineering}}},
  author = {Davis, Don and Patronis, Eugene},
  date = {2006},
  publisher = {{Taylor \& Francis}},
  abstract = {Sound System Engineering Third Edition is a complete revision and expansion of the former work. Written by two leading authorities in the field of audio engineering, this highly respected guide covers the fundamentals necessary for the understanding of today's systems as well as for those systems yet to come.  The space formerly occupied by outdated photographs of manufacturers' product and of older system installations has now been filled with new measurements and discussions of the measurement process. The "Mathematics for Audio” chapter has been expanded to include the mathematics of phasors. The "Interfacing Electrical and Acoustic Systems” chapter has a completely new section covering the analysis of alternating current circuits. Additionally, system gain structure is now treated by both the available input power method and the voltage only method, complete with illustrations of each. All chapters dealing with loudspeaker directivity and coverage, the acoustic environment, room acoustics, speech intelligibility, and acoustic gain appear in up to date versions. In addition there is new material on signal delay and synchronization and equalization. There are completely new chapters on microphones, loudspeakers and loudspeaker arrays including line arrays with steering and beam-width control, and signal processing, both analog and digital. The book runs the gamut of sound system design from the simplest all-analog paging system to the largest multipurpose digital systems.  In writing this third edition, the authors kept in mind the needs of sound system installers, sound system service technicians, and sound system designers. All three groups will find the material to be useful for everyday work as well as beneficial in the furtherance of their overall audio education.},
  eprint = {l1ul2d2jh0QC},
  eprinttype = {googlebooks},
  isbn = {978-0-240-80830-7},
  keywords = {Science / Acoustics & Sound},
  langid = {english},
  pagetotal = {506}
}

@online{DeepLearningConvolutional,
  title = {Deep Learning – {{Convolutional}} Neural Networks and Feature Extraction with {{Python}} | {{Terra Incognita}}},
  url = {http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/},
  urldate = {2017-02-17},
  abstract = {Convolutional neural networks (or ConvNets) are biologically-inspired variants of MLPs, they have different kinds of layers and each different layer works},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\P7X8TJMA\\convolutional-neural-networks-and-feature-extraction-with-python.html}
}

@article{DeeplearningConvolutional,
  title = {Deep {{Learning}} – {{Convolutional Neural Networks}} and {{Feature Extraction}} with {{Python}} | {{Terra Incognita}}},
  abstract = {Convolutional neural networks (or ConvNets) are biologically-inspired variants of MLPs, they have different kinds of layers and each different layer works},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FWAU46FP\\convolutional-neural-networks-and-feature-extraction-with-python.html}
}

@online{DefinitionSOUNDFIELD,
  title = {Definition of {{SOUND FIELD}}},
  url = {https://www.merriam-webster.com/dictionary/sound+field},
  urldate = {2017-12-09},
  abstract = {a region in a material medium in which sound waves are being propagated… See the full definition},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RDBQNJ4B\\sound field.html}
}

@inproceedings{dehkordi_compressive_2011,
  title = {A Compressive Sensing Based Compressed Neural Network for Sound Source Localization},
  booktitle = {2011 {{International Symposium}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  author = {Dehkordi, M. B. and Abutalebi, H. R. and Ghanei, H.},
  date = {2011-06},
  pages = {6--10},
  doi = {10.1109/AISP.2011.5960980},
  abstract = {Microphone arrays are today employed to specify the sound source locations in numerous real time applications such as speech processing in large rooms or acoustic echo cancellation. Signal sources may exist in the near field or far field with respect to the microphones. Current Neural Networks (NNs) based source localization approaches assume far field narrowband sources. One of the important limitations of these NN-based approaches is making balance between computational complexity and the development of NNs; an architecture that is too large or too small will affect the performance in terms of generalization and computational cost. In the previous analysis, saliency subject has been employed to determine the most suitable structure, however, it is time-consuming and the performance is not robust. In this paper, a family of new algorithms for compression of NNs is presented based on Compressive Sampling (CS) theory. The proposed framework makes it possible to find a sparse structure for NNs, and then the designed neural network is compressed by using CS. The key difference between our algorithm and the state-of-the-art techniques is that the mapping is continuously done using the most effective features; therefore, the proposed method has a fast convergence. The empirical work demonstrates that the proposed algorithm is an effective alternative to traditional methods in terms of accuracy and computational complexity.},
  eventtitle = {2011 {{International Symposium}} on {{Artificial Intelligence}} and {{Signal Processing}} ({{AISP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BHNABWAR\\Dehkordi et al. - 2011 - A compressive sensing based compressed neural netw.pdf;C\:\\Users\\sauli\\Zotero\\storage\\JPX3W4HP\\5960980.html},
  keywords = {acoustic echo cancellation,acoustic signal processing,Approximation algorithms,Artificial neural networks,Biological neural networks,Classification algorithms,compressed neural network,compressive sampling,compressive sampling theory,compressive sensing,computational complexity,far field narrowband sources,greedy algorithms,microphone arrays,Microphones,multilayer Perceptron,neural nets,neural network,pruning,signal sampling,sound source,sound source localization,Sparse matrices,speech processing,Training}
}

@inproceedings{del_galdo_three-dimensional_2010,
  title = {Three-{{Dimensional Sound Field Analysis}} with {{Directional Audio Coding Based}} on {{Signal Adaptive Parameter Estimators}}},
  author = {Del Galdo, Giovanni and Kuech, Fabian and Prus, Magdalena and Thiergart, Oliver},
  date = {2010-10-08},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15558},
  urldate = {2017-01-10},
  abstract = {Directional audio coding (DirAC) provides an efficient description of spatial sound in terms of an audio downmix signal and parametric side information, namely the direction of arrival and diffuseness of sound. The sound scene can be reproduced based on this information with any audio reproduction system such as multichannel playback or binaural rendering. Input to the DirAC analysis are acoustic signals, e.g., captured by a microphone array. The accuracy of the DirAC parameter estimation can...},
  eventtitle = {Audio {{Engineering Society Conference}}: 40th {{International Conference}}: {{Spatial Audio}}: {{Sense}} the {{Sound}} of {{Space}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9GP5ZWU6\\Del Galdo et al. - 2010 - Three-Dimensional Sound Field Analysis with Direct.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BH2NI2JJ\\browse.html},
  langid = {english}
}

@inproceedings{diaz-guerraDirectionArrivalEstimation2018,
  title = {Direction of {{Arrival Estimation}} with {{Microphone Arrays Using SRP}}-{{PHAT}} and {{Neural Networks}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Diaz-Guerra, D. and Beltran, J. R.},
  date = {2018-07},
  pages = {617--621},
  doi = {10.1109/SAM.2018.8448492},
  abstract = {The Steered Response Power with phase transform (SRP-PHAT) is one of the most employed techniques for Direction of Arrival (DOA) estimation with microphone arrays due its robustness against acoustical conditions as reverberation or noise. Among its main drawbacks is the growth of its computational complexity when the search space increases. To solve this issue, we propose the use of Neural Networks (NN) to obtain the DOA as a regression problem from a low resolution SRP-PHAT power map. The NNs can learn and exploit the information of the acoustic reflections of the room where the array is located with a training method that can be easily performed by an end user without technical knowledge.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SAW7GFTU\\Diaz-Guerra and Beltran - 2018 - Direction of Arrival Estimation with Microphone Ar.pdf;C\:\\Users\\sauli\\Zotero\\storage\\MTP8WYBH\\8448492.html},
  keywords = {acoustic reflections,Array signal processing,Artificial neural networks,computational complexity,direction of arrival,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,DOA,Estimation,low resolution SRP-PHAT power map,microphone array,microphone arrays,Microphone arrays,neural netowrks,neural nets,Neural Networks,regression analysis,Sensor arrays,Sound Source Localization,SRP,SRP-PHAT,SSL,Steered Response Power with phase transform,teleconference system,Training,WASN,Wireless Acoustic Sensor Network,WSN}
}

@inproceedings{diaz-guerraDirectionArrivalEstimation2018a,
  title = {Direction of {{Arrival Estimation}} with {{Microphone Arrays Using SRP}}-{{PHAT}} and {{Neural Networks}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Diaz-Guerra, D. and Beltran, J. R.},
  date = {2018-07},
  pages = {617--621},
  issn = {2151-870X},
  doi = {10.1109/SAM.2018.8448492},
  abstract = {The Steered Response Power with phase transform (SRP-PHAT) is one of the most employed techniques for Direction of Arrival (DOA) estimation with microphone arrays due its robustness against acoustical conditions as reverberation or noise. Among its main drawbacks is the growth of its computational complexity when the search space increases. To solve this issue, we propose the use of Neural Networks (NN) to obtain the DOA as a regression problem from a low resolution SRP-PHAT power map. The NNs can learn and exploit the information of the acoustic reflections of the room where the array is located with a training method that can be easily performed by an end user without technical knowledge.},
  keywords = {acoustic reflections,Array signal processing,Artificial neural networks,computational complexity,direction of arrival,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,DOA,Estimation,low resolution SRP-PHAT power map,microphone array,microphone arrays,Microphone arrays,neural netowrks,neural nets,Neural Networks,regression analysis,Sensor arrays,Sound Source Localization,SRP,SRP-PHAT,SSL,Steered Response Power with phase transform,teleconference system,Training,WASN,Wireless Acoustic Sensor Network,WSN}
}

@incollection{dibiaseRobustLocalizationReverberant2001,
  title = {Robust {{Localization}} in {{Reverberant Rooms}}},
  booktitle = {Microphone {{Arrays}}},
  author = {DiBiase, Joseph H. and Silverman, Harvey F. and Brandstein, Michael S.},
  editor = {Brandstein, Michael and Ward, Darren},
  date = {2001},
  pages = {157--180},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-04619-7_8},
  url = {http://link.springer.com/10.1007/978-3-662-04619-7_8},
  urldate = {2019-08-15},
  abstract = {Talker localization with microphone arrays has received significant attention lately as a means for the automated tracking of individuals in an enclosure and as a necessary component of any general purpose speech capture system. Several algorithmic approaches are available for speech source localization with multi-channel data. This chapter summarizes the current field and comments on the general merits and shortcomings of each genre. A new localization method is then presented in detail. By utilizing key features of existing methods, this new algorithm is shown to be significantly more robust to acoustical conditions, particularly reverberation effects, than the traditional localization techniques in use today.},
  editorb = {Lacroix, Arild and Venetsanopoulos, Anastasios},
  editorbtype = {redactor},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8H3NWU2D\\DiBiase et al. - 2001 - Robust Localization in Reverberant Rooms.pdf},
  isbn = {978-3-642-07547-6 978-3-662-04619-7},
  keywords = {*****},
  langid = {english}
}

@inproceedings{digheExploitingEigenposteriorsSemiSupervised2017,
  title = {Exploiting {{Eigenposteriors}} for {{Semi}}-{{Supervised Training}} of {{DNN Acoustic Models}} with {{Sequence Discrimination}}},
  booktitle = {{{INTERSPEECH}}},
  author = {Dighe, Pranay and Asaei, Afsaneh and Bourlard, Hervé},
  date = {2017},
  doi = {10.21437/interspeech.2017-1784},
  abstract = {Deep neural network (DNN) acoustic models yield posterior probabilities of senone classes. Recent studies support the existence of low-dimensional subspaces underlying senone posteriors. Principal component analysis (PCA) is applied to identify eigenposteriors and perform low-dimensional projection of the training data posteriors. The resulted enhanced posteriors are applied as soft targets for training better DNN acoustic model under the student-teacher framework. The present work advances this approach by studying incorporation of sequence discriminative training. We demonstrate how to combine the gains from eigenposterior based enhancement with sequence discrimination to improve ASR using semi-supervised training. Evaluation on AMI meeting corpus yields nearly 4\% absolute reduction in word error rate (WER) compared to the baseline DNN trained with cross entropy objective. In this context, eigenposterior enhancement of the soft targets is crucial to enable additive improvement using out-of-domain untranscribed data.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6GPPZW3W\\Dighe et al. - 2017 - Exploiting Eigenposteriors for Semi-Supervised Tra.pdf},
  keywords = {Acoustic cryptanalysis,Acoustic model,Artificial neural network,Baseline (configuration management),Cross entropy,Deep learning,Discriminative model,Principal component analysis,Semi-supervised learning,Semiconductor industry,Utility functions on indivisible goods,Word error rate}
}

@inproceedings{dimoulas_improved_2009,
  title = {Improved {{Localization}} of {{Sound Sources Using Multi}}-{{Band Processing}} of {{Ambisonic Components}}},
  author = {Dimoulas, Charalampos and Kalliris, George and Avdelidis, Konstantinos and Papanikolaou, George},
  date = {2009-05-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=14887},
  urldate = {2017-01-10},
  abstract = {The current paper focuses on the use of multi-band ambisonic-processing for improved sound source localization. Energy-based localization can be easily delivered using soundfield microphone pairs, as long as free field conditions and the single omni-directional-point-source model apply. Multi-band SNR-based selective processing improves the noise tolerance and the localization accuracy, eliminating the influence of reverberation and background noise. Band-related sound-localization statistics...},
  eventtitle = {Audio {{Engineering Society Convention}} 126},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QTPRVXUE\\Improved Localization of Sound Sources Using Multi-Band Processing of Ambisonic Components.pdf;C\:\\Users\\sauli\\Zotero\\storage\\852HVERI\\browse.html},
  langid = {english}
}

@online{DirectionArrivalEstimation,
  title = {(2) {{Direction}} of {{Arrival Estimation Using Microphone Array Processing}} for {{Moving Humanoid Robots}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/281487994_Direction_of_Arrival_Estimation_Using_Microphone_Array_Processing_for_Moving_Humanoid_Robots/figures?lo=1},
  urldate = {2018-05-29},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TZDDHN3M\\(2) Direction of Arrival Estimation Using Micropho.pdf;C\:\\Users\\sauli\\Zotero\\storage\\62QZ96L9\\figures.html},
  langid = {english}
}

@online{DirectionArrivalOverview,
  title = {Direction of {{Arrival}} - an Overview | {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/engineering/direction-of-arrival},
  urldate = {2019-03-22},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\66WA9WLU\\direction-of-arrival.html}
}

@online{DirectPathRelativeTransfer2015,
  title = {Direct-{{Path Relative Transfer Function}} for {{Speaker Localization}}},
  date = {2015-07-06T10:56:39+00:00},
  url = {https://team.inria.fr/perception/research/ssl-rtf/},
  urldate = {2019-10-22},
  abstract = {Xiaofei Li, Yutong Ban, Laurent Girin, Xavier Alameda-Pineda and Radu Horaud.~Online Localization and Tracking of Multiple Moving Speakers in Reverberant Environments.~~IEEE Journal of Selected Topics in Signal Processing, 13 (1), pp. 88 - 103, 2019. [research page] ~ Xiaofei Li, Laurent Girin, Radu Horaud, Sharon Gannot. Multiple-Speaker Localization Based on Direct-Path Features and Likelihood Maximization ...},
  langid = {american}
}

@online{DirectPathRelativeTransfer2015a,
  title = {Direct-{{Path Relative Transfer Function}} for {{Speaker Localization}}},
  date = {2015-07-06T10:56:39+00:00},
  url = {https://team.inria.fr/perception/research/ssl-rtf/},
  urldate = {2019-10-21},
  abstract = {Xiaofei Li, Yutong Ban, Laurent Girin, Xavier Alameda-Pineda and Radu Horaud.~Online Localization and Tracking of Multiple Moving Speakers in Reverberant Environments.~~IEEE Journal of Selected Topics in Signal Processing, 13 (1), pp. 88 - 103, 2019. [research page] ~ Xiaofei Li, Laurent Girin, Radu Horaud, Sharon Gannot. Multiple-Speaker Localization Based on Direct-Path Features and Likelihood Maximization ...},
  langid = {american}
}

@article{dmochowskiGeneralizedSteeredResponse2007,
  title = {A {{Generalized Steered Response Power Method}} for {{Computationally Viable Source Localization}}},
  author = {Dmochowski, J. P. and Benesty, J. and Affes, S.},
  date = {2007-11},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  pages = {2510--2526},
  issn = {1558-7916},
  doi = {10.1109/TASL.2007.906694},
  abstract = {The process of locating an acoustic source given measurements of the sound field at multiple microphones is of significant interest as both a classical array signal processing problem, and more recently, as a solution to the problems of automatic camera steering, teleconferencing, hands-free processing, and others. Despite the proven efficacy of steered-beamformer approaches to localization in harsh conditions, their practical application to real-time settings is hindered by undesirably high computational demands. This paper presents a computationally viable implementation of the steered response power (SRP) source localization method. The conventional approach is generalized by introducing an inverse mapping that maps relative delays to sets of candidate locations. Instead of traversing the three-dimensional location space, the one-dimensional relative delay space is traversed; at each lag, all locations which are inverse mapped by that delay are updated. This means that the computation of the SRP map is no longer performed sequentially in space. Most importantly, by subsetting the space of relative delays to only those that achieve a high level of cross-correlation, the required number of algorithm updates is drastically reduced without compromising localization accuracy. The generalization is scalable in the sense that the level of subsetting is an algorithm parameter. It is shown that this generalization may be viewed as a spatial decomposition of the SRP energy map into weighted basis functions-in this context, it becomes evident that the full SRP search considers all basis functions (even the ones with very low weighting). On the other hand, it is shown that by only including a few basis functions per microphone pair, the SRP map is quite accurately represented. As a result, in a real environment, the proposed generalization achieves virtually the same anomaly rate as the full SRP search while only performing 10\% the amount of algorithm updates as the full sea- rch.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3WMNXCWM\\4317561.html},
  keywords = {acoustic arrays,Acoustic measurements,acoustic signal processing,array signal processing,array signal processing problem,Cameras,computationally viable acoustic source localization,Delay,inverse mapping,Jacobian matrices,microphone arrays,multiple microphones,Position measurement,Signal processing algorithms,Source localization,spectral estimation,SRP source localization method,steered response power (SRP),steered response power method,steered-beamformer approaches},
  number = {8}
}

@article{DmochowskiGeneralizedSteeredResponse2007,
  title = {A {{Generalized Steered Response Power Method}} for {{Computationally Viable Source Localization}}},
  author = {Dmochowski, J. P. and Benesty, J. and Affes, S.},
  date = {2007-11},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  pages = {2510--2526},
  issn = {1558-7916},
  doi = {10.1109/TASL.2007.906694},
  abstract = {The process of locating an acoustic source given measurements of the sound field at multiple microphones is of significant interest as both a classical array signal processing problem, and more recently, as a solution to the problems of automatic camera steering, teleconferencing, hands-free processing, and others. Despite the proven efficacy of steered-beamformer approaches to localization in harsh conditions, their practical application to real-time settings is hindered by undesirably high computational demands. This paper presents a computationally viable implementation of the steered response power (SRP) source localization method. The conventional approach is generalized by introducing an inverse mapping that maps relative delays to sets of candidate locations. Instead of traversing the three-dimensional location space, the one-dimensional relative delay space is traversed; at each lag, all locations which are inverse mapped by that delay are updated. This means that the computation of the SRP map is no longer performed sequentially in space. Most importantly, by subsetting the space of relative delays to only those that achieve a high level of cross-correlation, the required number of algorithm updates is drastically reduced without compromising localization accuracy. The generalization is scalable in the sense that the level of subsetting is an algorithm parameter. It is shown that this generalization may be viewed as a spatial decomposition of the SRP energy map into weighted basis functions-in this context, it becomes evident that the full SRP search considers all basis functions (even the ones with very low weighting). On the other hand, it is shown that by only including a few basis functions per microphone pair, the SRP map is quite accurately represented. As a result, in a real environment, the proposed generalization achieves virtually the same anomaly rate as the full SRP search while only performing 10\% the amount of algorithm updates as the full sea- rch.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FMZ7NKBJ\\4317561.html},
  keywords = {acoustic arrays,Acoustic measurements,acoustic signal processing,array signal processing,array signal processing problem,Cameras,computationally viable acoustic source localization,Delay,inverse mapping,Jacobian matrices,microphone arrays,multiple microphones,Position measurement,Signal processing algorithms,Source localization,spectral estimation,SRP source localization method,steered response power (SRP),steered response power method,steered-beamformer approaches},
  number = {8}
}

@online{Doi101016,
  title = {Doi:10.1016/{{S0925}}-2312(03)00395-3 - 74d9ad5784ffeb32c490541868de858192a7.Pdf},
  url = {https://pdfs.semanticscholar.org/2a79/74d9ad5784ffeb32c490541868de858192a7.pdf},
  urldate = {2017-04-03},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CEX2H2QR\\74d9ad5784ffeb32c490541868de858192a7.html}
}

@article{doi101016,
  title = {Doi:10.1016/{{S0925}}-2312(03)00395-3 - 74d9ad5784ffeb32c490541868de858192a7.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/2a79/74d9ad5784ffeb32c490541868de858192a7.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4VBWME9M\\doi10.1016S0925-2312(03)00395-3 - 74d9ad5784ffeb.html}
}

@online{Doi101016a,
  title = {Doi:10.1016/{{S0925}}-2312(03)00395-3 - 74d9ad5784ffeb32c490541868de858192a7.Pdf},
  url = {https://pdfs.semanticscholar.org/2a79/74d9ad5784ffeb32c490541868de858192a7.pdf},
  urldate = {2017-02-08},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R9SWW2DG\\doi10.1016S0925-2312(03)00395-3 - 74d9ad5784ffeb.html}
}

@article{doi101016a,
  title = {Doi:10.1016/{{S0925}}-2312(03)00395-3 - 74d9ad5784ffeb32c490541868de858192a7.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/2a79/74d9ad5784ffeb32c490541868de858192a7.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\T8PRRSGI\\74d9ad5784ffeb32c490541868de858192a7.html}
}

@article{dokmanicAcousticEchoesReveal2013,
  title = {Acoustic Echoes Reveal Room Shape},
  author = {Dokmanić, Ivan and Parhizkar, Reza and Walther, Andreas and Lu, Yue M. and Vetterli, Martin},
  date = {2013-07-23},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  volume = {110},
  pages = {12186--12191},
  issn = {0027-8424},
  doi = {10.1073/pnas.1221464110},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3725100/},
  abstract = {Imagine that you are blindfolded inside an unknown room. You snap your fingers and listen to the room’s response. Can you hear the shape of the room? Some people can do it naturally, but can we design computer algorithms that hear rooms? We show how to compute the shape of a convex polyhedral room from its response to a known sound, recorded by a few microphones. Geometric relationships between the arrival times of echoes enable us to “blindfoldedly” estimate the room geometry. This is achieved by exploiting the properties of Euclidean distance matrices. Furthermore, we show that under mild conditions, first-order echoes provide a unique description of convex polyhedral rooms. Our algorithm starts from the recorded impulse responses and proceeds by learning the correct assignment of echoes to walls. In contrast to earlier methods, the proposed algorithm reconstructs the full 3D geometry of the room from a single sound emission, and with an arbitrary geometry of the microphone array. As long as the microphones can hear the echoes, we can position them as we want. Besides answering a basic question about the inverse problem of room acoustics, our results find applications in areas such as architectural acoustics, indoor localization, virtual reality, and audio forensics.},
  eprint = {23776236},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NSA44RE4\\Dokmanić et al. - 2013 - Acoustic echoes reveal room shape.pdf},
  number = {30},
  pmcid = {PMC3725100}
}

@article{dokmanicAcousticEchoesReveal2013a,
  title = {Acoustic Echoes Reveal Room Shape},
  author = {Dokmanic, I. and Parhizkar, R. and Walther, A. and Lu, Y. M. and Vetterli, M.},
  date = {2013-07-23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  pages = {12186--12191},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1221464110},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1221464110},
  urldate = {2020-02-21},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3F2BWS8V\\Dokmanic et al. - 2013 - Acoustic echoes reveal room shape.pdf},
  langid = {english},
  number = {30}
}

@inproceedings{doRealTimeSRPPHATSource2007,
  title = {A {{Real}}-{{Time SRP}}-{{PHAT Source Location Implementation}} Using {{Stochastic Region Contraction}}({{SRC}}) on a {{Large}}-{{Aperture Microphone Array}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Do, H. and Silverman, H. F. and Yu, Y.},
  date = {2007-04},
  volume = {1},
  pages = {I-121-I-124},
  doi = {10.1109/ICASSP.2007.366631},
  abstract = {In most microphone array applications, it is essential to localize sources in a noisy, reverberant environment. It has been shown that computing the steered response power (SRP) is more robust than faster, two-stage, direct time-difference of arrival methods. The problem with computing SRP is that the SRP space has many local maxima and thus computationally-intensive grid-search methods are used to find a global maximum. Grid search is too expensive for a real-time system. Several papers have addressed this issue. In this paper we propose using stochastic region contraction (SRC) to make computing the SRP practical. We discuss one important SRP method, computing it from the phase transform (SRP-PHAT), review SRC, and show the computational saving. Using real data from human talkers, we show that SRC saves computation by more than two orders of magnitude with almost no loss in accuracy.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DBIPH94A\\Do et al. - 2007 - A Real-Time SRP-PHAT Source Location Implementatio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3WPHBWS2\\4217031.html},
  keywords = {Acoustic noise,acoustic position measurement,acoustic signal processing,Arrays,computationally-intensive grid-search methods,direct time-difference of arrival methods,Grid computing,large-aperture microphone array applications,microphone arrays,Microphones,Optimization methods,phase transform,Position measurement,Power engineering and energy,Power engineering computing,Real time systems,real-time SRP-PHAT source location implementation,reverberant environment,Robustness,Steered response power,stochastic processes,stochastic region contraction,System testing,time-of-arrival estimation}
}

@inproceedings{doRealTimeSRPPHATSource2007a,
  title = {A {{Real}}-{{Time SRP}}-{{PHAT Source Location Implementation}} Using {{Stochastic Region Contraction}}({{SRC}}) on a {{Large}}-{{Aperture Microphone Array}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Do, H. and Silverman, H. F. and Yu, Y.},
  date = {2007-04},
  volume = {1},
  pages = {I-121-I-124},
  doi = {10.1109/ICASSP.2007.366631},
  abstract = {In most microphone array applications, it is essential to localize sources in a noisy, reverberant environment. It has been shown that computing the steered response power (SRP) is more robust than faster, two-stage, direct time-difference of arrival methods. The problem with computing SRP is that the SRP space has many local maxima and thus computationally-intensive grid-search methods are used to find a global maximum. Grid search is too expensive for a real-time system. Several papers have addressed this issue. In this paper we propose using stochastic region contraction (SRC) to make computing the SRP practical. We discuss one important SRP method, computing it from the phase transform (SRP-PHAT), review SRC, and show the computational saving. Using real data from human talkers, we show that SRC saves computation by more than two orders of magnitude with almost no loss in accuracy.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\K2CGD6G2\\4217031.html},
  keywords = {Acoustic noise,acoustic position measurement,acoustic signal processing,Arrays,computationally-intensive grid-search methods,direct time-difference of arrival methods,Grid computing,large-aperture microphone array applications,microphone arrays,Microphones,Optimization methods,phase transform,Position measurement,Power engineering and energy,Power engineering computing,Real time systems,real-time SRP-PHAT source location implementation,reverberant environment,Robustness,Steered response power,stochastic processes,stochastic region contraction,System testing,time-of-arrival estimation}
}

@inproceedings{DoRealTimeSRPPHATSource2007a,
  title = {A {{Real}}-{{Time SRP}}-{{PHAT Source Location Implementation Using Stochastic Region Contraction}}({{SRC}}) on a {{Large}}-{{Aperture Microphone Array}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Do, H. and Silverman, H. F. and Yu, Y.},
  date = {2007-04},
  volume = {1},
  pages = {I-121--I--124},
  doi = {10.1109/ICASSP.2007.366631},
  abstract = {In most microphone array applications, it is essential to localize sources in a noisy, reverberant environment. It has been shown that computing the steered response power (SRP) is more robust than faster, two-stage, direct time-difference of arrival methods. The problem with computing SRP is that the SRP space has many local maxima and thus computationally-intensive grid-search methods are used to find a global maximum. Grid search is too expensive for a real-time system. Several papers have addressed this issue. In this paper we propose using stochastic region contraction (SRC) to make computing the SRP practical. We discuss one important SRP method, computing it from the phase transform (SRP-PHAT), review SRC, and show the computational saving. Using real data from human talkers, we show that SRC saves computation by more than two orders of magnitude with almost no loss in accuracy.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SC53GA8V\\Do et al. - 2007 - A Real-Time SRP-PHAT Source Location Implementatio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EIB7Y4UI\\4217031.html},
  keywords = {Acoustic noise,acoustic position measurement,acoustic signal processing,Arrays,computationally-intensive grid-search methods,direct time-difference of arrival methods,Grid computing,large-aperture microphone array applications,microphone arrays,Microphones,Optimization methods,phase transform,Position measurement,Power engineering and energy,Power engineering computing,Real time systems,real-time SRP-PHAT source location implementation,reverberant environment,Robustness,Steered response power,stochastic processes,stochastic region contraction,System testing,time-of-arrival estimation}
}

@inproceedings{doRealTimeSRPPHATSource2007b,
  title = {A {{Real}}-{{Time SRP}}-{{PHAT Source Location Implementation}} Using {{Stochastic Region Contraction}}({{SRC}}) on a {{Large}}-{{Aperture Microphone Array}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Do, H. and Silverman, H. F. and Yu, Y.},
  date = {2007-04},
  volume = {1},
  pages = {I-121-I-124},
  doi = {10.1109/ICASSP.2007.366631},
  abstract = {In most microphone array applications, it is essential to localize sources in a noisy, reverberant environment. It has been shown that computing the steered response power (SRP) is more robust than faster, two-stage, direct time-difference of arrival methods. The problem with computing SRP is that the SRP space has many local maxima and thus computationally-intensive grid-search methods are used to find a global maximum. Grid search is too expensive for a real-time system. Several papers have addressed this issue. In this paper we propose using stochastic region contraction (SRC) to make computing the SRP practical. We discuss one important SRP method, computing it from the phase transform (SRP-PHAT), review SRC, and show the computational saving. Using real data from human talkers, we show that SRC saves computation by more than two orders of magnitude with almost no loss in accuracy.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9V5JPEUY\\Do et al. - 2007 - A Real-Time SRP-PHAT Source Location Implementatio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\VWQJWYWB\\4217031.html},
  keywords = {Acoustic noise,acoustic position measurement,acoustic signal processing,arrays,computationally-intensive grid-search methods,direct time-difference of arrival methods,Grid computing,large-aperture microphone array applications,microphone arrays,Microphone arrays,microphones,Optimization methods,phase transform,Position measurement,Power engineering and energy,Power engineering computing,Real time systems,real-time SRP-PHAT source location implementation,reverberant environment,Robustness,steered response power,stochastic processes,Stochastic processes,stochastic region contraction,System testing,time-of-arrival estimation}
}

@thesis{doREALTIMESRPPHATSOURCE2009,
  title = {{{REAL}}-{{TIME SRP}}-{{PHAT SOURCE LOCATION IMPLEMENTATIONS ON A LARGE}}-{{APERTURE MICROPHONE}}},
  author = {Do, Hoang Tran Huy},
  date = {2009},
  institution = {{Brown University}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\93HALHD5\\Do - REAL-TIME SRP-PHAT SOURCE LOCATION IMPLEMENTATIONS.pdf},
  langid = {english},
  type = {Master thesis}
}

@article{dorfanDistributedExpectationMaximizationAlgorithm2018,
  title = {Distributed {{Expectation}}-{{Maximization Algorithm}} for {{Speaker Localization}} in {{Reverberant Environments}}},
  author = {Dorfan, Y. and Plinge, A. and Hazan, G. and Gannot, S.},
  date = {2018-03},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {682--695},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2788198},
  abstract = {Localization of acoustic sources has attracted a considerable amount of research attention in recent years. A major obstacle to achieving high localization accuracy is the presence of reverberation, the influence of which obviously increases with the number of active speakers in the room. Human hearing is capable of localizing acoustic sources even in extreme conditions. In this study, we propose to combine a method based on human hearing mechanisms and a modified incremental distributed expectation-maximization (IDEM) algorithm. Rather than using phase difference measurements that are modeled by a mixture of complex-valued Gaussians, as proposed in the original IDEM framework, we propose to use time difference of arrival measurements in multiple subbands and model them by a mixture of real-valued truncated Gaussians. Moreover, we propose to first filter the measurements in order to reduce the effect of the multipath conditions. The proposed method is evaluated using both simulated data and real-life recordings.},
  keywords = {Acoustic measurements,acoustic radiators,acoustic source localization,active speakers,auditory scene analysis,complex-valued Gaussian mixture model,Direction-of-arrival estimation,distributed algorithms,distributed expectation-maximization,Estimation,expectation-maximisation algorithm,extreme conditions,filter the measurements,Gaussian processes,hearing,high localization accuracy,human hearing mechanisms,incremental expectation-maximization,Microphones,mixture models,modified incremental distributed expectation-maximization algorithm,multi-path,multipath conditions,multiple subbands,onset dominance,original IDEM framework,phase difference measurements,Precedence effect,real-valued truncated Gaussian mixture,reverberant environments,reverberation,Reverberation,sound source localization,speaker localization,spectral masking,Speech,speech processing,Speech processing,time difference of arrival,time difference of arrival measurements,time-of-arrival estimation,truncated Gaussian},
  number = {3}
}

@article{dorfanDistributedExpectationMaximizationAlgorithm2018a,
  title = {Distributed {{Expectation}}-{{Maximization Algorithm}} for {{Speaker Localization}} in {{Reverberant Environments}}},
  author = {Dorfan, Yuval and Plinge, Axel and Hazan, Gershon and Gannot, Sharon},
  date = {2018-03},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{682-695\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2017.2788198\}},
  abstract = {Localization of acoustic sources has attracted a considerable amount of research attention in recent years. A major obstacle to achieving high localization accuracy is the presence of reverberation, the influence of which obviously increases with the number of active speakers in the room. Human hearing is capable of localizing acoustic sources even in extreme conditions. In this study, we propose to combine a method based on human hearing mechanisms and a modified incremental distributed expectation-maximization (IDEM) algorithm. Rather than using phase difference measurements that are modeled by a mixture of complex-valued Gaussians, as proposed in the original IDEM framework, we propose to use time difference of arrival measurements in multiple subbands and model them by a mixture of real-valued truncated Gaussians. Moreover, we propose to first filter the measurements in order to reduce the effect of the multipath conditions. The proposed method is evaluated using both simulated data and real-life recordings.},
  affiliation = {Gannot, S (Reprint Author), Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel. Dorfan, Yuval; Hazan, Gershon; Gannot, Sharon, Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel. Plinge, Axel, Tech Univ Dortmund, Dept Comp Sci, D-44227 Dortmund, Germany.},
  author-email = {dorfany@gmail.com axel.plinge@tu-dortmund.de hazanshl@gmail.com Sharon.Gannot@biu.ac.il},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Ampeliods D, 2010, SIGNAL PROCESS, V90, P1300, DOI 10.1016/j.sigpro.2009.10.015. Antonacci F, 2005, INT CONF ACOUST SPEE, P1061. Bertrand A., 2011, P IEEE S COMM VEH TE, P1, DOI DOI 10.1109/SCVT.2011.6101302. Bishop C. M., 2006, PATTERN RECOGNITION. Blatt D, 2006, IEEE T SIGNAL PROCES, V54, P3614, DOI 10.1109/TSP.2006.879312. Brandstein M., 2001, MICROPHONE ARRAYS SI, V8. Brutti A, 2008, INT CONF ACOUST SPEE, P4349, DOI 10.1109/ICASSP.2008.4518618. Burck M, 2007, J ACOUST SOC AM, V122, P2226, DOI 10.1121/1.2770545. Burshtein D, 2002, IEEE T SPEECH AUDI P, V10, P341, DOI 10.1109/TSA.2002.803420. Busso C, 2005, INT CONF ACOUST SPEE, P1117. Canclini A, 2015, IEEE-ACM T AUDIO SPE, V23, P1563, DOI 10.1109/TASLP.2015.2439040. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. Chakrabarty S, 2017, IEEE WORK APPL SIG, P136, DOI 10.1109/WASPAA.2017.8170010. Chen M., 2007, APPL SIGN PROC AUD A, P22. Cherkassky D, 2017, IEEE-ACM T AUDIO SPE, V25, P651, DOI 10.1109/TASLP.2017.2655259. Cooke M, 2006, J ACOUST SOC AM, V119, P1562, DOI 10.1121/1.2166600. Do H, 2007, INT CONF ACOUST SPEE, P121. Dorfan Y., 2016, P IEEE INT C SCI EL. Dorfan Y, 2016, EUR SIGNAL PR CONF, P1003, DOI 10.1109/EUSIPCO.2016.7760399. Dorfan Y, 2015, EUR SIGNAL PR CONF, P1256, DOI 10.1109/EUSIPCO.2015.7362585. Dorfan Y, 2015, IEEE-ACM T AUDIO SPE, V23, P1692, DOI 10.1109/TASLP.2015.2444654. Dorfan Y, 2014, 2014 4TH JOINT WORKSHOP ON HANDS-FREE SPEECH COMMUNICATION AND MICROPHONE ARRAYS (HSCMA), P72, DOI 10.1109/HSCMA.2014.6843254. Firoozabadi A. D., 2013, P 21 IR C EL ENG. Firoozabadi AD, 2012, 2012 SIXTH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P907, DOI 10.1109/ISTEL.2012.6483115. GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T. Griffin A., 2013, IEEE WORKSH APPL SIG. Grothe B, 2003, NAT REV NEUROSCI, V4, P540, DOI 10.1038/nrn1136. Hassan AS, 2013, PROCD SOC BEHV, V91, P1, DOI 10.1016/j.sbspro.2013.08.395. Hassani A, 2015, SIGNAL PROCESS, V107, P68, DOI 10.1016/j.sigpro.2014.09.001. Hennecke M. H., 2011, 2011 Joint Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA 2011), P127, DOI 10.1109/HSCMA.2011.5942378. Jank W, 2006, PERSPECTIVES OPERATI, V36, P367. Kwon B, 2010, INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2010), P2070. Laufer-Goldshtein B, 2017, IEEE-ACM T AUDIO SPE, V25, P1477, DOI 10.1109/TASLP.2017.2696310. Laufer-Goldshtein B, 2017, LECT NOTES COMPUT SC, V10169, P59, DOI 10.1007/978-3-319-53547-0_6. Lee G, 2012, COMPUT STAT DATA AN, V56, P2816, DOI 10.1016/j.csda.2012.03.003. Li D, 2003, EURASIP J APPL SIG P, V2003, P321, DOI 10.1155/S1110865703212075. Li WL, 2013, IEEE T VEH TECHNOL, V62, P2824, DOI 10.1109/TVT.2013.2247073. Liu ZC, 2007, INT CONF ACOUST SPEE, P761. Lyon R. F., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P1148. Lyon Richard F., 2017, HUMAN MACHINE HEARIN. Madhu N., 2010, P INT S COMM CONTR S, P1. Mandel M. I., 2007, ADV NEURAL INFORM PR, P953. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. Markovic I, 2010, ROBOT AUTON SYST, V58, P1185, DOI 10.1016/j.robot.2010.08.001. Martin R., 2008, ADV DIGITAL SPEECH T. May T., 2013, TECHNOLOGY BINAURAL, P397. Meesookho C, 2008, IEEE T SIGNAL PROCES, V56, P365, DOI 10.1109/TSP.2007.900757. Meng W, 2011, IEEE T INSTRUM MEAS, V60, P1017, DOI 10.1109/TIM.2010.2047035. NADAS A, 1989, IEEE T ACOUST SPEECH, V37, P1495, DOI 10.1109/29.35387. Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355. Nesta F, 2012, INT CONF ACOUST SPEE, P213, DOI 10.1109/ICASSP.2012.6287855. Nowak RD, 2003, IEEE T SIGNAL PROCES, V51, P2245, DOI 10.1109/TSP.2003.814623. Ocal Orhan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1429, DOI 10.1109/ICASSP.2014.6853833. Oualil Y., 2013, P EUR SIGN PROC C MA. Palomaki KJ, 2004, SPEECH COMMUN, V43, P361, DOI 10.1016/j.specom.2004.03.005. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Pertila P., 2011, 2011 Joint Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA 2011), P139, DOI 10.1109/HSCMA.2011.5942380. Pertila P, 2012, EUR SIGNAL PR CONF, P1314. Pertila P, 2008, EURASIP J AUDIO SPEE, DOI 10.1155/2008/278185. Plinge Axel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P614, DOI 10.1109/ICASSP.2014.6853669. Plinge A., 2012, P INT WORKSH AC SIGN. Plinge A., 2010, P INT WORKSH AC ECH. Plinge A., 2013, P EUR SIGN PROC C, P1. Plinge A, 2017, IEEE SIGNAL PROC LET, V24, P324, DOI 10.1109/LSP.2017.2662065. Plinge A, 2016, IEEE SIGNAL PROC MAG, V33, P14, DOI 10.1109/MSP.2016.2555198. Plinge A, 2011, INT CONF ACOUST SPEE, P2476. Ribeiro F, 2010, IEEE INT CON MULTI, P731, DOI 10.1109/ICME.2010.5583886. Sawada H, 2007, IEEE T AUDIO SPEECH, V15, P1592, DOI 10.1109/TASL.2007.899218. Schwartz O., 2016, P INT WORKSH AC ECH. Schwartz O, 2017, 2017 HANDS-FREE SPEECH COMMUNICATIONS AND MICROPHONE ARRAYS (HSCMA 2017), P86, DOI 10.1109/HSCMA.2017.7895567. Schwartz O, 2014, IEEE-ACM T AUDIO SPE, V22, P392, DOI 10.1109/TASLP.2013.2292361. Shen Q, 2015, IEEE-ACM T AUDIO SPE, V23, P1445, DOI 10.1109/TASLP.2015.2436214. Swartling M, 2006, INT CONF ACOUST SPEE, P833. Taseska M., 2015, P INT C MACH LEARN S, P59. Teng J, 2012, IEEE T VEH TECHNOL, V61, P2305, DOI 10.1109/TVT.2012.2190631. Tervo S, 2015, IEEE-ACM T AUDIO SPE, V23, P1539, DOI 10.1109/TASLP.2015.2439573. Tian Y, 2015, IEEE-ACM T AUDIO SPE, V23, P1637, DOI 10.1109/TASLP.2015.2442418. Unoki M, 1999, SPEECH COMMUN, V27, P261, DOI 10.1016/S0167-6393(98)00077-6. Wang D. L., 2006, COMPUTATIONAL AUDITO. Wolff T., 2008, P ITG C VOIC COMM SP. Xue W, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P142. Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.V8896. Zhao QP, 2012, PATTERN RECOGN LETT, V33, P2120, DOI 10.1016/j.patrec.2012.06.017.},
  da = {2018-10-18},
  doc-delivery-number = {FU0GF},
  funding-acknowledgement = {FITweltweit Programme of the German Academic Exchange Service (DAAD)},
  funding-text = {The work of A. Plinge was supported by a fellowship within the FITweltweit Programme of the German Academic Exchange Service (DAAD). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Mads Graesboll Christensen.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Precedence effect,auditory scene analysis,distributed  expectation-maximization,incremental expectation-maximization,multi-path,onset dominance,sound source  localization,spectral masking,time difference of arrival),truncated Gaussian},
  keywords-plus = {OF-ARRIVAL ESTIMATION; ACOUSTIC SOURCE LOCALIZATION; SOUND SOURCE LOCALIZATION; WIRELESS SENSOR NETWORKS; GAUSSIAN MIXTURE-MODELS; MICROPHONE ARRAY; EM ALGORITHMS; SPEECH RECOGNITION; SOURCE SEPARATION; TRACKING},
  langid = {english},
  number = {3},
  number-of-cited-references = {84},
  research-areas = {Acoustics; Engineering},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000423528700018},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{doSRPPHATMethodsLocating2010,
  title = {{{SRP}}-{{PHAT}} Methods of Locating Simultaneous Multiple Talkers Using a Frame of Microphone Array Data},
  author = {Do, Hoang and Silverman, Harvey F.},
  date = {2010},
  journaltitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  doi = {10.1109/ICASSP.2010.5496133},
  abstract = {Two new methods for locating multiple sound sources using a single segment of data from a large-aperture microphone array are presented. Both methods employ the proven-robust steered response power using the phase transform (SRP-PHAT) as a functional. To cluster the data points into highly probable regions containing global peaks, the first method fits a Gaussian mixture model (GMM), whereas the second one sequentially finds the points with highest SRP-PHAT values that most likely represent different clusters. Then the low-cost global optimization method, stochastic region contraction (SRC), is applied to each cluster to find the global peaks. We test the two methods using real data from five simultaneous talkers in a room with high noise and reverberation. Results are presented and discussed.}
}

@inproceedings{doSRPPHATMethodsLocating2010a,
  title = {{{SRP}}-{{PHAT}} Methods of Locating Simultaneous Multiple Talkers Using a Frame of Microphone Array Data},
  booktitle = {2010 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Do, Hoang and Silverman, Harvey F.},
  date = {2010-03},
  pages = {125--128},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2010.5496133},
  abstract = {Two new methods for locating multiple sound sources using a single segment of data from a large-aperture microphone array are presented. Both methods employ the proven-robust steered response power using the phase transform (SRP-PHAT) as a functional. To cluster the data points into highly probable regions containing global peaks, the first method fits a Gaussian mixture model (GMM), whereas the second one sequentially finds the points with highest SRP-PHAT values that most likely represent different clusters. Then the low-cost global optimization method, stochastic region contraction (SRC), is applied to each cluster to find the global peaks. We test the two methods using real data from five simultaneous talkers in a room with high noise and reverberation. Results are presented and discussed.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FS4PN6C3\\Do and Silverman - 2010 - SRP-PHAT methods of locating simultaneous multiple.pdf;C\:\\Users\\sauli\\Zotero\\storage\\FAN3Y95G\\5496133.html},
  keywords = {acoustic arrays,Acoustic noise,acoustic position measurement,Acoustic radiators,acoustic signal processing,Acoustical engineering,Clustering algorithms,Data engineering,Gaussian mixture model,Gaussian processes,global optimization,GMM,Integrated circuit modeling,large-aperture microphone array,microphone arrays,Microphone arrays,microphones,optimisation,Optimization methods,phase transform,Position measurement,Power engineering and energy,Reverberation,SRC,SRP-PHAT method,steered response power,stochastic region contraction}
}

@inproceedings{dou_algorithm_2016,
  title = {An Algorithm of Sound Source Localization Using Range Differences of Arrival and Energy Ratios of Arrival},
  booktitle = {2016 {{IEEE}} 11th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Dou, Y. and Wang, H. and Ma, S.},
  date = {2016-06},
  pages = {1547--1550},
  doi = {10.1109/ICIEA.2016.7603831},
  abstract = {An algorithm of sound source localization using range differences of arrival and energy ratios of arrival is proposed. This algorithm can localize the sound source using only two or three microphones. The problem is converted into a nonlinear least square optimization problem, which is solved by Levenberg-Marquardt method. The performance of this algorithm against noise is analyzed in this paper.},
  eventtitle = {2016 {{IEEE}} 11th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QEDPQ8IH\\Dou et al. - 2016 - An algorithm of sound source localization using ra.pdf;C\:\\Users\\sauli\\Zotero\\storage\\HU9EV7ZE\\7603831.html},
  keywords = {acoustic signal processing,Acoustics,Conferences,direction-of-arrival estimation,energy ratio of arrival estimation,energy ratios,least squares approximations,Levenberg-Marquardt,Levenberg-Marquardt method,Mathematical model,Microphones,nonlinear least square,nonlinear least square optimization problem,nonlinear programming,Optimization,range difference of arrival estimation,range differences,Signal processing algorithms,sound source localization algorithm,Source localization,Standards}
}

@article{duanAnalysisArrayGain2018,
  title = {Analysis of Array Gain Degradation of Near-Field Passive Synthetic Aperture Method},
  author = {Duan, Rui and Yang, Kunde and Lei, Zhixiong},
  date = {2018-12-01},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {141},
  pages = {\{151-161\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2018.07.005\}},
  abstract = {A previous study established a method for localizing low-frequency acoustic sources in the near field using a passive synthetic aperture method [Lei et al., 2015]. Sound fields sampled at different positions (sub-apertures) and times are synthesized to improve the spatial resolution of conventional beamforming. However, in practice, the position errors of the sub-apertures could significantly degrade the performance of the method. In this work, the analysis is extended to measure the sensitivity of the method to the position errors of the sub-apertures using array gain degradation (AGD). First, the position errors in three directions are assumed to be independent and obey the same Gaussian distribution to present a simple expression for AGD. Then, the analysis is generalized to the case of different Gaussian distributions to obtain an expression with explicit physical meaning. The two expressions are verified by simulations and partially validated using the experimental data sampled by a planar array in a semi-anechoic chamber. Finally, the results of localizing the sources on a transformer in a 750 kV substation are revealed.},
  affiliation = {Duan, R (Reprint Author), Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710072, Shaanxi, Peoples R China. Duan, Rui; Yang, Kunde; Lei, Zhixiong, Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710072, Shaanxi, Peoples R China. Duan, Rui; Yang, Kunde; Lei, Zhixiong, Northwestern Polytech Univ, Minist Ind & Informat Technol, Key Lab Ocean Acoust & Sensing, Xian 710072, Shaanxi, Peoples R China. Duan, Rui, Chinese Acad Sci, Inst Acoust, State Key Lab Acoust, Beijing 100190, Peoples R China.},
  author-email = {duanrui@nwpu.edu.cn ykdzym@nwpu.edu.cn},
  cited-references = {Ballesteros JA, 2015, APPL ACOUST, V93, P106, DOI 10.1016/j.apacoust.2015.01.019. Chu N, 2014, APPL ACOUST, V76, P197, DOI 10.1016/j.apacoust.2013.08.007. COX H, 1973, J ACOUST SOC AM, V54, P771, DOI 10.1121/1.1913659. FRIEDLANDER B, 1992, IEEE T AERO ELEC SYS, V28, P574, DOI 10.1109/7.144583. Guo F, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16071145. He H, 2009, ICASSP. Lee J, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040549. Legg M, 2013, IEEE T IMAGE PROCESS, V22, P4028, DOI 10.1109/TIP.2013.2268974. Lei ZX, 2015, J ACOUST SOC AM, V137, pEL255, DOI 10.1121/1.4915003. Liang JL, 2009, IEEE SENS J, V9, P953, DOI 10.1109/JSEN.2009.2025580. Martellotta F, 2013, APPL ACOUST, V74, P987, DOI 10.1016/j.apacoust.2013.02.004. Pesavento M, 2002, IEEE SIGNAL PROC LET, V9, P8, DOI 10.1109/97.988716. STERGIOPOULOS S, 1992, IEEE J OCEANIC ENG, V17, P16, DOI 10.1109/48.126950. STERGIOPOULOS S, 1989, J ACOUST SOC AM, V86, P158, DOI 10.1121/1.398335. Sullivan EJ, 2006, J ACOUST SOC AM, V120, pEL49, DOI 10.1121/1.2266024. Sullivan Edmund J., 2003, IEEE J OCEANIC ENG S, V38, P21. Wang B, 2010, CMCE IEEE. WILLIAMS R, 1992, IEEE J OCEANIC ENG, V17, P8, DOI 10.1109/48.126949. Zhi W, 2007, ICASSP.},
  da = {2018-10-18},
  doc-delivery-number = {GU9YN},
  eissn = {1872-910X},
  funding-acknowledgement = {National Natural Science Foundation of China [11704313, 61701405]; State Key Laboratory of Acoustics, Chinese Academy of Sciences [SKLA201713]; Fundamental Research Funds for the Central Universities [G2017KY0102]},
  funding-text = {This work was supported by the National Natural Science Foundation of China (Grant No. 11704313 and 61701405); the State Key Laboratory of Acoustics, Chinese Academy of Sciences (Grant No. SKLA201713); and the Fundamental Research Funds for the Central Universities (Grant No. G2017KY0102).},
  journal-iso = {Appl. Acoust.},
  keywords = {(Acoustic source localization,Array gain  degradation),Passive synthetic aperture},
  keywords-plus = {LOCALIZATION; MICROPHONE},
  langid = {english},
  number-of-cited-references = {19},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000445713300017},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Acoustics}
}

@book{dumciusGarsoInzinerija2012,
  title = {Garso Inžinerija},
  author = {Dumčius, Antanas},
  date = {2012-09-05},
  edition = {1},
  publisher = {{KTU leidykla "Technologija"}},
  doi = {10.5755/e01.9786090206140},
  url = {http://www.ebooks.ktu.lt/eb/658/garso_inzinerija/},
  urldate = {2018-02-05},
  isbn = {978-609-02-0614-0}
}

@inproceedings{duraiswamiActiveSpeechSource2001,
  title = {Active Speech Source Localization by a Dual Coarse-to-Fine Search},
  booktitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  author = {Duraiswami, R. and Zotkin, D. and Davis, L.S.},
  date = {2001-05},
  volume = {5},
  pages = {3309-3312 vol.5},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940366},
  abstract = {Accurate and fast localization of multiple speech sound sources is a significant problem in videoconferencing systems. Based on the observation that the wavelengths of the sound from a speech source are comparable to the dimensions of the space being searched, and that the source is broadband, we develop an efficient search strategy that finds the source(s) in a given space. The search is made efficient by using coarse-to-fine strategies in both space and frequency. The algorithm is shown to be robust compared to typical delay-based estimators and fast enough for real-time implementation. Its performance can be further improved by using constraints from computer vision.},
  eventtitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XLXSGKVN\\Duraiswami et al. - 2001 - Active speech source localization by a dual coarse.pdf;C\:\\Users\\sauli\\Zotero\\storage\\GQKULVZB\\940366.html},
  keywords = {active speech source localization,array signal processing,Array signal processing,Computer interfaces,Delay effects,Delay estimation,delay-based estimators,dual coarse-to-fine search,frequency,Inverse problems,Laboratories,multiple speech sound sources,Position measurement,real-time implementation,Sensor arrays,Signal processing algorithms,space,Speech,speech processing,teleconferencing,videoconferencing systems}
}

@phdthesis{durkovic_localization_2012,
  title = {Localization, {{Tracking}}, and {{Separation}} of {{Sound Sources}} for {{Cognitive Robots}}},
  author = {Đurković, Marko and others},
  date = {2012},
  institution = {{Universitätsbibliothek der TU München}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VGH46QZ9\\Localization, Tracking, and Separation of Sound Sources for Cognitive Robots - dissertation.pdf}
}

@article{durkovicLowLatencyLocalization2011,
  title = {Low Latency Localization of Multiple Sound Sources in Reverberant Environments},
  author = {Ðurković, Marko and Habigt, Tim and Rothbucher, Martin and Diepold, Klaus},
  date = {2011-11-10},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {130},
  pages = {EL392-EL398},
  issn = {0001-4966},
  doi = {10.1121/1.3659146},
  url = {http://asa.scitation.org/doi/10.1121/1.3659146},
  urldate = {2017-05-11},
  abstract = {Sound source localization algorithms determine the physical position of a sound source in respect to a listener. For practical applications, a localization algorithm design has to take into account real world conditions like multiple active sources, reverberation, and noise. The application can impose additional constraints on the algorithm, e.g., a requirement for low latency. This work defines the most important constraints for practical applications, introduces an algorithm, which tries to fulfill all requirements as good as possible, and compares it to state-of-the-art sound source localization approaches.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VD56U7GT\\Ðurković et al. - 2011 - Low latency localization of multiple sound sources.pdf;C\:\\Users\\sauli\\Zotero\\storage\\HXARFJD2\\1.html},
  number = {6}
}

@article{DurkovicLowlatencylocalization2011,
  title = {Low {{Latency Localization}} of {{Multiple Sound Sources}} in {{Reverberant Environments}}},
  author = {Durković, Marko and Habigt, Tim and Rothbucher, Martin and Diepold, Klaus},
  date = {2011-12},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {130},
  pages = {EL392-398},
  issn = {1520-8524},
  doi = {10.1121/1.3659146},
  abstract = {Sound source localization algorithms determine the physical position of a sound source in respect to a listener. For practical applications, a localization algorithm design has to take into account real world conditions like multiple active sources, reverberation, and noise. The application can impose additional constraints on the algorithm, e.g., a requirement for low latency. This work defines the most important constraints for practical applications, introduces an algorithm, which tries to fulfill all requirements as good as possible, and compares it to state-of-the-art sound source localization approaches.},
  eprint = {22225132},
  eprinttype = {pmid},
  keywords = {Algorithms,Auditory Perception,Humans,Mathematics,Sensory Gating,Sound localization},
  langid = {english},
  number = {6}
}

@article{durkovicLowLatencyLocalization2011a,
  title = {Low Latency Localization of Multiple Sound Sources in Reverberant Environments},
  author = {Durković, Marko and Habigt, Tim and Rothbucher, Martin and Diepold, Klaus},
  date = {2011-12},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {130},
  pages = {EL392-398},
  issn = {1520-8524},
  doi = {10.1121/1.3659146},
  abstract = {Sound source localization algorithms determine the physical position of a sound source in respect to a listener. For practical applications, a localization algorithm design has to take into account real world conditions like multiple active sources, reverberation, and noise. The application can impose additional constraints on the algorithm, e.g., a requirement for low latency. This work defines the most important constraints for practical applications, introduces an algorithm, which tries to fulfill all requirements as good as possible, and compares it to state-of-the-art sound source localization approaches.},
  eprint = {22225132},
  eprinttype = {pmid},
  keywords = {Algorithms,Auditory Perception,Humans,Mathematics,Sensory Gating,Sound localization},
  langid = {english},
  number = {6}
}

@article{DurkovicLowlatencylocalization2011a,
  title = {Low {{Latency Localization}} of {{Multiple Sound Sources}} in {{Reverberant Environments}}},
  author = {Ðurković, Marko and Habigt, Tim and Rothbucher, Martin and Diepold, Klaus},
  date = {2011-11},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {130},
  pages = {EL392-EL398},
  issn = {0001-4966},
  doi = {10.1121/1.3659146},
  abstract = {Sound source localization algorithms determine the physical position of a sound source in respect to a listener. For practical applications, a localization algorithm design has to take into account real world conditions like multiple active sources, reverberation, and noise. The application can impose additional constraints on the algorithm, e.g., a requirement for low latency. This work defines the most important constraints for practical applications, introduces an algorithm, which tries to fulfill all requirements as good as possible, and compares it to state-of-the-art sound source localization approaches.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WVNZZA4G\\Ðurković et al. - 2011 - Low latency localization of multiple sound sources.pdf;C\:\\Users\\sauli\\Zotero\\storage\\E72SYC2W\\1.html},
  number = {6}
}

@inproceedings{ea-eejanSoundSourceLocalization1996,
  title = {Sound Source Localization in Reverberant Environments Using an Outlier Elimination Algorithm},
  booktitle = {Proceeding of {{Fourth International Conference}} on {{Spoken Language Processing}}. {{ICSLP}} '96},
  author = {{Ea-Ee Jan} and Flanagan, J.},
  date = {1996},
  volume = {3},
  pages = {1321--1324},
  publisher = {{IEEE}},
  location = {{Philadelphia, PA, USA}},
  doi = {10.1109/ICSLP.1996.607856},
  url = {http://ieeexplore.ieee.org/document/607856/},
  urldate = {2019-04-03},
  eventtitle = {Proceeding of {{Fourth International Conference}} on {{Spoken Language Processing}}. {{ICSLP}} '96},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U8MG58L3\\Ea-Ee Jan and Flanagan - 1996 - Sound source localization in reverberant environme.pdf},
  isbn = {978-0-7803-3555-4},
  langid = {english}
}

@article{elbadawyDirectionArrivalOne2018,
  title = {Direction of {{Arrival With One Microphone}}, a {{Few LEGOs}}, and {{Non}}-{{Negative Matrix Factorization}}},
  author = {El Badawy, Dalia and Dokmanic, Ivan},
  date = {2018-12},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{2436-2446\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2867081\}},
  abstract = {Conventional approaches to sound source localization require at least two microphones. It is known, however, that people with unilateral hearing loss can also localize sounds. Monaural localization is possible thanks to the scattering by the head, though it hinges on learning the spectra of the various sources. We take inspiration from this human ability to propose algorithms for accurate sound source localization using a single microphone embedded in an arbitrary scattering structure. The structure modifies the frequency response of the microphone in a direction-dependent way giving each direction a signature. While knowing those signatures is sufficient to localize sources of white noise, localizing speech is much more challenging: it is an ill-posed inverse problem, which we regularize by prior knowledge in the form of learned non-negative dictionaries. We demonstrate a monaural speech localization algorithm based on non-negative matrix factorization that does not depend on sophisticated, designed scatterers. In fact, we show experimental results with ad hoc scatterers made of LEGO bricks. Even with these rudimentary structures we can accurately localize arbitrary speakers; that is, we do not need to learn the dictionary for the particular speaker to be localized. Finally, we discuss multi-source localization and the related limitations of our approach.},
  affiliation = {El Badawy, D (Reprint Author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland. El Badawy, Dalia, Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland. Dokmanic, Ivan, Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.},
  author-email = {dalia.elbadawy@epfl.ch dokmanic@illinois.edu},
  cited-references = {Algazi VR, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P99, DOI 10.1109/ASPAA.2001.969552. Aytekin M, 2004, J ACOUST SOC AM, V116, P3594, DOI [10.1121/1.1811412, 10.1121/1.1811412]]. Badawy D.El, 2017, P 13 INT C LAT VAR A, P489. Blauert J., 1997, SPATIAL HEARING PSYC. Boufounos P. T., 2011, P SOC PHOTO-OPT INS, V8138. Cagli E., 2013, P IEEE WORKSH APPL S, P1. Cichocki A, 2006, INT CONF ACOUST SPEE, P621. Colton D, 2000, SIAM REV, V42, P369, DOI 10.1137/S0036144500367337. Colton D., 2013, APPL MATH SCI. Dikmen Onur, 2009, 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), P93, DOI 10.1109/ASPAA.2009.5346508. Dokmanic I., 2015, THESIS. Dokmanic I, 2012, INT CONF ACOUST SPEE, P2617, DOI 10.1109/ICASSP.2012.6288453. Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132. Fevotte C, 2011, NEURAL COMPUT, V23, P2421, DOI 10.1162/NECO_a_00168. Fevotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771. Friedman J., 2010, ARXIV10010736. Garofolo J., 1993, 4930 NAT I STAND TEC. Harris JG, 2000, ANALOG INTEGR CIRC S, V23, P163, DOI 10.1023/A:1008350127376. HEBRANK J, 1974, J ACOUST SOC AM, V56, P935, DOI 10.1121/1.1903351. Kitamura D., 2016, 2016 IEEE INT WORKSH, P1. Kowalski M, 2010, IEEE T AUDIO SPEECH, V18, P1818, DOI 10.1109/TASL.2010.2050089. Langville A. N., 2014, CORR, V1407, P7299. Le Roux J., 2015, TR2015023 MITS EL RE. Ledoux M., 2001, MATH SURVEYS MONOGRA. Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565. Lefevre A, 2011, INT CONF ACOUST SPEE, P21. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. Mohammadiha N, 2013, IEEE T AUDIO SPEECH, V21, P2140, DOI 10.1109/TASL.2013.2270369. MUSICANT AD, 1984, J ACOUST SOC AM, V75, P1195, DOI 10.1121/1.390770. OLDFIELD SR, 1986, PERCEPTION, V15, P67, DOI 10.1068/p150067. Parra L, 2000, IEEE T SPEECH AUDI P, V8, P320, DOI 10.1109/89.841214. Reeder RM, 2015, AUDIOL NEURO-OTOL, V20, P31, DOI 10.1159/000380745. RICE JJ, 1992, HEARING RES, V58, P132, DOI 10.1016/0378-5955(92)90123-5. Saxena Ashutosh, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P1737, DOI 10.1109/ROBOT.2009.5152861. Schmidt MN, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2614. Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860. Smaragdis P, 2007, IEEE T AUDIO SPEECH, V15, P1, DOI 10.1109/TASL.2006.876726. Sun DL, 2013, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2013.6637625. Traa Johannes, 2015, 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Proceedings, P1, DOI 10.1109/WASPAA.2015.7336944. Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253. Wierstorf H., 2011, AUDIO ENG SOC CONVEN, V130. Woodruff J, 2012, IEEE T AUDIO SPEECH, V20, P1503, DOI 10.1109/TASL.2012.2183869. Xie YB, 2015, P NATL ACAD SCI USA, V112, P10595, DOI 10.1073/pnas.1502276112.},
  da = {2018-10-18},
  doc-delivery-number = {GT8YX},
  funding-acknowledgement = {Swiss National Science Foundation [20FP-1 151073]},
  funding-text = {This work was supported by the Swiss National Science Foundation under Grant 20FP-1 151073, Inverse Problems regularized by Sparsity. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Augusto Sarti.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Direction-of-arrival estimation,group sparsity,monaural localization,non-negative matrix factorization,one microphone,sound scattering,universal speech  model)},
  keywords-plus = {SOUND LOCALIZATION; SOURCE SEPARATION; SPECTRAL CUES; DIVERGENCE; ALGORITHMS},
  langid = {english},
  number = {12},
  number-of-cited-references = {43},
  oa = {green_published},
  orcid-numbers = {Dokmanic, Ivan/0000-0001-7132-5214},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000444827100002},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@inproceedings{elkoSteerableVariableFirstorder1997,
  title = {A Steerable and Variable First-Order Differential Microphone Array},
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Elko, G. W. and {Anh-Tho Nguyen Pong}},
  date = {1997-04},
  volume = {1},
  pages = {223-226 vol.1},
  doi = {10.1109/ICASSP.1997.599609},
  abstract = {A new first-order differential microphone array with an infinitely steerable and variable beampattern is described. The microphone consists of 6 small pressure microphones flush-mounted on the surface of a 3/4" diameter rigid nylon sphere. The microphones are located on the surface at points where included octahedron vertices contact the spherical surface. By appropriately combining the three Cartesian orthogonal pairs with simple scalar weightings, a general first-order differential microphone beam (or beams) can be realized and directed to any angle in 4/spl pi/ steradian space. A working real-time version has been created and measured results from this microphone are shown. This microphone should be useful for surround sound recording/playback applications and to virtual reality audio applications.},
  eventtitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7YNZJLG9\\Elko and Anh-Tho Nguyen Pong - 1997 - A steerable and variable first-order differential .pdf;C\:\\Users\\sauli\\Zotero\\storage\\AQE9CVR3\\599609.html},
  keywords = {Acoustic beams,Acoustic noise,acoustic signal processing,acoustic transducer arrays,Acoustic waves,array signal processing,audio recording,Broadcasting,Cardiology,Cartesian orthogonal pairs,direction-of-arrival estimation,DSP implementation,first-order differential microphone array,first-order differential microphone beam,measured results,Microphone arrays,microphones,octahedron vertices,pressure microphones,Reverberation,rigid nylon sphere,scalar weightings,Sensor arrays,spherical surface,steerable beampattern,surround sound recording/playback,Switches,variable beampattern,virtual reality,Virtual reality,virtual reality audio applications}
}

@online{EndtoEndAcousticLocalization,
  title = {Towards {{End}}-to-{{End Acoustic Localization Using Deep Learning}}: {{From Audio Signals}} to {{Source Position Coordinates}}},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6210564/},
  urldate = {2019-05-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XHGJWAA4\\PMC6210564.html}
}

@online{EngineeringAmbientIntelligence,
  title = {Engineering Ambient Intelligence Systems Using Agent Technology - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/abstract/document/7006373},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FNSEUQV7\\7006373.html}
}

@inproceedings{escobarAccuracyStudyRealTime2013,
  title = {Accuracy {{Study}} of a {{Real}}-{{Time Hybrid Sound Source Localization Algorithm}}},
  booktitle = {Intelligent {{Technologies}} for {{Interactive Entertainment}}},
  author = {Escobar, Fernando A. and Chang, Xin and Ibala, Christian and Valderrama, Carlos},
  editor = {Mancas, Matei and d’ Alessandro, Nicolas and Siebert, Xavier and Gosselin, Bernard and Valderrama, Carlos and Dutoit, Thierry},
  date = {2013-07-03},
  pages = {146--155},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-03892-6_17},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-03892-6_17},
  urldate = {2017-01-11},
  abstract = {Sound source localization in real time can be employed in numerous applications such as filtering, beamforming, security system integration, etc. Algorithms employed in this field require not only fast processing speed but also enough accuracy to properly cope with the application requirements. This work presents accuracy benchmarks of a hybrid approach previously proposed, which is based on the Generalized Cross Correlation (GCC), and the Delay and Sum beamforming (DSB). Tests were performed considering a linear microphone array simulated in MATLAB. Analysis through variations in array size, number of microphones, spacing and other characteristics, were included. Results obtained show that the proposed algorithm is as good as the DSB under some conditions that can be easily met.},
  eventtitle = {International {{Conference}} on {{Intelligent Technologies}} for {{Interactive Entertainment}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AHIKWUWR\\978-3-319-03892-6_17.html},
  isbn = {978-3-319-03891-9 978-3-319-03892-6},
  keywords = {Accuracy,Beamforming,computational complexity,Computer Applications,Computer Imaging; Vision; Pattern Recognition and Graphics,generalized cross correlation,Media Design,Real Time,Sound localization,User Interfaces and Human Computer Interaction},
  langid = {english},
  series = {Lecture {{Notes}} of the {{Institute}} for {{Computer Sciences}}, {{Social Informatics}} and {{Telecommunications Engineering}}}
}

@inproceedings{escolano_bayesian_2012,
  title = {A {{Bayesian Framework}} for {{Sound Source Localization}}},
  booktitle = {Audio {{Engineering Society Convention}} 132},
  author = {Escolano, José and Cobos, Máximo and Pérez-Lorenzo, Jose M. and López, José J. and Xiang, Ning},
  date = {2012-04},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=16306},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GCD39T5D\\A Bayesian framework for sound source localization.pdf},
  keywords = {Purpose}
}

@inproceedings{escolanoBayesianFrameworkSound2012,
  title = {A {{Bayesian Framework}} for {{Sound Source Localization}}},
  booktitle = {Audio {{Engineering Society Convention}} 132},
  author = {Escolano, José and Cobos, Máximo and Pérez-Lorenzo, Jose M. and López, José J. and Xiang, Ning},
  date = {2012-04},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=16306},
  keywords = {Purpose}
}

@online{EtherSoundProgrammerManual,
  title = {{{EtherSound Programmer}}'s {{Manual}} - {{Digigram}}\_{{EtherSound}}\_{{Overview}}.Pdf},
  url = {http://www.point-sourceaudio.com/network_audio/Digigram_EtherSound_Overview.pdf},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EBWJJIAE\\Digigram_EtherSound_Overview.html}
}

@article{eversAcousticSLAM2018,
  title = {Acoustic {{SLAM}}},
  author = {Evers, C. and Naylor, P. A.},
  date = {2018-09},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {1484--1498},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2828321},
  abstract = {An algorithm is presented that enables devices equipped with microphones, such as robots, to move within their environment in order to explore, adapt to, and interact with sound sources of interest. Acoustic scene mapping creates a three-dimensional (3D) representation of the positional information of sound sources across time and space. In practice, positional source information is only provided by Direction-of-Arrival (DoA) estimates of the source directions; the source-sensor range is typically difficult to obtain. DoA estimates are also adversely affected by reverberation, noise, and interference, leading to errors in source location estimation and consequent false DoA estimates. Moreover, many acoustic sources, such as human talkers, are not continuously active, such that periods of inactivity lead to missing DoA estimates. Withal, the DoA estimates are specified relative to the observer's sensor location and orientation. Accurate positional information about the observer therefore is crucial. This paper proposes Acoustic Simultaneous Localization and Mapping (aSLAM), which uses acoustic signals to simultaneously map the 3D positions of multiple sound sources while passively localizing the observer within the scene map. The performance of aSLAM is analyzed and evaluated using a series of realistic simulations. Results are presented to show the impact of the observer motion and sound source localization accuracy.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HWTEL6NK\\Evers and Naylor - 2018 - Acoustic SLAM.pdf;C\:\\Users\\sauli\\Zotero\\storage\\DIGT3B42\\8340823.html},
  keywords = {accurate positional information,acoustic generators,Acoustic scene mapping,acoustic signal processing,acoustic signals,Acoustic Simultaneous Localization,Acoustic SLAM,acoustic sources,array signal processing,Bayes methods,consequent false DoA estimates,Direction-of-Arrival,direction-of-arrival estimation,Direction-of-arrival estimation,inactivity lead,microphones,missing DoA estimates,multiple sound sources,observer motion,Observers,positional source information,Probability density function,reverberation,Reverberation,robot audition,scene map,simultaneous localization and mapping,Simultaneous localization and mapping,SLAM (robots),sound source localization accuracy,source directions,source location estimation,source-sensor range,Speech,three-dimensional representation},
  number = {9}
}

@article{eversAcousticSLAM2018a,
  title = {Acoustic {{SLAM}}},
  author = {Evers, Christine and Naylor, Patrick A.},
  date = {2018-09},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1484-1498\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2828321\}},
  abstract = {An algorithm is presented that enables devices equipped with microphones, such as robots, to move within their environment in order to explore, adapt to, and interact with sound sources of interest. Acoustic scene mapping creates a three-dimensional (3D) representation of the positional information of sound sources across time and space. In practice, positional source information is only provided by Direction-of-Arrival (DoA) estimates of the source directions; the source-sensor range is typically difficult to obtain. DoA estimates are also adversely affected by reverberation, noise, and interference, leading to errors in source location estimation and consequent false DoA estimates. Moreover, many acoustic sources, such as human talkers, are not continuously active, such that periods of inactivity lead tomissing DoA estimates. Withal, the DoA estimates are specified relative to the observer's sensor location and orientation. Accurate positional information about the observer therefore is crucial. This paper proposes Acoustic Simultaneous Localization and Mapping (aSLAM), which uses acoustic signals to simultaneously map the 3D positions of multiple sound sources while passively localizing the observer within the scene map. The performance of aSLAM is analyzed and evaluated using a series of realistic simulations. Results are presented to show the impact of the observer motion and sound source localization accuracy.},
  affiliation = {Evers, C (Reprint Author), Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England. Evers, Christine; Naylor, Patrick A., Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England.},
  author-email = {c.evers@imperial.ac.uk p.naylor@imperial.ac.uk},
  cited-references = {Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374. Banerjee A, 2005, J MACH LEARN RES, V6, P1345. Benesty J, 2008, SPRINGER TOP SIGN PR, V1, P1. Botev ZI, 2010, ANN STAT, V38, P2916, DOI 10.1214/10-AOS799. Burnham K. P., 2002, MODEL SELECTION MULT. Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754. Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81. Daley D. J., 2003, INTRO THEORY POINT P, VI. Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038. Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022. Evers C, 2018, IEEE T SIGNAL PROCES, V66, P863, DOI 10.1109/TSP.2017.2775590. Evers C, 2017, INT CONF ACOUST SPEE, P6145, DOI 10.1109/ICASSP.2017.7953337. Evers C, 2016, EUR SIGNAL PR CONF, P1008, DOI 10.1109/EUSIPCO.2016.7760400. Evers C, 2016, INT CONF ACOUST SPEE, P6, DOI 10.1109/ICASSP.2016.7471626. Fischer C, 2010, IEEE PERVAS COMPUT, V9, P38, DOI 10.1109/MPRV.2009.91. Gannot S., 2008, SPRINGER HDB SPEECH. Garofolo J., 1993, TIMIT ACOUSTIC PHONE. Gebru ID, 2016, IEEE T PATTERN ANAL, V38, P2402, DOI 10.1109/TPAMI.2016.2522425. George L, 2013, IEEE-RAS INT C HUMAN, P329, DOI 10.1109/HUMANOIDS.2013.7029995. GEWEKE J, 1989, ECONOMETRICA, V57, P1317, DOI 10.2307/1913710. Greenwood P.E., 1996, WILEY SERIES PROBABI. Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925. Habets E. A. P., 2006, ROOM IMPULSE RESPONS. Hu JM, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1564. Jarrett DP, 2012, J ACOUST SOC AM, V132, P1462, DOI 10.1121/1.4740497. Jarrett D. P., 2016, THEORY APPL SPHERICA. Jazar R., 2010, THEORY APPL ROBOTICS. Kelly IJ, 2014, IEEE-ACM T AUDIO SPE, V22, P1139, DOI 10.1109/TASLP.2014.2321472. KREKOVIC M, 2016, P IEEE INT C AC SPEE, P11. Lehmann EA, 2010, IEEE T AUDIO SPEECH, V18, P1429, DOI 10.1109/TASL.2009.2035038. Li XR, 2003, IEEE T AERO ELEC SYS, V39, P1333, DOI 10.1109/TAES.2003.1261132. Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823. Mahler RPS, 2004, IEEE AERO EL SYS MAG, V19, P53, DOI 10.1109/MAES.2004.1263231. Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119. Mardia K., 1999, DIRECTIONAL STAT. Montemerlo M., 2007, SPRINGER TRACTS ADV, V27. Moore AH, 2017, IEEE-ACM T AUDIO SPE, V25, P178, DOI 10.1109/TASLP.2016.2613280. Nadiri O, 2014, IEEE-ACM T AUDIO SPE, V22, P1494, DOI 10.1109/TASLP.2014.2337846. Nakadai K, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P832. Naylor PA, 2010, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-84996-056-4. Nuchter A, 2009, SPRINGER TRAC ADV RO, V52, P1. Ogiso S., 2015, ROBOMECH J, V2, P1. Rafaely B, 2017, INT CONF ACOUST SPEE, P6120, DOI 10.1109/ICASSP.2017.7953332. RANDALL P, 1993, P SOC PHOTO-OPT INS, V1950, P96, DOI 10.1117/12.156615. Ristic B, 2013, PARTICLE FILTERS RAN. Ristic B, 2011, IEEE T SIGNAL PROCES, V59, P3452, DOI 10.1109/TSP.2011.2140111. Salmond DJ, 2009, IEEE T AERO ELEC SYS, V45, P667, DOI 10.1109/TAES.2009.5089549. Schon T, 2005, IEEE T SIGNAL PROCES, V53, P2279, DOI 10.1109/TSP.2005.849151. Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469. Tourbabin V, 2014, IEEE-ACM T AUDIO SPE, V22, P1803, DOI 10.1109/TASLP.2014.2351133. Traa J, 2013, IEEE SIGNAL PROC LET, V20, P1257, DOI 10.1109/LSP.2013.2287125. Valin JM, 2007, ROBOT AUTON SYST, V55, P216, DOI 10.1016/j.robot.2006.08.004. Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190. Wallace C. S., 2005, INFORM SCI STAT SERI. Wang T, 2016, INT CONF ACOUST SPEE, P21, DOI 10.1109/ICASSP.2016.7471629.},
  da = {2018-10-18},
  doc-delivery-number = {GH4KD},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PME68Y69\\8340823.html},
  funding-acknowledgement = {U.K. EPSRC Fellowship [EP/P001017/1]; European Union's Seventh Framework Programme (FP7) [609465]},
  funding-text = {This work was supported by the U.K. EPSRC Fellowship under Grant EP/P001017/1 and in part by the European Union's Seventh Framework Programme (FP7/2007-2013) under Grant 609465.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Bayes methods,reverberation,robot audition,simultaneous localization  and mapping)},
  keywords-plus = {ROOM IMPULSE RESPONSES; SIMULTANEOUS LOCALIZATION; PERFORMANCE EVALUATION; MICROPHONE ARRAY; PARTICLE FILTERS; TRACKING; ALGORITHMS; MODELS; SIMULATION; TUTORIAL},
  langid = {english},
  number = {9},
  number-of-cited-references = {55},
  oa = {gold_or_bronze},
  orcid-numbers = {Naylor, Patrick/0000-0001-8546-8013},
  research-areas = {Acoustics; Engineering},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000433371500002},
  usage-count-last-180-days = {33},
  usage-count-since-2013 = {33},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{eversAcousticSLAM2018b,
  title = {Acoustic {{SLAM}}},
  author = {Evers, Christine and Naylor, Patrick A.},
  date = {2018-09},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {26},
  pages = {1484--1498},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2018.2828321},
  url = {https://ieeexplore.ieee.org/document/8340823/},
  urldate = {2019-08-16},
  abstract = {An algorithm is presented that enables devices equipped with microphones, such as robots, to move within their environment in order to explore, adapt to and interact with sound sources of interest. Acoustic scene mapping creates a 3D representation of the positional information of sound sources across time and space. In practice, positional source information is only provided by Direction-of-Arrival (DoA) estimates of the source directions; the source-sensor range is typically difficult to obtain. DoA estimates are also adversely affected by reverberation, noise, and interference, leading to errors in source location estimation and consequent false DoA estimates. Moroever, many acoustic sources, such as human talkers, are not continuously active, such that periods of inactivity lead to missing DoA estimates. Withal, the DoA estimates are specified relative to the observer’s sensor location and orientation. Accurate positional information about the observer therefore is crucial. This paper proposes Acoustic Simultaneous Localization and Mapping (aSLAM), which uses acoustic signals to simultaneously map the 3D positions of multiple sound sources whilst passively localizing the observer within the scene map. The performance of aSLAM is analyzed and evaluated using a series of realistic simulations. Results are presented to show the impact of the observer motion and sound source localization accuracy.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MNXBDR9W\\Evers and Naylor - 2018 - Acoustic SLAM.pdf},
  keywords = {*****,MUST READ},
  langid = {english},
  number = {9}
}

@inproceedings{eversLocataChallengeEvaluationTasks2018,
  title = {Locata {{Challenge}}-{{Evaluation Tasks}} and {{Measures}}},
  booktitle = {2018 16th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Evers, C. and Löllmann, H. W. and Mellmann, H. and Schmidt, A. and Barfuss, H. and Naylor, P. A. and Kellermann, W.},
  date = {2018-09},
  pages = {565--569},
  doi = {10.1109/IWAENC.2018.8521288},
  abstract = {Sound source localization and tracking algorithms provide estimates of the positional information about active sound sources in acoustic environments. Despite substantial advances and significant interest in the research community, a comprehensive benchmarking campaign of the various approaches using a common database of audio recordings has, to date, not been performed. The aim of the IEEE-AASP Challenge on sound source localization and tracking (LOCATA) is to objectively benchmark state-of-the-art localization and tracking algorithms using an open-access data corpus of recordings for scenarios typically encountered in audio and acoustic signal processing applications. The challenge tasks range from the localization of a single source with a static microphone array to tracking of multiple moving sources with a moving microphone array. This paper provides an overview of the challenge tasks, describes the performance measures used for evaluation of the LOCATA Challenge, and presents baseline results for the development dataset.},
  eventtitle = {2018 16th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\C82PQV33\\8521288.html},
  keywords = {Acoustics,Array signal processing,Arrays,benchmark,Databases,LOCATA challenge,Microphone arrays,source localization,source tracking,Task analysis}
}

@article{eversLOCATACHALLENGEOVERVIEW,
  title = {{{LOCATA CHALLENGE}} - {{OVERVIEW OF EVALUATION MEASURES}}},
  author = {Evers, Christine and Lollmann, Heinrich W and Mellmann, Heinrich and Schmidt, Alexander and Barfuss, Hendrik and Naylor, Patrick A and Kellermann, Walter},
  pages = {2},
  abstract = {This document provides an overview of the evaluation measures used for the LOCATA Challenge. The content of this document is an excerpt from a manuscript submitted to IWAENC 2018. Further information about the LOCATA Challenge and its data corpus can be found in [1].},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GG3GFWGC\\Evers et al. - LOCATA CHALLENGE - OVERVIEW OF EVALUATION MEASURES.pdf},
  langid = {english}
}

@inproceedings{eversMultipleSourceLocalisation2014,
  title = {Multiple Source Localisation in the Spherical Harmonic Domain},
  booktitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Evers, C. and Moore, A. H. and Naylor, P. A.},
  date = {2014-09},
  pages = {258--262},
  doi = {10.1109/IWAENC.2014.6954298},
  abstract = {Spherical arrays facilitate processing and analysis of sound fields with the potential for high resolution in three dimensions in the spherical harmonic domain. Using the captured sound field, robust source localisation systems are required for speech acquisition, speaker tracking and environment mapping. Source localisation becomes a challenging problem in reverberant environments and under noisy conditions, leading to potentially poor performance in cocktail party scenarios. This paper evaluates the performance of a low-complexity localisation approach using spherical harmonics in reverberant environments for multiple speakers. Eigen-beams are used to estimate pseudo-intensity vectors pointing in the direction of sound intensity. This paper proposes a clustering approach in which the intensity vectors of active sound sources and strong reflections are extracted, yielding an estimate of the source direction in azimuth and inclination as an approach to source localisation.},
  eventtitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6S7EPA3E\\Evers et al_2014_Multiple source localisation in the spherical harmonic domain.pdf;C\:\\Users\\sauli\\Zotero\\storage\\YQ4CGUDP\\Acoustic_Source_Localization_Using_Microphone_Arra (1).pdf;C\:\\Users\\sauli\\Zotero\\storage\\2A8XXHV4\\6954298.html},
  keywords = {acoustic signal processing,Acoustics,active sound sources,clustering,clustering approach,Direction-of-arrival estimation,eigen-beams,eigenvalues and eigenfunctions,Harmonic analysis,low-complexity localisation approach,Microphone arrays,multiple source localisation,multiple speaker lo-calisation,multiple speakers,pseudo-intensity vector estimation,pseudo-intensity vectors,reverberant environments,reverberation,sound intensity,Speech,speech processing,spherical harmonic domain,Spherical harmonics,Vectors}
}

@online{Experiments,
  title = {A.{{I}}. {{Experiments}}},
  url = {https://aiexperiments.withgoogle.com/},
  urldate = {2017-01-12},
  abstract = {AI Experiments is a showcase for simple experiments that let anyone play with artificial intelligence and machine learning in hands-on ways, through pictures, drawings, language, music, and more.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UWF3WRX5\\aiexperiments.withgoogle.com.html}
}

@online{FAQsAvnuAlliance,
  title = {{{FAQs}} | {{Avnu Alliance}}},
  url = {http://avnu.org/faqs/},
  urldate = {2017-06-13},
  abstract = {What is the Avnu Alliance and its Mission?},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Z4AZANAM\\faqs.html}
}

@article{fariaAutonomous3DExploration2019,
  title = {Autonomous {{3D Exploration}} of {{Large Structures Using}} an {{UAV Equipped}} with a {{2D LIDAR}}},
  author = {Faria, Margarida and Ferreira, António Sérgio and Pérez-Leon, Héctor and Maza, Ivan and Viguria, Antidio},
  date = {2019-11-08},
  journaltitle = {Sensors (Basel, Switzerland)},
  shortjournal = {Sensors (Basel)},
  volume = {19},
  issn = {1424-8220},
  doi = {10.3390/s19224849},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6891309/},
  urldate = {2020-01-17},
  abstract = {This paper addressed the challenge of exploring large, unknown, and unstructured industrial environments with an unmanned aerial vehicle (UAV). The resulting system combined well-known components and techniques with a new manoeuvre to use a low-cost 2D laser to measure a 3D structure. Our approach combined frontier-based exploration, the Lazy Theta* path planner, and a flyby sampling manoeuvre to create a 3D map of large scenarios. One of the novelties of our system is that all the algorithms relied on the multi-resolution of the octomap for the world representation. We used a Hardware-in-the-Loop (HitL) simulation environment to collect accurate measurements of the capability of the open-source system to run online and on-board the UAV in real-time. Our approach is compared to different reference heuristics under this simulation environment showing better performance in regards to the amount of explored space. With the proposed approach, the UAV is able to explore 93\% of the search space under 30 min, generating a path without repetition that adjusts to the occupied space covering indoor locations, irregular structures, and suspended obstacles.},
  eprint = {31717255},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NUIP2TFQ\\Faria et al. - 2019 - Autonomous 3D Exploration of Large Structures Usin.pdf},
  number = {22},
  pmcid = {PMC6891309}
}

@article{farmaniBiasCompensatedInformedSound2018,
  title = {Bias-{{Compensated Informed Sound Source Localization Using Relative Transfer Functions}}},
  author = {Farmani, M. and Pedersen, M. S. and Tan, Z. and Jensen, J.},
  date = {2018-07},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {1275--1289},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2825110},
  abstract = {In this paper, we consider the problem of estimating the target sound direction of arrival (DoA) for a hearing aid (HA) system, which can connect to a wireless microphone worn by the talker of interest. The wireless microphone “informs” the HA system about the noise-free target speech. To estimate the DoA, we consider a maximum-likelihood approach, and we assume that a database of DoA-dependent relative transfer functions (RTFs) has been measured in advance and is available. The proposed DoA estimator is able to take the available noise-free target speech, ambient noise characteristics, and the shadowing effect of the user's head on the received signals into account, and it supports both monaural and binaural microphone array configurations. Moreover, we analytically analyze the bias in the proposed estimator and introduce a modified estimator, which has been compensated for the bias. We demonstrate that the proposed method has lower computational complexity and better performance than recent RTF-based estimators. Furthermore, to decrease the number of parameters required to be wirelessly exchanged between the HAs in binaural configurations, we propose an information fusion strategy, which avoids transmitting microphone signals between the HAs. An important benefit of the proposed IF strategy is that the number of parameters to be exchanged between the HAs is independent of the number of HA microphones. Finally, we investigate the performance of variants of the proposed estimator extensively in different noisy and reverberant situations.},
  keywords = {acoustic signal processing,ambient noise characteristics,bias-compensated informed sound source localization,binaural configurations,binaural microphone array configurations,computational complexity,Databases,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,DoA estimator,DoA-dependent relative transfer functions,HA microphones,hearing aid,hearing aid system,hearing aids,Hearing aids,information fusion strategy,lower computational complexity,maximum likelihood,maximum likelihood estimation,Maximum likelihood estimation,maximum-likelihood approach,microphone arrays,microphone signals,Microphones,monaural microphone array configurations,noise-free target speech,received signals,relative transfer function,reverberation,sensor fusion,shadowing effect,Sound source localization,Speech,target sound direction,transfer functions,Wireless communication,wireless microphone informs the HA system},
  number = {7}
}

@article{farmaniBiasCompensatedInformedSound2018a,
  title = {Bias-{{Compensated Informed Sound Source Localization Using Relative Transfer Functions}}},
  author = {Farmani, Mojtaba and Pedersen, Michael Syskind and Tan, Zheng-Hua and Jensen, Jesper},
  date = {2018-07},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1271-1285\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2825110\}},
  abstract = {In this paper, we consider the problem of estimating the target sound direction of arrival (DoA) for a hearing aid (HA) system, which can connect to a wireless microphone worn by the talker of interest. The wireless microphone “informs” the HA system about the noise-free target speech. To estimate the DoA, we consider a maximum-likelihood approach, and we assume that a database of DoA-dependent relative transfer functions (RTFs) has been measured in advance and is available. The proposed DoA estimator is able to take the available noise-free target speech, ambient noise characteristics, and the shadowing effect of the user's head on the received signals into account, and it supports bothmonaural and binaural microphone array configurations. Moreover, we analytically analyze the bias in the proposed estimator and introduce a modified estimator, which has been compensated for the bias. We demonstrate that the proposed method has lower computational complexity and better performance than recent RTF-based estimators. Furthermore, to decrease the number of parameters required to be wirelessly exchanged between the HAs in binaural configurations, we propose an information fusion strategy, which avoids transmitting microphone signals between the HAs. An important benefit of the proposed IF strategy is that the number of parameters to be exchanged between the HAs is independent of the number of HA microphones. Finally, we investigate the performance of variants of the proposed estimator extensively in different noisy and reverberant situations.},
  affiliation = {Farmani, M (Reprint Author), Oticon AS, DK-2765 Smorum, Denmark. Farmani, Mojtaba; Pedersen, Michael Syskind; Jensen, Jesper, Oticon AS, DK-2765 Smorum, Denmark. Tan, Zheng-Hua; Jensen, Jesper, Aalborg Univ, Dept Elect Syst, Signal & Informat Proc Sect, DK-9220 Aalborg, Denmark.},
  author-email = {mofa@oticon.com micp@oticon.com zt@es.aau.dk jesj@oticon.com},
  cited-references = {Avargel Y., 2008, THESIS. Bayat A., 2013, IRANIAN RED CRESCENT, V15. Blauert J., 1997, SPATIAL HEARING PSYC. Box G. E. P., 2005, WILEY SERIES PROBABI. Braun S, 2015, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-015-0077-2. Bregman AS, 1994, AUDITORY SCENE ANAL. Brillinger D. R., 1981, TIME SERIES DATA ANA. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. Cheng C. I., 1999, P 107 AUD ENG SOC CO, P231. Courtois G., 2014, AUDIO ENG SOC CONVEN, V136, P9034. Duda RO, 1998, J ACOUST SOC AM, V104, P3048, DOI 10.1121/1.423886. DUHAMEL P, 1990, SIGNAL PROCESS, V19, P259, DOI 10.1016/0165-1684(90)90158-U. Fagerland MW, 2012, BMC MED RES METHODOL, V12, DOI 10.1186/1471-2288-12-78. Farmani M, 2017, IEEE-ACM T AUDIO SPE, V25, P611, DOI 10.1109/TASLP.2017.2651373. Farmani M, 2016, INT CONF ACOUST SPEE, P360, DOI 10.1109/ICASSP.2016.7471697. Farmani M, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P953, DOI 10.1109/GlobalSIP.2015.7418338. Farmani M, 2015, INT CONF ACOUST SPEE, P439, DOI 10.1109/ICASSP.2015.7178007. Gannot S, 2001, IEEE T SIGNAL PROCES, V49, P1614, DOI 10.1109/78.934132. Goetze S., 2007, INT S INT SIGN PROC, P84. Hjorungnes A, 2007, IEEE T SIGNAL PROCES, V55, P2740, DOI 10.1109/TSP.2007.893762. Huang YT, 2001, IEEE T SPEECH AUDI P, V9, P943, DOI 10.1109/89.966097. Jensen J., 2016, U. S. Patent, Patent No. [20 160 112 811, 20160112811]. Jeub M, 2010, INT CONF ACOUST SPEE, P4710, DOI 10.1109/ICASSP.2010.5495179. Kabal P., 2002, TECH REP. Kates J. M., 2016, INT HEAR AID RES C A. Kayser H, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/298605. Keyrouz F, 2014, IEEE T INSTRUM MEAS, V63, P2098, DOI 10.1109/TIM.2014.2308051. Kotus J, 2014, MULTIMED TOOLS APPL, V68, P5, DOI 10.1007/s11042-012-1183-0. Kuklasinski A, 2016, IEEE-ACM T AUDIO SPE, V24, P1599, DOI 10.1109/TASLP.2016.2573591. MacDonald JA, 2008, J ACOUST SOC AM, V123, P4290, DOI 10.1121/1.2909566. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. Martin R, 2001, IEEE T SPEECH AUDI P, V9, P504, DOI 10.1109/89.928915. Murray J. C., 2004, P NEUROBOTICS WORKSH, P89. Onyiah L., 2008, STAT SERIES TXB MONO. Papoulis A., 2002, MCGRAW HILL SERIES E. Stachurski J, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P93, DOI 10.1109/AVSS.2013.6636622. Valin JM, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1228. Wang H, 1997, INT CONF ACOUST SPEE, P187, DOI 10.1109/ICASSP.1997.599595. WILLIAMSON DF, 1989, ANN INTERN MED, V110, P916, DOI 10.7326/0003-4819-110-11-916. ZOHOURIAN M, 2016, P IEEE INT C AC SPEE, P430.},
  da = {2018-10-18},
  doc-delivery-number = {GE1TC},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Sound source localization,direction of arrival estimation,hearing aid,maximum likelihood,relative transfer function)},
  keywords-plus = {ENHANCEMENT; SPEECH; NOISE; MODEL},
  langid = {english},
  number = {7},
  number-of-cited-references = {40},
  orcid-numbers = {Farmani, Mojtaba/0000-0002-3872-5301},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000430998700009},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {8},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@incollection{fayIntroductionSoundSource2005,
  title = {Introduction to {{Sound Source Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Fay, Richard R. and Popper, Arthur N.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {1--5},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_1},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_1},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8CJUW3SV\\0-387-28863-5_1.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{FayIntroductionSoundSource2005,
  title = {Introduction to {{Sound Source Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Fay, Richard R. and Popper, Arthur N.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {1--5},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_1},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FC2L7Q4J\\0-387-28863-5_1.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{faySoundSourceLocalization2005,
  title = {Sound {{Source Localization}} by {{Fishes}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Fay, Richard R.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {36--66},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_3},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_3},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2DIK3B72\\0-387-28863-5_3.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{FaySoundSourceLocalization2005,
  title = {Sound {{Source Localization}} by {{Fishes}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Fay, Richard R.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {36--66},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_3},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GB55SEX7\\0-387-28863-5_3.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@article{FeedbackSystemsFeedback2013,
  title = {Feedback {{Systems}} and {{Feedback Control Systems}}},
  date = {2013-08},
  journaltitle = {Basic Electronics Tutorials},
  abstract = {Electronics Tutorial about the various Feedback Systems and Feedback Control Systems used in Feedback Amplifier and Process Control Systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QFLQ8GC8\\feedback-systems.html}
}

@article{FeedbackSystemsFeedback2013a,
  title = {Feedback {{Systems}} and {{Feedback Control Systems}}},
  date = {2013-08},
  journaltitle = {Basic Electronics Tutorials},
  abstract = {Electronics Tutorial about the various Feedback Systems and Feedback Control Systems used in Feedback Amplifier and Process Control Systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MPE2UE3J\\feedback-systems.html}
}

@online{FeedbackSystemsFeedback2013b,
  title = {Feedback {{Systems}} and {{Feedback Control Systems}}},
  date = {2013-08-30T14:23:56Z},
  journaltitle = {Basic Electronics Tutorials},
  url = {http://www.electronics-tutorials.ws/systems/feedback-systems.html},
  urldate = {2017-01-31},
  abstract = {Electronics Tutorial about the various Feedback Systems and Feedback Control Systems used in Feedback Amplifier and Process Control Systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W4BKKV6S\\feedback-systems.html}
}

@online{FeedbackSystemsFeedback2013c,
  title = {Feedback {{Systems}} and {{Feedback Control Systems}}},
  date = {2013-08-30T14:23:56Z},
  journaltitle = {Basic Electronics Tutorials},
  url = {http://www.electronics-tutorials.ws/systems/feedback-systems.html},
  urldate = {2017-01-31},
  abstract = {Electronics Tutorial about the various Feedback Systems and Feedback Control Systems used in Feedback Amplifier and Process Control Systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7Q5BDVTT\\feedback-systems.html}
}

@inproceedings{fengkangCorrelatedLabelPropagation2006,
  title = {Correlated {{Label Propagation}} with {{Application}} to {{Multi}}-Label {{Learning}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 2 ({{CVPR}}'06)},
  author = {{Feng Kang} and {Rong Jin} and Sukthankar, R.},
  date = {2006},
  volume = {2},
  pages = {1719--1726},
  publisher = {{IEEE}},
  location = {{New York, NY, USA}},
  doi = {10.1109/CVPR.2006.90},
  url = {http://ieeexplore.ieee.org/document/1640962/},
  urldate = {2019-11-15},
  abstract = {Many computer vision applications, such as scene analysis and medical image interpretation, are ill-suited for traditional classification where each image can only be associated with a single class. This has stimulated recent work in multi-label learning where a given image can be tagged with multiple class labels. A serious problem with existing approaches is that they are unable to exploit correlations between class labels. This paper presents a novel framework for multi-label learning termed Correlated Label Propagation (CLP) that explicitly models interactions between labels in an efficient manner. As in standard label propagation, labels attached to training data points are propagated to test data points; however, unlike standard algorithms that treat each label independently, CLP simultaneously co-propagates multiple labels. Existing work eschews such an approach since naive algorithms for label co-propagation are intractable. We present an algorithm based on properties of submodular functions that efficiently finds an optimal solution. Our experiments demonstrate that CLP leads to significant gains in precision/recall against standard techniques on two real-world computer vision tasks involving several hundred labels.},
  eventtitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 2 ({{CVPR}}'06)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NGB82XPD\\Feng Kang et al. - 2006 - Correlated Label Propagation with Application to M.pdf},
  isbn = {978-0-7695-2597-6},
  langid = {english}
}

@inproceedings{fergusonSoundSourceLocalization2018,
  title = {Sound {{Source Localization}} in a {{Multipath Environment Using Convolutional Neural Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ferguson, E. L. and Williams, S. B. and Jin, C. T.},
  date = {2018-04},
  pages = {2386--2390},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462024},
  abstract = {The propagation of sound in a shallow water environment is characterized by boundary reflections from the sea surface and sea floor. These reflections result in multiple (indirect) sound propagation paths, which can degrade the performance of passive sound source localization methods. This paper proposes the use of convolutional neural networks (CNNs) for the localization of sources of broadband acoustic radiated noise (such as motor vessels) in shallow water multipath environments. It is shown that CNNs operating on cepstrogram and generalized cross-correlogram inputs are able to estimate more reliably the instantaneous range and bearing of transiting motor vessels when the source localization performance of conventional passive ranging methods is degraded. The ensuing improvement in source localization performance is demonstrated using real data collected during an at-sea experiment.},
  keywords = {acoustic signal processing,acoustic wave reflection,at-sea experiment,boundary reflections,broadband acoustic radiated noise,cepstrogram,Cepstrum,CNNs,conventional passive ranging methods,convolutional neural networks,Delay effects,DOA estimation,Estimation,feedforward neural nets,generalized cross-correlogram inputs,motor vessels,multiple sound propagation paths,passive sonar,passive sound source localization methods,reverberation,sea floor,sea surface,Sensors,shallow water multipath environments,Sonar equipment,source localization,source localization performance,underwater acoustic propagation}
}

@article{finetteStochasticMatchedfieldLocalization2018,
  title = {Stochastic Matched-Field Localization of an Acoustic Source Based on Principles of {{Riemannian}} Geometry},
  author = {Finette, Steven and Mignerey, Peter C.},
  date = {2018-06},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{3628-3638\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5040492\}},
  abstract = {Passive localization of acoustic sources is treated within a geometric framework where non-Euclidean distance measures are computed between a cross-spectral density estimate of received data on a vertical array and a set of stochastic replica steering matrices, rather than traditional replica steering vectors. A processing scheme involving matrix-matrix comparisons where steering matrices, as functions of the replica source coordinates, naturally incorporate environmental variability or uncertainty provides a general framework for considering the acoustic inverse source problem in an ocean waveguide. Within this context a subset of matched-field processors is examined, based on recent advances in the application of non-Euclidean geometry to statistical classification of data feature clusters. The matrices are interpreted abstractly as points in a Riemannian manifold, and an appropriately defined distance measure between pairs of matrices on this manifold defines a matched-field processor for estimating source location. Acoustic simulations are performed for a waveguide comprising both a depth-dependent sound-speed profile perturbed by linear internal gravity waves and a depth-correlated surface noise field, providing an example of the viability of this approach to passive source localization in the presence of sound-speed variability.},
  affiliation = {Finette, S (Reprint Author), Naval Res Lab, Acoust Div, Code 7160, Washington, DC 20375 USA. Finette, Steven; Mignerey, Peter C., Naval Res Lab, Acoust Div, Code 7160, Washington, DC 20375 USA.},
  author-email = {steven.finette@nrl.navy.mil},
  cited-references = {Amari S., 1985, DIFFERENTIAL GEOMETR. Baggeroer AB, 2000, CONF REC ASILOMAR C, P662, DOI 10.1109/ACSSC.2000.911037. BAGGEROER AB, 1993, IEEE J OCEANIC ENG, V18, P401, DOI 10.1109/48.262292. Barbaresco F, 2009, LECT NOTES COMPUT SC, V5416, P124. Bhatia R, 2007, PRINC SER APPL MATH, P1. Carter KM, 2009, IEEE T PATTERN ANAL, V31, P2093, DOI 10.1109/TPAMI.2009.67. Chapman N. R., 2001, ADV SIGNAL PROCESSIN. Chen TY, 2012, IEEE J OCEANIC ENG, V37, P261, DOI 10.1109/JOE.2011.2181269. Cheng YQ, 2013, INFORM FUSION, V14, P311, DOI 10.1016/j.inffus.2012.02.005. Czenszak SP, 1997, J ACOUST SOC AM, V101, P749, DOI 10.1121/1.417958. Debever C, 2007, J ACOUST SOC AM, V122, P1979, DOI 10.1121/1.2769830. DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X. Finette S, 2007, J ACOUST SOC AM, V121, P2575, DOI 10.1121/1.2713724. Finette S, 2006, J ACOUST SOC AM, V120, P2567, DOI 10.1121/1.2335425. GARRETT C, 1979, ANNU REV FLUID MECH, V11, P339, DOI 10.1146/annurev.fl.11.010179.002011. Gemba KL, 2017, J ACOUST SOC AM, V141, P3411, DOI 10.1121/1.4983467. Gemba KL, 2017, J ACOUST SOC AM, V141, P92, DOI 10.1121/1.4973528. Hua XQ, 2017, DIGIT SIGNAL PROCESS, V69, P106, DOI 10.1016/j.dsp.2017.06.019. Jensen F. B., 1994, COMPUTATIONAL OCEAN, P564. Jost J., 2005, RIEMANNIAN GEOMETRY. Kotz S., 2004, MULTIVARIATE T DISTR. Kulis B, 2012, FDN TRENDS MACH LEAR, V5, P287, DOI DOI 10.1561/2200000019. KULLBACK S., 1968, INFORM THEORY STAT. KUPERMAN WA, 1980, J ACOUST SOC AM, V67, P1988, DOI 10.1121/1.384439. Le Gall Y, 2016, J ACOUST SOC AM, V139, P993, DOI 10.1121/1.4941997. Le Gall Y, 2014, IEEE T SIGNAL PROCES, V62, P5825, DOI 10.1109/TSP.2014.2360818. Li Y, 2012, IET SIGNAL PROCESS, V6, P288, DOI 10.1049/iet-spr.2011.0234. Li YL, 2013, IEEE J-STSP, V7, P655, DOI 10.1109/JSTSP.2013.2260320. Maiwald D, 2000, IEE P-RADAR SON NAV, V147, P162, DOI 10.1049/ip-rsn:20000493. Mantzel W, 2012, J ACOUST SOC AM, V132, P90, DOI 10.1121/1.4728224. Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937. Moakher M, 2011, J MATH IMAGING VIS, V40, P171, DOI 10.1007/s10851-010-0255-x. Singh V, 2012, J ACOUST SOC AM, V131, P292, DOI 10.1121/1.3664089. SULLIVAN EJ, 1993, IEEE J OCEANIC ENG, V18, P156, DOI 10.1109/JOE.1993.236354. Tollefsen D, 2017, IEEE J OCEANIC ENG, V42, P654, DOI 10.1109/JOE.2016.2615720. WESTWOOD EK, 1992, J ACOUST SOC AM, V91, P2777, DOI 10.1121/1.402958. Worthmann BM, 2017, J ACOUST SOC AM, V141, P543, DOI 10.1121/1.4973955. Xu W, 2006, IEEE J OCEANIC ENG, V31, P325, DOI 10.1109/JOE.2006.875106. Yang TC, 2014, J ACOUST SOC AM, V135, P1218, DOI 10.1121/1.4863270. Yang TC, 1999, IEEE J OCEANIC ENG, V24, P333, DOI 10.1109/48.775295. Zhou Y, 2016, IEEE J OCEANIC ENG, V41, P395, DOI 10.1109/JOE.2015.2431740.},
  da = {2018-10-18},
  doc-delivery-number = {GL3LI},
  eissn = {1520-8524},
  funding-acknowledgement = {NRL through the Office of Naval Research},
  funding-text = {This work was sponsored by NRL base funding through the Office of Naval Research. The authors thank the reviewers for their detailed constructive comments on the manuscript.},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {OCEAN; CLASSIFICATION; MATRICES; NOISE; ARRAY},
  langid = {english},
  number = {6},
  number-of-cited-references = {41},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000437036000053},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@inproceedings{firoozabadiSoundSourceLocalization2018,
  title = {Sound {{Source Localization}} by {{Proposed Subband Adaptive GEVD Algorithm Based}} on {{GammaTone Filter Bank}} in {{Undesirable Acoustical Conditions}}},
  booktitle = {2018 11th {{International Symposium}} on {{Communication Systems}}, {{Networks Digital Signal Processing}} ({{CSNDSP}})},
  author = {Firoozabadi, A. D. and Durney, H. and Soto, I. and Olave, M. S.},
  date = {2018-07},
  pages = {1--6},
  doi = {10.1109/CSNDSP.2018.8471763},
  abstract = {Sound source localization methods are implemented with different algorithms. Some methods are based on received energy to microphones and some other methods estimate the Time Difference Of Arrival (TDOA) of sound sources. The energy-based methods have high computational complexity but the Time Delay Estimation (TDE)-based methods have low accuracy in undesirable acoustical conditions. Adaptive Generalized EigenValue Decomposition (GEVD) algorithm is a method for TDE that has appropriate accuracy in noisy conditions but it does not have an acceptable performance in noisy-reverberant scenarios. The proposed method in this paper is subband adaptive GEVD based on GammaTone filter bank for TDE. Since the speech signal information is different in frequency bands, then it is necessary to present a method to concentrate on low frequency components of speech signal to have better accuracy for sound source localization. In proposed method, firstly the two microphone signals are divided to different subbands with GammaTone filter bank. This filter bank is designed based on the human auditory. Then, the GEVD function is implemented on these subbands information. Finally, the output of GEVD function are weighted and combined based on the frequency spectrum energy in different subbands. The experiments on noisy and reverberant scenarios show the superiority of proposed method in comparison with GEVD algorithm in different scenarios. Although proposed method has more computational complexity because of subband processing, but the improvement in accuracy compensate this complexity.},
  keywords = {adaptive generalized eigenvalue decomposition algorithm,computational complexity,Eigenvalue and Eigenvector,eigenvalues and eigenfunctions,Eigenvalues and eigenfunctions,energy-based methods,Estimation,Filter banks,Filtering algorithms,frequency spectrum energy,GammaTone filter bank,GEVD function,high computational complexity,Information filters,microphone signals,microphones,Microphones,noisy-reverberant scenarios,signal processing,Sound source localization,sound source localization methods,sound sources,speech signal information,subband adaptive GEVD algorithm,Subband processing,subbands information,Time Delay Estimation (TDE),time delay estimation-based methods,Time Difference Of Arrival (TDOA),time-of-arrival estimation}
}

@book{fletcherPhysicsMusicalInstruments2008,
  title = {The {{Physics}} of {{Musical Instruments}}},
  author = {Fletcher, Neville H. and Rossing, Thomas},
  date = {2008-05-23},
  publisher = {{Springer Science \& Business Media}},
  abstract = {When we wrote the first edition of this book, we directed our presenta tion to the reader with a compelling interest in musical instruments who has "a reasonable grasp of physics and who is not frightened by a little mathematics." We are delighted to find how many such people there are. The opportunity afforded by the preparation of this second edition has allowed us to bring our discussion up to date by including those new insights that have arisen from the work of many dedicated researchers over the past decade. We have also taken the opportunity to revise our presentation of some aspects of the subject to make it more general and, we hope, more immediately accessible. We have, of course, corrected any errors that have come to our attention, and we express our thanks to those friends who pointed out such defects in the early printings of the first edition. We hope that this book will continue to serve as a guide, both to those undertaking research in the field and to those who simply have a deep interest in the subject. June 1991 N.H.F and T.D.R.},
  eprint = {9CRSRYQlRLkC},
  eprinttype = {googlebooks},
  isbn = {978-0-387-98374-5},
  keywords = {Music / General,Science / Acoustics & Sound,Science / Physics / General,Science / Waves & Wave Mechanics},
  langid = {english},
  pagetotal = {784}
}

@online{FollowingDigitalAudio,
  title = {Following the {{Digital Audio Chain}}},
  url = {https://mkpe.com/publications/theme_park_systems/digitalaudiochain.php},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FAR7DK2T\\digitalaudiochain.html}
}

@article{fortuinSurveyLiteratureReflection1970,
  title = {Survey of Literature on Reflection and Scattering of Sound Waves at the Sea Surface},
  author = {Fortuin, Leonard},
  date = {1970},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {47},
  pages = {1209--1228},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SW2DIMHX\\Fortuin - 1970 - Survey of literature on reflection and scattering .pdf},
  issue = {5B}
}

@online{FrequencyMapSelection,
  title = {Frequency Map Selection Using a {{RBFN}}-Based Classifier in the {{MVDR}} Beamformer for Speaker Localization in Reverberant Rooms - {{Semantic Scholar}}},
  url = {/paper/Frequency-map-selection-using-a-RBFN-based-Salvati-Drioli/80ec4373108a8dc96b99937f14956f39ed284e91},
  urldate = {2017-01-12},
  abstract = {Semantic Scholar extracted view of \&quot;Frequency map selection using a RBFN-based classifier in the MVDR beamformer for speaker localization in reverberant rooms\&quot; by Daniele Salvati et al.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7KZV2QN3\\80ec4373108a8dc96b99937f14956f39ed284e91.html}
}

@online{FtmlDviFtml,
  title = {Ftml.Dvi - Ftml.Pdf},
  url = {http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf},
  urldate = {2017-01-30},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KPQFJIZI\\ftml.html}
}

@article{ftmldviftml,
  title = {Ftml.{{Dvi}} - {{Ftml}}.{{Pdf}}},
  url = {http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WKTA76HE\\ftml.html}
}

@article{gandemerPerceptionSurroundingSound2018,
  title = {Perception of {{Surrounding Sound Source Trajectories}} in the {{Horizontal Plane}}: {{A Comparison}} of {{VBAP}} and {{Basic}}-{{Decoded HOA}}},
  author = {Gandemer, Lennie and Parseihian, Gaetan and Bourdin, Christophe and Kronland-Martinet, Richard},
  date = {2018-03/2018-04},
  journaltitle = {ACTA ACUSTICA UNITED WITH ACUSTICA},
  volume = {104},
  pages = {\{338-350\}},
  publisher = {S HIRZEL VERLAG},
  location = {POSTFACH 10 10 61, D-70 009 STUTTGART, GERMANY},
  issn = {1610-1928},
  doi = {\{10.3813/AAA.919176\}},
  abstract = {Despite the fundamental role played by sound in multiple virtual reality contexts, few studies have explored the perception of virtual sound source motion in the acoustic space. The goal of this study was to compare the localization of virtual moving sound sources rendered with two different spatialization techniques: Vector Base Amplitude Panning (VBAP) and fifth-order Ambisonics (HOA), both implemented in a soundproofed room and in their most basic form (basic decoding of HOA, VBAP without spread parameter). The perception of virtual sound trajectories surrounding untrained subjects (n=23) was evaluated using a new method based on a drawing-augmented multiple-choice questionnaire. In the spherical loudspeaker array used in this study, VBAP proved to be a robust spatialization technique for sound trajectory rendering in terms of trajectory recognition and height perception. In basic-decoded HOA, subjects exhibited far more disparate trajectory recognition and height perception performances but performed better in perceiving sound source movement homogeneity.},
  affiliation = {Gandemer, L (Reprint Author), Aix Marseille Univ, CNRS, PRISM Percept Representat Image Sound Mus, Marseille, France. Gandemer, Lennie; Parseihian, Gaetan; Kronland-Martinet, Richard, Aix Marseille Univ, CNRS, PRISM Percept Representat Image Sound Mus, Marseille, France. Bourdin, Christophe, Aix Marseille Univ, CNRS, ISM, Marseille, France.},
  author-email = {lennie.gandemer@gmail.com},
  cited-references = {[Anonymous], 2012, COMPUTING R FDN STAT. Bahu H, 2016, ACTA ACUST UNITED AC, V102, P107, DOI 10.3813/AAA.918928. Ballas J. A., 2001, P 7 INT C AUD DISPL. Bates D., 2012, LME4 LINEAR MIXED EF. Batke J.-M., 2010, 2 AMB S PAR. Begault D. R., 2000, TREJO 3 SOUND VIRTUA. Blauert J., 1997, SPATIAL HEARING PSYC. Borss C., 2014, AUDIO ENG SOC CONVEN, V137. BRONKHORST AW, 1995, J ACOUST SOC AM, V98, P2542, DOI 10.1121/1.413219. Carlile S, 2002, J ACOUST SOC AM, V111, P1026, DOI 10.1121/1.1436067. Carlile S, 2016, TRENDS HEAR, V20, DOI [10.1177/2331216516644254, 10.1177/2331216516614254]. Carpentier T., 2015, RECENTS DEV SPATIALI. CHOWNING JM, 1971, J AUDIO ENG SOC, V19, P2. Daniel J., 2004, AUDIO ENG SOC CONVEN, V116. Daniel J., 2003, AUDIO ENG SOC CONVEN, V114. Daniel J., 2003, AUD ENG SOC C 23 INT. Djelani T, 2000, ACUSTICA, V86, P1046. Dunai L., 2011, VIRTUAL MOVING SOUND. Feron FX, 2010, J ACOUST SOC AM, V128, P3703, DOI 10.1121/1.3502456. Gadia D, 2014, DISPLAYS, V35, P206, DOI 10.1016/j.displa.2014.05.010. Gandemer L, 2014, EXP BRAIN RES, V232, P3813, DOI 10.1007/s00221-014-4066-y. Gerzon M. A., 1992, AUDIO ENG SOC CONVEN, V92. GERZON MA, 1985, J AUDIO ENG SOC, V33, P859. GILKEY RH, 1995, BEHAV RES METH INSTR, V27, P1, DOI 10.3758/BF03203614. GRANTHAM DW, 1986, J ACOUST SOC AM, V79, P1939, DOI 10.1121/1.393201. Grohn M., 2002, AUD ENG SOC C 22 INT. Kaczmarek T, 2005, J ACOUST SOC AM, V117, P3149, DOI 10.1121/1.1880832. Lewald J, 2013, NEUROPSYCHOLOGIA, V51, P181, DOI 10.1016/j.neuropsychologia.2012.11.017. Lutfi RA, 1999, J ACOUST SOC AM, V106, P919, DOI 10.1121/1.428033. Malham D. G., 1992, P I AC ENGL, V14, P209. Marentakis G, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2536764.2536769. Moreno D. Caro, 2015, SPATIAL SOUND DESIGN. Neelon MF, 2003, HEARING RES, V180, P57, DOI 10.1016/S0378-5955(03)00095-9. Oreinos C, 2015, J ACOUST SOC AM, V137, P3447, DOI 10.1121/1.4919330. Parseihian G., 2015, INT C SPAT AUD ICSA. PERROTT DR, 1977, J ACOUST SOC AM, V62, P1463, DOI 10.1121/1.381675. Prime SL, 2010, EXP BRAIN RES, V203, P249, DOI 10.1007/s00221-010-2224-4. Pulkki V, 2001, J AUDIO ENG SOC, V49, P753. Pulkki V., 2000, P INT COMP MUS C, P304. Riva G, 2005, CYBERPSYCHOL BEHAV, V8, P220, DOI 10.1089/cpb.2005.8.220. Rojas D., 2015, P 7 INT WORKSH QUAL, P1. Rosenblum LD, 2000, ECOL PSYCHOL, V12, P273, DOI 10.1207/S15326969ECO1204_02. Seth A, 2011, VIRTUAL REAL-LONDON, V15, P5, DOI 10.1007/s10055-009-0153-y. Spiegel DP, 2013, NEUROTHERAPEUTICS, V10, P831, DOI 10.1007/s13311-013-0200-y. Spors S, 2007, J ACOUST SOC AM, V122, P354, DOI 10.1121/1.2737669. Thoret E., 2015, SENS THEOR PERC CONS. Valjamae A, 2009, BRAIN RES REV, V61, P240, DOI 10.1016/j.brainresrev.2009.07.001. WIGHTMAN FL, 1989, J ACOUST SOC AM, V85, P868, DOI 10.1121/1.397558. Wittek H., 2002, AUDIO ENG SOC CONVEN, V112. Wuerger S, 2010, INFORM FUSION, V11, P45, DOI 10.1016/j.inffus.2009.04.005. Yost WA, 2015, J ACOUST SOC AM, V138, P3293, DOI 10.1121/1.4935091. Zahorik P, 2002, J ACOUST SOC AM, V111, P1832, DOI 10.1121/1.1458027. Zahorik P, 2006, J ACOUST SOC AM, V120, P343, DOI 10.1121/1.2208429. Zotter F, 2012, ACTA ACUST UNITED AC, V98, P37, DOI 10.3813/AAA.918490. Zotter F., 2010, P 1 EAA EUROREGIO.},
  da = {2018-10-18},
  doc-delivery-number = {GA3VS},
  eissn = {1861-9959},
  funding-acknowledgement = {French National Research Agency (ANR) under the SoniMove: Inform, Guide and Learn Actions by Sounds project [ANR-14-CE24-0018-01]},
  funding-text = {This work was funded by the French National Research Agency (ANR) under the SoniMove: Inform, Guide and Learn Actions by Sounds project (ANR-14-CE24-0018-01). The authors would like to thank Clement Ruffini who took part in the setting up and the operation of the localization test.},
  journal-iso = {Acta Acust. United Acust.},
  keywords-plus = {AUDITORY MOTION; SOURCE VELOCITY; LOCALIZATION; DISCRIMINATION; LISTENERS; AMBISONICS; SIMULATION; RESPONSES; HEAD},
  langid = {english},
  number = {2},
  number-of-cited-references = {55},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000428258600018},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Acoustics}
}

@online{geitgeyMachineLearningFun2016,
  title = {Machine {{Learning}} Is {{Fun Part}} 6: {{How}} to Do {{Speech Recognition}} with {{Deep Learning}}},
  shorttitle = {Machine {{Learning}} Is {{Fun Part}} 6},
  author = {Geitgey, Adam},
  date = {2016-12-24T00:19:06.082Z},
  journaltitle = {Medium},
  url = {https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a},
  urldate = {2018-04-29},
  abstract = {Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BJKWA5A8\\machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a.html}
}

@article{ghahramaniGraphbasedSemisupervisedLearning,
  title = {Graph-Based {{Semi}}-Supervised {{Learning}}},
  author = {Ghahramani, Zoubin},
  pages = {46},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RSXQS699\\Ghahramani - Graph-based Semi-supervised Learning.pdf},
  langid = {english}
}

@online{GIRDSystemsIntro,
  title = {{{GIRD}}\_{{Systems}}\_{{Intro}}\_to\_{{MUSIC}}\_{{ESPRIT}}.Pdf},
  url = {http://www.girdsystems.com/pdf/GIRD_Systems_Intro_to_MUSIC_ESPRIT.pdf},
  urldate = {2017-01-30},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UIE2PAG8\\GIRD_Systems_Intro_to_MUSIC_ESPRIT.html}
}

@article{glasbergModelLoudnessApplicable2002,
  title = {A {{Model}} of {{Loudness Applicable}} to {{Time}}-{{Varying Sounds}}},
  author = {Glasberg, Brian R. and Moore, Brian C. J.},
  date = {2002-05-15},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {JAES},
  volume = {50},
  pages = {331--342},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=11081},
  urldate = {2017-12-04},
  abstract = {Previously we described a model for calculating the loudness of steady sounds from their spectrum. Here a new version of the model is presented, which uses a waveform as its input. The stages of the model are as follows. (a) A finite impulse response filter representing transfer through the outer and middle ear. (b) Calculation of the short-term spectrum using the fast Fourier transform (FFT). To give adequate spectral resolution at low frequencies, combined with adequate temporal resolution...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HP76NCT8\\browse.html},
  langid = {english},
  number = {5}
}

@article{GlobalISOMAPLocal,
  title = {Global ({{ISOMAP}}) versus {{Local}} ({{LLE}}) {{Methods}} in {{Nonlinear Dimensionality Reduction}}},
  pages = {41},
  abstract = {Let Y ⊆ Rd be a convex domain and let f : Y → RD be a smooth embedding into a manifold M for some D {$>$} d. Hidden data \{yi \} are generated randomly and then mapped using f to \{xi = f (yi )\}. We would like to recover Y and f based on a given set \{xi \} of observed data in RD .},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GS8IJ5AV\\Global (ISOMAP) versus Local (LLE) Methods in Nonl.pdf},
  langid = {english}
}

@inproceedings{goupell_accuracy_2008,
  title = {The {{Accuracy}} of {{Localizing Virtual Sound Sources}}: {{Effects}} of {{Pointing Method}} and {{Visual Environment}}},
  shorttitle = {The {{Accuracy}} of {{Localizing Virtual Sound Sources}}},
  author = {Goupell, Matthew and Laback, Bernhard and Majdak, Piotr and Mihocic, Michael},
  date = {2008-05-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=14537},
  urldate = {2017-01-10},
  abstract = {The ability to localize sound sources in 3D-space was tested in humans. The subjects listened to noises filtered with subject-specific head-related transfer functions. In the experiment using naïve subjects, the conditions included the type of visual environment (darkness or structured virtual world) presented via head mounted display and pointing method (head and manual pointing). The results show that the errors in the horizontal dimension were smaller when head pointing was used. Manual...},
  eventtitle = {Audio {{Engineering Society Convention}} 124},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QFMN9KHW\\Goupell et al. - 2008 - The Accuracy of Localizing Virtual Sound Sources .pdf;C\:\\Users\\sauli\\Zotero\\storage\\GJTV5FTD\\browse.html},
  langid = {english}
}

@inproceedings{grohn_static_2002,
  title = {Static and {{Dynamic Sound Source Localization}} in a {{Virtual Room}}},
  author = {Grohn, Matti and Lokki, Tapio and Takala, Tapio},
  date = {2002-06-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=11159},
  urldate = {2017-01-10},
  abstract = {An audio localization test comparing accuracy of static and moving sound sources was carried out in a spatially immersive virtual environment, using loudspeaker array with vector based amplitude panning for virtual sound sources. Azimuth and elevation error in localization were measured. Different sound signals were compared as well. As was expected errors in azimuth localization accuracy were smaller than errors in elevation accuracy. There were more localization blur with virtual sound...},
  eventtitle = {Audio {{Engineering Society Conference}}: 22nd {{International Conference}}: {{Virtual}}, {{Synthetic}}, and {{Entertainment Audio}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QGIDDH9T\\Grohn et al. - 2002 - Static and Dynamic Sound Source Localization in a .pdf;C\:\\Users\\sauli\\Zotero\\storage\\VPFKW3UD\\browse.html},
  langid = {english}
}

@article{grondin_manyears_2013,
  title = {The {{ManyEars}} Open Framework},
  author = {Grondin, François and Létourneau, Dominic and Ferland, François and Rousseau, Vincent and Michaud, François},
  date = {2013-02-02},
  volume = {34},
  pages = {217--232},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-012-9316-x},
  url = {http://link.springer.com/article/10.1007/s10514-012-9316-x},
  urldate = {2017-01-09},
  abstract = {ManyEars is an open framework for microphone array-based audio processing. It consists of a sound source localization, tracking and separation system that can provide an enhanced speaker signal for improved speech and sound recognition in real-world settings. ManyEars software framework is composed of a portable and modular C library, along with a graphical user interface for tuning the parameters and for real-time monitoring. This paper presents the integration of the ManyEars Library with Willow Garage’s Robot Operating System. To facilitate the use of ManyEars on various robotic platforms, the paper also introduces the customized microphone board and sound card distributed as an open hardware solution for implementation of robotic audition systems.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\V8Z82XZM\\10.html},
  number = {3}
}

@article{grondin_manyears_2013-2,
  title = {The {{ManyEars}} Open Framework},
  author = {Grondin, François and Létourneau, Dominic and Ferland, François and Rousseau, Vincent and Michaud, François},
  date = {2013-04-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {34},
  pages = {217--232},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-012-9316-x},
  url = {http://link.springer.com/article/10.1007/s10514-012-9316-x},
  urldate = {2017-01-10},
  abstract = {ManyEars is an open framework for microphone array-based audio processing. It consists of a sound source localization, tracking and separation system that can provide an enhanced speaker signal for improved speech and sound recognition in real-world settings. ManyEars software framework is composed of a portable and modular C library, along with a graphical user interface for tuning the parameters and for real-time monitoring. This paper presents the integration of the ManyEars Library with Willow Garage’s Robot Operating System. To facilitate the use of ManyEars on various robotic platforms, the paper also introduces the customized microphone board and sound card distributed as an open hardware solution for implementation of robotic audition systems.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\77REEATV\\Grondin et al. - 2013 - The ManyEars open framework.pdf;C\:\\Users\\sauli\\Zotero\\storage\\DPI2EZD3\\s10514-012-9316-x.html},
  langid = {english},
  number = {3}
}

@article{grondinManyEarsOpenFramework2013,
  title = {The {{ManyEars}} Open Framework},
  author = {Grondin, François and Létourneau, Dominic and Ferland, François and Rousseau, Vincent and Michaud, François},
  date = {2013-02-02},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {34},
  pages = {217--232},
  issn = {0929-5593, 1573-7527},
  doi = {10.1007/s10514-012-9316-x},
  url = {http://link.springer.com/article/10.1007/s10514-012-9316-x},
  urldate = {2017-01-09},
  abstract = {ManyEars is an open framework for microphone array-based audio processing. It consists of a sound source localization, tracking and separation system that can provide an enhanced speaker signal for improved speech and sound recognition in real-world settings. ManyEars software framework is composed of a portable and modular C library, along with a graphical user interface for tuning the parameters and for real-time monitoring. This paper presents the integration of the ManyEars Library with Willow Garage’s Robot Operating System. To facilitate the use of ManyEars on various robotic platforms, the paper also introduces the customized microphone board and sound card distributed as an open hardware solution for implementation of robotic audition systems.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\P6ZEN7NR\\10.html},
  langid = {english},
  number = {3}
}

@book{groupMusicalInstrumentsWorld1997,
  title = {Musical {{Instruments}} of the {{World}}: {{An Illustrated Encyclopedia}}},
  shorttitle = {Musical {{Instruments}} of the {{World}}},
  author = {Group, Diagram},
  date = {1997},
  publisher = {{Sterling Publishing Company}},
  abstract = {"One of the nicest things.is that you don't have to be a musician to love it. It's a browser's delight."--The New York Times  It's the best-looking musical encyclopedia ever! Using a completely visual approach, this ultimate reference has over 4,000 drawings showcasing the evolution of instruments--and music--from primitive whistles to electric guitars (complete with fuzz box and pedal). Exquisite illustrations accompany each entry, with painstaking detail and attention to structure, function, and decoration. Most extraordinary is the range of instruments included: orchestral, popular, classical, ancient, and folk. Classed by family groups, they are cross-referenced with material on geographical distribution and historical periods. Plus scores, diagrams of correct playing techniques, major figures, and other fascinating information provided by over forty researchers, writers, and illustrators, in cooperation with 200 scholars from 20 countries!},
  eprint = {PucO4Mq413UC},
  eprinttype = {googlebooks},
  isbn = {978-0-8069-9847-3},
  keywords = {Music / Musical Instruments / General},
  langid = {english},
  pagetotal = {324}
}

@article{guentchevLearningBasedThreeDimensional,
  title = {Learning-{{Based Three Dimensional Sound Localization Using}} a {{Compact Non}}-{{Coplanar Array}} of {{Microphones}}},
  author = {Guentchev, Kamen and Weng, John},
  pages = {10},
  abstract = {Oneof the various humansensory capabilities is to identify the direction of perceived sounds. The goal of this workis to studysoundsourcelocalization in three dimensionsusing someof the most important cues the humanuses. Havingrobotics as a major application, the approachinvolves a compactsensor structure that can be placed on a mobile platform. Theobjective is to estimate the relative soundsourceposition in three dimensional space without imposingexcessive restrictions on its spatio-temporal characteristics and the environmentstructure. Twotypes of features are considered, interaural time and level differences. Theirrelative effectivenessfor localization is studied, as well as a practical wayof using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknownsounds are processed by the trained systemto estimate the 3Dlocation of the sound source. Results from the experiments showedunder ±3° in average angular error and less than ±20\%in average radial distance error.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RSI5I977\\Guentchev and Weng - Learning-Based Three Dimensional Sound Localizatio.pdf},
  langid = {english}
}

@article{guentchevLearningBasedThreeDimensionala,
  title = {Learning-{{Based Three Dimensional Sound Localization Using}} a {{Compact Non}}-{{Coplanar Array}} of {{Microphones}}},
  author = {Guentchev, Kamen and Weng, John},
  pages = {10},
  abstract = {Oneof the various humansensory capabilities is to identify the direction of perceived sounds. The goal of this workis to studysoundsourcelocalization in three dimensionsusing someof the most important cues the humanuses. Havingrobotics as a major application, the approachinvolves a compactsensor structure that can be placed on a mobile platform. Theobjective is to estimate the relative soundsourceposition in three dimensional space without imposingexcessive restrictions on its spatio-temporal characteristics and the environmentstructure. Twotypes of features are considered, interaural time and level differences. Theirrelative effectivenessfor localization is studied, as well as a practical wayof using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknownsounds are processed by the trained systemto estimate the 3Dlocation of the sound source. Results from the experiments showedunder ±3° in average angular error and less than ±20\%in average radial distance error.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DAV4Z6KI\\Guentchev and Weng - Learning-Based Three Dimensional Sound Localizatio.pdf},
  langid = {english}
}

@inproceedings{gustafssonPositioningUsingTimedifference2003,
  title = {Positioning Using Time-Difference of Arrival Measurements},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  author = {Gustafsson, F. and Gunnarsson, F.},
  date = {2003-04},
  volume = {6},
  pages = {VI-553},
  doi = {10.1109/ICASSP.2003.1201741},
  abstract = {The problem of position estimation from time difference of arrival (TDOA) measurements occurs in a range of applications from wireless communication networks to electronic warfare positioning. Correlation analysis of the transmitted signal to two receivers gives rise to one hyperbolic function. With more than two receivers, we can compute more hyperbolic functions, which ideally intersect in one unique point. With TDOA measurement uncertainty, we face a non-linear estimation problem. We suggest and compare a Monte Carlo based method for positioning and a gradient search algorithm using a nonlinear least squares framework. The former has the feature of being easily extended to a dynamic framework where a motion model of the transmitter is included. A small simulation study is presented.},
  eventtitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TYFMNHWH\\Gustafsson and Gunnarsson - 2003 - Positioning using time-difference of arrival measu.pdf;C\:\\Users\\sauli\\Zotero\\storage\\H7GKWWZC\\1201741.html},
  keywords = {correlation analysis,Electronic warfare,electronic warfare positioning,gradient methods,gradient search algorithm,hyperbolic function,least squares approximations,Least squares methods,Measurement uncertainty,Military computing,Monte Carlo method,Monte Carlo methods,navigation,nonlinear least squares framework,parameter estimation,position estimation,Position measurement,radio receivers,radio transmitters,Signal analysis,TDOA measurements,Time difference of arrival,Time measurement,time-difference of arrival measurements,Wireless communication,wireless communication networks}
}

@inproceedings{gustafssonPositioningUsingTimedifference2003a,
  title = {Positioning Using Time-Difference of Arrival Measurements},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2003. {{Proceedings}}. ({{ICASSP}} '03).},
  author = {Gustafsson, F. and Gunnarsson, F.},
  date = {2003},
  volume = {1},
  pages = {VI-553-6},
  publisher = {{IEEE}},
  location = {{Hong Kong, China}},
  doi = {10.1109/ICASSP.2003.1201741},
  url = {http://ieeexplore.ieee.org/document/1201741/},
  urldate = {2018-11-20},
  abstract = {The problem of position estimation from Time Difference Of Arrival (TDOA) measurements occurs in a range of applications from wireless communication networks to electronic warfare positioning. Correlation analysis of the transmitted signal to two receivers gives rise to one hyperbolic function. With more than two receivers, we can compute more hyperbolic functions, which ideally intersect in one unique point. With TDOA measurement uncertainty, we face a non-linear estimation problem. We here suggest and compare both a Monte Carlo based method for positioning and a gradient search algorithm using a non-linear least squares framework. The former has the feature to be easily extended to a dynamic framework where a motion model of the transmitter is included. A small simulation study is presented.},
  eventtitle = {International {{Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}}'03)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7X9Z3X7S\\Gustafsson and Gunnarsson - 2003 - Positioning using time-difference of arrival measu.pdf},
  isbn = {978-0-7803-7663-2},
  langid = {english}
}

@article{guy-bartComparisonDifferentImpulse,
  title = {Comparison of Different Impulse Response Measurement Techniques},
  author = {Guy-Bart, STAN and Jean-Jacques, EMBRECHTS and Dominique, ARCHAMBEAU},
  pages = {17},
  abstract = {The impulse response of an acoustical space or transducer is one of their most important characterization. In order to perform the measurement of their impulse responses, four of the most suited methods are compared : MLS (Maximum Length Sequence), IRS (Inverse Repeated Sequence), Time-Stretched Pulses and SineSweep. These different methods have already been described in the literature. Nevertheless, the choice of one of these methods depending on the measurement conditions is critical. Therefore, an extensive comparison has been realized. This comparison has been done through the implementation and realization of a complete, fast, reliable and cheap measurement system. Finally, a conclusion for the use of each method according to the principal measurment conditions is presented. It is shown that in the presence of non white noise, the MLS and IRS techniques seem to be more accurate. On the contrary, in quiet environments the Logarithmic SineSweep method seems to be the most appropriate.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\83MSFQT4\\Guy-Bart et al. - Comparison of diﬀerent impulse response measuremen.pdf},
  langid = {english}
}

@article{habetsRoomImpulseResponse,
  title = {Room {{Impulse Response Generator}}},
  author = {Habets, Emanuel A.P.},
  pages = {21},
  abstract = {This report provides a short overview of different methods that can be used for simulating room acoustics and focusses on the imagemethod that was proposed by Allen and Berkley in 1979. The image method is probably one of the methods most commonly used in the acoustic signal processing community, and will therefore be discussed in more detail. A mex-function, which can be used in MATLAB, has been created to generate multichannel Room Impulse Responses using the image method. This function enables the user to control the reflection order, room dimension and microphone directivity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\S2ZAFBVP\\Room Impulse Response Generator.pdf},
  langid = {english}
}

@thesis{hackMultipleSourceLocalization2015,
  title = {Multiple {{Source Localization}} with {{Distributed Tetrahedral Microphone Arrays}}},
  author = {Hack, Philipp},
  date = {2015},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\P4XW6HH9\\Hack - Multiple Source Localization with Distributed Tetr.pdf},
  langid = {english}
}

@article{hakMeasuringRoomImpulse2012,
  title = {Measuring Room Impulse Responses : Impact of the Decay Range on Derived Room Acoustic Parameters},
  shorttitle = {Measuring Room Impulse Responses},
  author = {Hak, C. C. J. M. and Wenmaekers, R. H. C. and L. C.J. Luxemburg, Van},
  date = {2012},
  journaltitle = {Acta Acustica united with Acustica},
  volume = {98},
  pages = {907--915},
  issn = {1610-1928},
  doi = {10.3813/AAA.918574},
  url = {https://research.tue.nl/en/publications/measuring-room-impulse-responses-impact-of-the-decay-range-on-der},
  urldate = {2019-06-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\24EMJBQC\\Hak et al. - 2012 - Measuring room impulse responses  impact of the d.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IDMB4TDV\\measuring-room-impulse-responses-impact-of-the-decay-range-on-der.html},
  langid = {english},
  number = {6}
}

@inproceedings{hammondRobustFullSphereBinaural2018,
  title = {Robust {{Full}}-{{Sphere Binaural Sound Source Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hammond, B. R. and Jackson, P. J. B.},
  date = {2018-04},
  pages = {86--90},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462103},
  abstract = {We propose a novel method for full-sphere binaural sound source localization that is designed to be robust to real world recording conditions. A mask is proposed that is designed to remove diffuse noise and early room reflections. The method makes use of the interaural phase difference (IPD) for lateral angle localization and spectral cues for polar angle localization. The method is tested using different HRTF datasets to generate the test data and training data. The method is also tested with the presence of additive noise and reverberation. The method outperforms the state of the art binaural localization methods for most testing conditions.},
  keywords = {acoustic noise,acoustic signal processing,additive noise,architectural acoustics,Binaural,Cepstrum,diffuse noise,Ear,early room reflections,Frequency estimation,HRTF,HRTF datasets,Indexes,interaural phase difference,lateral angle localization,Localization,mask,polar angle localization,real world recording conditions,reverberation,Reverberation,robust full-sphere binaural sound source localization,Robustness,spectral cues,test data,testing conditions,Time-frequency analysis}
}

@inproceedings{harmaAutomaticSurveillanceAcoustic2005,
  title = {Automatic {{Surveillance}} of the {{Acoustic Activity}} in {{Our Living Environment}}},
  booktitle = {2005 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Harma, A. and McKinney, M.F. and Skowronek, J.},
  date = {2005},
  pages = {634--637},
  publisher = {{IEEE}},
  location = {{Amsterdam, The Netherlands}},
  doi = {10.1109/ICME.2005.1521503},
  url = {http://ieeexplore.ieee.org/document/1521503/},
  urldate = {2019-07-29},
  abstract = {We report an experiment with an acoustic surveillance system comprised of a computer and microphone situated in a typical office environment. The system continuously analyzes the acoustic activity at the recording site, separates all interesting events, and stores them in a database. All interesting acoustic events over a duration of more than two months were recorded. A number of low-level signal features are computed from the audio signal and used to classify and identify sound events. The analysis reveals interesting patterns and activities which would be difficult to find by any other means.},
  eventtitle = {2005 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HAH4YAIG\\Harma et al. - 2005 - Automatic Surveillance of the Acoustic Activity in.pdf},
  isbn = {978-0-7803-9331-8},
  langid = {english}
}

@article{hawkesWidebandSourceLocalization2003,
  title = {Wideband Source Localization Using a Distributed Acoustic Vector-Sensor Array},
  author = {Hawkes, M. and Nehorai, A.},
  date = {2003-06},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {51},
  pages = {1479--1491},
  issn = {1053-587X},
  doi = {10.1109/TSP.2003.811225},
  abstract = {We derive fast wideband algorithms, based on measurements of the acoustic intensity, for determining the bearings of a target using an acoustic vector sensor (AVS) situated in free space or on a reflecting boundary. We also obtain a lower bound on the mean-square angular error (MSAE) of such estimates. We then develop general closed-form weighted least-squares (WLS) and reweighted least-squares algorithms that compute the three-dimensional (3-D) location of a target whose bearing to a number of dispersed locations has been measured. We devise a scheme for adaptively choosing the weights for the WLS routine when measures of accuracy for the bearing estimates, such as the lower bound on the MSAE, are available. In addition, a measure of the potential estimation accuracy of a distributed system is developed based on a two-stage application of the Cramer-Rao bound. These 3-D results are quite independent of how bearing estimates are obtained. Naturally, the two parts of the paper are tied together by examining how well distributed arrays of AVSs located on the ground, seabed, and in free space can determine the 3-D position of a target The results are relevant to the localization of underwater and airborne sources using freely drifting, moored, or ground sensors. Numerical simulations illustrate the effectiveness of our estimators and the new potential performance measure.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\M544XG7I\\Hawkes and Nehorai - 2003 - Wideband source localization using a distributed a.pdf;C\:\\Users\\sauli\\Zotero\\storage\\CBSU99ZU\\1200138.html},
  keywords = {acoustic arrays,acoustic intensity measurements,Acoustic measurements,Acoustic sensors,acoustic signal processing,airborne sources,array signal processing,closed-form reweighted least-squares algorithm,closed-form weighted least-squares algorithm,Cramer-Rao bound,direction-of-arrival estimation,distributed acoustic vector-sensor array,distributed processing,distributed system,fast wideband algorithms,freely drifting sensors,ground sensors,least mean squares methods,lower bound,mean-square angular error,moored sensors,reflecting boundary,reweighted least-squares algorithms,Sea measurements,seabed,Sensor arrays,Surveillance,target 3D location,target bearing estimation,target three-dimensional location,Underwater acoustics,underwater sound,underwater sources,Underwater tracking,Velocity measurement,Wideband,wideband decentralized processing,wideband source localization,WLS routine},
  number = {6}
}

@article{hayashiDurationControlledLSTMPolyphonic2017,
  title = {Duration-{{Controlled LSTM}} for {{Polyphonic Sound Event Detection}}},
  author = {Hayashi, T. and Watanabe, S. and Toda, T. and Hori, T. and Roux, J. Le and Takeda, K.},
  date = {2017-11},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  pages = {2059--2070},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2740002},
  abstract = {This paper presents a new hybrid approach called duration-controlled long short-term memory (LSTM) for polyphonic sound event detection (SED). It builds upon a state-of-the-art SED method that performs frame-by-frame detection using a bidirectional LSTM recurrent neural network (BLSTM), and incorporates a duration-controlled modeling technique based on a hidden semi-Markov model. The proposed approach makes it possible to model the duration of each sound event precisely and to perform sequence-by-sequence detection without having to resort to thresholding, as in conventional frame-by-frame methods. Furthermore, to effectively reduce sound event insertion errors, which often occur under noisy conditions, we also introduce a binary-mask-based postprocessing that relies on a sound activity detection network to identify segments with any sound event activity, an approach inspired by the well-known benefits of voice activity detection in speech recognition systems. We conduct an experiment using the DCASE2016 task 2 dataset to compare our proposed method with typical conventional methods, such as nonnegative matrix factorization and standard BLSTM. Our proposed method outperforms the conventional methods both in an event-based evaluation, achieving a 75.3\% F1 score and a 44.2\% error rate, and in a segment-based evaluation, achieving an 81.1\% F1 score, and a 32.9\% error rate, outperforming the best results reported in the DCASE2016 task 2 Challenge.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5Q2VD7V8\\Hayashi et al. - 2017 - Duration-Controlled LSTM for Polyphonic Sound Even.pdf;C\:\\Users\\sauli\\Zotero\\storage\\U8FDM8BP\\8010445.html},
  keywords = {bidirectional LSTM recurrent neural network,binary-mask-based postprocessing,BLSTM,Computer architecture,Duration control,duration-controlled LSTM modeling technique,Event detection,frame-by-frame detection,Hidden Markov models,hidden semi-Markov model,hidden semi-Markov model (HSMM),hybrid model,Logic gates,long short-term memory (LSTM),nonnegative matrix factorization,polyphonic sound event detection,polyphonic sound event detection (SED),recurrent neural nets,recurrent neural network,Recurrent neural networks,SED method,sequence-by-sequence detection,Speech,speech processing,speech recognition,speech recognition systems},
  number = {11}
}

@article{haykin_cocktail_2005,
  title = {The {{Cocktail Party Problem}}},
  author = {Haykin, Simon and Chen, Zhe},
  date = {2005-09-01},
  volume = {17},
  pages = {1875--1902},
  issn = {0899-7667},
  doi = {10.1162/0899766054322964},
  url = {http://www.mitpressjournals.org/doi/10.1162/0899766054322964},
  urldate = {2017-01-09},
  abstract = {This review presents an overview of a challenging problem in auditory perception, the cocktail party phenomenon, the delineation of which goes back to a classic paper by Cherry in 1953. In this review, we address the following issues: (1) human auditory scene analysis, which is a general process carried out by the auditory system of a human listener; (2) insight into auditory perception, which is derived from Marr's vision theory; (3) computational auditory scene analysis, which focuses on specific approaches aimed at solving the machine cocktail party problem; (4) active audition, the proposal for which is motivated by analogy with active vision, and (5) discussion of brain theory and independent component analysis, on the one hand, and correlative neural firing, on the other.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MQHG3TJH\\0899766054322964.html},
  number = {9}
}

@article{haykin_cocktail_2005-3,
  title = {The {{Cocktail Party Problem}}},
  author = {Haykin, S. and Chen, Z.},
  date = {2005-09},
  journaltitle = {Neural Computation},
  volume = {17},
  pages = {1875--1902},
  issn = {0899-7667},
  doi = {10.1162/0899766054322964},
  abstract = {This review presents an overview of a challenging problem in auditory perception, the cocktail party phenomenon, the delineation of which goes back to a classic paper by Cherry in 1953. In this review, we address the following issues: (1) human auditory scene analysis, which is a general process carried out by the auditory system of a human listener; (2) insight into auditory perception, which is derived from Marr's vision theory; (3) computational auditory scene analysis, which focuses on specific approaches aimed at solving the machine cocktail party problem; (4) active audition, the proposal for which is motivated by analogy with active vision, and (5) discussion of brain theory and independent component analysis, on the one hand, and correlative neural firing, on the other.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DINNNHAZ\\6788551.html},
  number = {9}
}

@article{haykinCocktailPartyProblem2005,
  title = {The {{Cocktail Party Problem}}},
  author = {Haykin, S. and Chen, Z.},
  date = {2005-09},
  journaltitle = {Neural Computation},
  volume = {17},
  pages = {1875--1902},
  issn = {0899-7667},
  doi = {10.1162/0899766054322964},
  abstract = {This review presents an overview of a challenging problem in auditory perception, the cocktail party phenomenon, the delineation of which goes back to a classic paper by Cherry in 1953. In this review, we address the following issues: (1) human auditory scene analysis, which is a general process carried out by the auditory system of a human listener; (2) insight into auditory perception, which is derived from Marr's vision theory; (3) computational auditory scene analysis, which focuses on specific approaches aimed at solving the machine cocktail party problem; (4) active audition, the proposal for which is motivated by analogy with active vision, and (5) discussion of brain theory and independent component analysis, on the one hand, and correlative neural firing, on the other.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\USCCHTJ7\\6788551.html},
  number = {9}
}

@article{haykinCocktailPartyProblem2005a,
  title = {The {{Cocktail Party Problem}}},
  author = {Haykin, Simon and Chen, Zhe},
  date = {2005-09-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {17},
  pages = {1875--1902},
  issn = {0899-7667},
  doi = {10.1162/0899766054322964},
  url = {http://www.mitpressjournals.org/doi/10.1162/0899766054322964},
  urldate = {2017-01-09},
  abstract = {This review presents an overview of a challenging problem in auditory perception, the cocktail party phenomenon, the delineation of which goes back to a classic paper by Cherry in 1953. In this review, we address the following issues: (1) human auditory scene analysis, which is a general process carried out by the auditory system of a human listener; (2) insight into auditory perception, which is derived from Marr's vision theory; (3) computational auditory scene analysis, which focuses on specific approaches aimed at solving the machine cocktail party problem; (4) active audition, the proposal for which is motivated by analogy with active vision, and (5) discussion of brain theory and independent component analysis, on the one hand, and correlative neural firing, on the other.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RA5ZTHDP\\0899766054322964.html},
  number = {9}
}

@article{heADAPTATIONMULTIPLESOUND,
  title = {{{ADAPTATION OF MULTIPLE SOUND SOURCE LOCALIZATION NEURAL NETWORKS WITH WEAK SUPERVISION AND DOMAIN}}-{{ADVERSARIAL TRAINING}}},
  author = {He, Weipeng and Motlicek, Petr and Odobez, Jean-Marc},
  pages = {5},
  abstract = {Despite the recent success of deep neural network-based approaches in sound source localization, these approaches suffer the limitations that the required annotation process is costly, and the mismatch between the training and test conditions undermines the performance. This paper addresses the question of how models trained with simulation can be exploited for multiple sound source localization in real scenarios by domain adaptation. In particular, two domain adaptation methods are investigated: weak supervision and domainadversarial training. Our experiments show that the weak supervision with the knowledge of the number of sources can significantly improve the performance of an unadapted model. However, the domain-adversarial training does not yield significant improvement for this particular problem.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NW8B7P77\\He et al. - ADAPTATION OF MULTIPLE SOUND SOURCE LOCALIZATION N.pdf},
  keywords = {*****},
  langid = {english}
}

@inproceedings{heAdaptationMultipleSound2019,
  title = {Adaptation of {{Multiple Sound Source Localization Neural Networks}} with {{Weak Supervision}} and {{Domain}}-Adversarial {{Training}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {He, Weipeng and Motlicek, Petr and Odobez, Jean-Marc},
  date = {2019-05},
  pages = {770--774},
  publisher = {{IEEE}},
  location = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682655},
  url = {https://ieeexplore.ieee.org/document/8682655/},
  urldate = {2019-05-16},
  abstract = {Despite the recent success of deep neural network-based approaches in sound source localization, these approaches suffer the limitations that the required annotation process is costly, and the mismatch between the training and test conditions undermines the performance. This paper addresses the question of how models trained with simulation can be exploited for multiple sound source localization in real scenarios by domain adaptation. In particular, two domain adaptation methods are investigated: weak supervision and domainadversarial training. Our experiments show that the weak supervision with the knowledge of the number of sources can significantly improve the performance of an unadapted model. However, the domain-adversarial training does not yield significant improvement for this particular problem.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LZCZDTXJ\\He et al. - 2019 - Adaptation of Multiple Sound Source Localization N.pdf},
  isbn = {978-1-4799-8131-1},
  langid = {english}
}

@article{heDeepNeuralNetworks2017,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  author = {He, Weipeng and Motlicek, Petr and Odobez, Jean-Marc},
  date = {2017-11-30},
  doi = {10.1109/ICRA.2018.8461267},
  url = {https://arxiv.org/abs/1711.11565},
  urldate = {2018-11-05},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WU4WBAHN\\He et al. - 2017 - Deep Neural Networks for Multiple Speaker Detectio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\MN46TEUK\\1711.html},
  langid = {english}
}

@inproceedings{heDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {He, W. and Motlicek, P. and Odobez, J.},
  date = {2018-05},
  pages = {74--79},
  doi = {10.1109/ICRA.2018.8461267},
  abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\K6FPEVU4\\He et al. - 2018 - Deep Neural Networks for Multiple Speaker Detectio.pdf},
  keywords = {*****,acoustic generators,Artificial neural networks,deep neural networks,Delays,encoding,Encoding,Estimation,human-robot interaction,likelihood-based encoding,microphone arrays,Microphones,multiple sound sources,multiple speaker detection,network output,neural nets,neural network-based sound source localization methods,Robots,simultaneous detection,single sound source,sound mixtures,spatial spectrum-based approaches,speaker recognition}
}

@inproceedings{heDeepNeuralNetworks2018a,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {He, W. and Motlicek, P. and Odobez, J.},
  date = {2018-05},
  pages = {74--79},
  doi = {10.1109/ICRA.2018.8461267},
  abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X2NNTST9\\He et al. - 2018 - Deep Neural Networks for Multiple Speaker Detectio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\8GAB69N9\\8461267.html},
  keywords = {acoustic generators,Artificial neural networks,deep neural networks,Delays,encoding,Encoding,Estimation,GCC-FB,GCC-PHAT,human-robot interaction,likelihood-based encoding,microphone arrays,Microphones,multiple sound sources,multiple speaker detection,network output,neural nets,neural network-based sound source localization methods,Robots,simultaneous detection,single sound source,sound mixtures,spatial spectrum-based approaches,speaker recognition}
}

@inproceedings{heDeepNeuralNetworks2018b,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {He, W. and Motlicek, P. and Odobez, J.},
  date = {2018-05},
  pages = {74--79},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8461267},
  abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IVT56RQ8\\stamp.html},
  keywords = {acoustic generators,Artificial neural networks,deep neural networks,Delays,encoding,Encoding,Estimation,human-robot interaction,likelihood-based encoding,microphone arrays,Microphones,multiple sound sources,multiple speaker detection,network output,neural nets,neural network-based sound source localization methods,Robots,simultaneous detection,single sound source,sound mixtures,spatial spectrum-based approaches,speaker recognition}
}

@article{heusserHyperToolsPythonToolbox2017,
  title = {{{HyperTools}}: {{A Python}} Toolbox for Visualizing and Manipulating High-Dimensional Data},
  shorttitle = {{{HyperTools}}},
  author = {Heusser, Andrew C. and Ziman, Kirsten and Owen, Lucy L. W. and Manning, Jeremy R.},
  date = {2017-01-28},
  url = {http://arxiv.org/abs/1701.08290},
  abstract = {Data visualizations can reveal trends and patterns that are not otherwise obvious from the raw data or summary statistics. While visualizing low-dimensional data is relatively straightforward (for example, plotting the change in a variable over time as (x,y) coordinates on a graph), it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way. Here we present HypeTools, a Python toolbox for visualizing and manipulating large, high-dimensional datasets. Our primary approach is to use dimensionality reduction techniques (Pearson, 1901; Tipping \& Bishop, 1999) to embed high-dimensional datasets in a lower-dimensional space, and plot the data using a simple (yet powerful) API with many options for data manipulation [e.g. hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot styling. The toolbox is designed around the notion of data trajectories and point clouds. Just as the position of an object moving through space can be visualized as a 3D trajectory, HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories may be plotted as interactive static plots or visualized as animations. These same dimensionality reduction and alignment algorithms can also reveal structure in static datasets (e.g. collections of observations or attributes). We present several examples showcasing how using our toolbox to explore data through trajectories and low-dimensional embeddings can reveal deep insights into datasets across a wide variety of domains.},
  archivePrefix = {arXiv},
  eprint = {1701.08290},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EAT5WRMU\\1701.html},
  keywords = {Statistics - Other Statistics},
  primaryClass = {stat}
}

@article{hew.etal.DeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} for {{Multiple Speaker Detection}} and {{Localization}}},
  author = {He, W. et al.},
  date = {2018-05},
  journaltitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {74--79},
  doi = {10.1109/ICRA.2018.8461267},
  url = {http://arxiv.org/abs/1711.11565},
  urldate = {2019-04-24},
  abstract = {We propose to use neural networks for simultaneous detection and localization of multiple sound sources in human-robot interaction. In contrast to conventional signal processing techniques, neural network-based sound source localization methods require fewer strong assumptions about the environment. Previous neural network-based methods have been focusing on localizing a single sound source, which do not extend to multiple sources in terms of detection and localization. In this paper, we thus propose a likelihood-based encoding of the network output, which naturally allows the detection of an arbitrary number of sources. In addition, we investigate the use of sub-band cross-correlation information as features for better localization in sound mixtures, as well as three different network architectures based on different motivations. Experiments on real data recorded from a robot show that our proposed methods significantly outperform the popular spatial spectrum-based approaches.},
  archivePrefix = {arXiv},
  eprint = {1711.11565},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4Z85FDUP\\He et al. - 2018 - Deep Neural Networks for Multiple Speaker Detectio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\84LXWEQP\\1711.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multimedia,Computer Science - Robotics,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{horaudShortTutorialGraph,
  title = {A {{Short Tutorial}} on {{Graph Laplacians}}, {{Laplacian Embedding}}, and {{Spectral Clustering}}},
  author = {Horaud, Radu and Horaud, Radu},
  pages = {43},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4ZUNGA4W\\Horaud and Horaud - A Short Tutorial on Graph Laplacians, Laplacian Em.pdf},
  langid = {english}
}

@article{hoshibaAssessmentMUSICBasedNoiseRobust2018,
  title = {Assessment of {{MUSIC}}-{{Based Noise}}-{{Robust Sound Source Localization}} with {{Active Frequency Range Filtering}}},
  author = {Hoshiba, Kotaro and Nakadai, Kazuhiro and Kumon, Makoto and Okuno, Hiroshi G.},
  date = {2018-06},
  journaltitle = {JOURNAL OF ROBOTICS AND MECHATRONICS},
  volume = {30},
  pages = {\{426-435\}},
  publisher = {FUJI TECHNOLOGY PRESS LTD},
  location = {4F TORANOMON SANGYO BLDG, 2-29, TORANOMON 1-CHOME, MINATO-KU, TOKYO, 105-0001, JAPAN},
  issn = {0915-3942},
  doi = {\{10.20965/jrm.2018.p0426\}},
  abstract = {We have studied sound source localization, using a microphone array embedded on a UAV (unmanned aerial vehicle), for the purpose of detecting for people to rescue from disaster-stricken areas or other dangerous situations, and we have proposed sound source localization methods for use in outdoor environments. In these methods, noise robustness and real-time processing have a trade-off relationship, which is a problem to be solved for the practical application of the methods. Sound source localization in a disaster area requires both noise robustness and real-time processing. For this we propose a sound source localization method using an active frequency range filter based on the MU-SIC (MUltiple Signal Classification) method. Our proposed method can successively create and apply a frequency range filter by simply using the four arithmetic operations, so it can ensure both noise robustness and real-time processing. As numerical simulations carried out to compare the successful localization rate and the processing delay with conventional methods have affirmed the usefulness of the proposed method, we have successfully produced a sound source localization method that has both noise robustness and real-time processing.},
  affiliation = {Hoshiba, K (Reprint Author), Kanagawa Univ, Fac Engn, Dept Elect Elect & Informat Engn, Kanagawa Ku, 3-27-1 Rokkakubashi, Yokohama, Kanagawa 2218686, Japan. Hoshiba, Kotaro, Kanagawa Univ, Fac Engn, Dept Elect Elect & Informat Engn, Kanagawa Ku, 3-27-1 Rokkakubashi, Yokohama, Kanagawa 2218686, Japan. Nakadai, Kazuhiro, Tokyo Inst Technol, Sch Engn, Dept Syst & Control Engn, Meguro Ku, 2-12-1 Ookayama, Tokyo 1528552, Japan. Nakadai, Kazuhiro, Honda Res Inst Japan Co Ltd, 8-1 Honcho, Wako, Saitama 3510188, Japan. Kumon, Makoto, Kumamoto Univ, Fac Adv Sci & Technol, Chuo Ku, 2-39-1 Kurokami, Kumamoto, Kumamoto 8608555, Japan. Okuno, Hiroshi G., Waseda Univ, Fac Sci & Engn, Shinjuku Ku, 2-4-12 Okubo, Tokyo 1698555, Japan.},
  author-email = {hoshiba@kanagawa-u.ac.jp hoshiba@ra.sc.e.titech.ac.jp},
  cited-references = {Furukawa K, 2013, IEEE INT C INT ROBOT, P3943, DOI 10.1109/IROS.2013.6696920. Hoshiba K, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112535. Hoshiba K, 2017, J ROBOT MECHATRON, V29, P154, DOI 10.20965/jrm.2017.p0154. Nakadai K, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P832. Nakadai K, 2010, ADV ROBOTICS, V24, P739, DOI 10.1163/016918610X493561. NAKAMURA K, 2012, P IEEE INT C INT ROB, P694. Nakamura K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P664, DOI 10.1109/IROS.2009.5354419. Ohata T, 2014, IEEE INT C INT ROBOT, P1902, DOI 10.1109/IROS.2014.6942813. Okuno HG, 2017, J ROBOT MECHATRON, V29, P15, DOI 10.20965/jrm.2017.p0015. Okuno HG, 2015, INT CONF ACOUST SPEE, P5610, DOI 10.1109/ICASSP.2015.7179045. Okutani K., 2012, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012), P3288, DOI 10.1109/IROS.2012.6385994. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. WANG L, 2017, P 2017 IEEE INT C AC, P496. Xu C., 2017, P INTERSPEECH 2017, P1894.},
  da = {2018-10-18},
  doc-delivery-number = {GJ7WJ},
  eissn = {1883-8049},
  funding-acknowledgement = {JSPS KAKENHI [16H02884, 16K00294, 17K00365]; JST ImPACT Touch Robotics Challenge},
  funding-text = {This work was supported by JSPS KAKENHI 16H02884, 16K00294, 17K00365, and JST ImPACT Touch Robotics Challenge.},
  journal-iso = {J. Robot. Mechatron.},
  keywords = {(robot audition,active frequency range filter,multiple signal  classification,sound source localization,unmanned aerial vehicle)},
  keywords-plus = {SYSTEM; DESIGN},
  langid = {english},
  number = {3},
  number-of-cited-references = {14},
  research-areas = {Robotics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000435600300013},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Robotics}
}

@inproceedings{huangBiomimeticSystemLocalization1994,
  title = {A Biomimetic System for Localization and Separation of Multiple Sound Sources},
  booktitle = {Conference {{Proceedings}}. 10th {{Anniversary}}. {{IMTC}}/94. {{Advanced Technologies}} in {{I M}}. 1994 {{IEEE Instrumentation}} and {{Measurement Technolgy Conference}} ({{Cat}}. {{No}}.{{94CH3424}}-9)},
  author = {Huang, Jie and Ohnishi, N. and Sugie, N.},
  date = {1994-05},
  pages = {967-970 vol.2},
  doi = {10.1109/IMTC.1994.351945},
  abstract = {We present a system for sound source localization and separation inspired by the auditory mechanisms of biological systems. The system consists of three omni-directional microphones, banks of band-pass filters and a personal computer with a digital signal processor (DSP). Each microphone is set at a vertex of an equilateral triangle with a side length of 13.5 cm. First temporal disparities between microphones are detected by each band-pass filtered signal with some time duration (e.g. 30 sec.). It uses the onsets of a signal which are not corrupted with other sound sources including sound reflected by walls, etc. From the estimated azimuth of sound sources, we can calculate the time differences of sound arriving from different microphones. Each sound source is separated from each microphone signal using an inverse filter designed with this time difference. Experiments were carried out in an anechoic chamber and an empty room using two sound sources located with azimuth offset of 38 deg. We used a radio weather forecast by a male announcer and a radio talk show by a male and a female hosts as our sound sources. We localized each sound source with an error of less than 3 deg. and the separation of each sound source had a quality of 25 dB attenuation of each original sound},
  eventtitle = {Conference {{Proceedings}}. 10th {{Anniversary}}. {{IMTC}}/94. {{Advanced Technologies}} in {{I M}}. 1994 {{IEEE Instrumentation}} and {{Measurement Technolgy Conference}} ({{Cat}}. {{No}}.{{94CH3424}}-9)},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JL9SQPPS\\Huang et al. - 1994 - A biomimetic system for localization and separatio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\72EZ7DF7\\351945.html},
  keywords = {13.5 cm,25 dB,30 s,acoustic transducers,acoustic variables measurement,anechoic chamber,anechoic chambers,auditory mechanisms,Azimuth,Band pass filters,band-pass filters,biological systems,Biological systems,biomimetic system,Biomimetics,critical frequency,Digital signal processing,digital signal processor,Digital signal processors,equilateral triangle,hearing,histograms,microcomputer applications,Microcomputers,microphones,Microphones,multiple sound sources,noise,omni-directional microphones,personal computer,radio talk show,radio weather forecast,Signal design,signal processing equipment,speech analysis and processing,speech recognition,spurious disparities,temporal disparities,Weather forecasting}
}

@article{huangSnapshotEnsemblesTrain2017,
  title = {Snapshot {{Ensembles}}: {{Train}} 1, Get {{M}} for Free},
  shorttitle = {Snapshot {{Ensembles}}},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  date = {2017-03-31},
  url = {http://arxiv.org/abs/1704.00109},
  urldate = {2019-12-09},
  abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
  archivePrefix = {arXiv},
  eprint = {1704.00109},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6VLP8TSM\\Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf},
  keywords = {Computer Science - Machine Learning},
  langid = {english},
  primaryClass = {cs}
}

@article{huangTwoStepSphericalHarmonics2018,
  title = {Two-{{Step Spherical Harmonics ESPRIT}}-{{Type Algorithms}} and {{Performance Analysis}}},
  author = {Huang, Qinghua and Zhang, Lin and Fang, Yong},
  date = {2018-09},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1684-1697\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2836436\}},
  abstract = {Spherical arrays have been widely used in direction-of-arrival (DOA) estimation in recent years, and the high-resolution estimation of signal parameter via rotational invariance technique (ESPRIT) was developed in the spherical harmonics domain. However, the spherical harmonics ESPRIT (SHESPRIT) cannot estimate the DOA when the elevation approaches 90 degrees. To solve this problem, we present a two-step SHESPRIT (TS-SHESPRIT) based on two new recurrence relations of complex spherical harmonics. Furthermore, we develop a real-valued two-step SHESPRIT (RTS-SHESPRIT) that exploits a unitary matrix to obtain a real-valued relation between the signal subspace and the steering matrix to further reduce the computational complexity. However, the number of sources that are estimated by RTS-SHESPRIT is limited. Therefore, we propose the semi-RTS-SHESPRIT method, which reduces the computational complexity associated with eigenvalue decomposition (EVD) and avoids the limitations of RTS-SHESPRIT. Relative to SHESPRIT and TS-SHESPRIT, RTS-SHESPRIT and semi-RTS-SHESPRIT reduce the computational burden by 75\% during EVD. Furthermore, we derive the mean square errors (MSEs) of the above algorithms and significantly simplify the MSE expressions. Different expressions for the MSEs are due to different recurrence relations used by different SHESPRIT-type algorithms. All proposed two-step SHESPRIT-type algorithms have higher accuracy than traditional SHESPRIT. The simulation results demonstrate the satisfactory performance of our methods.},
  affiliation = {Huang, QH (Reprint Author), Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200072, Peoples R China. Huang, Qinghua; Zhang, Lin; Fang, Yong, Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200072, Peoples R China.},
  author-email = {qinghua@shu.edu.cn sherlin0911@163.com yfang@shu.edu.cn},
  cited-references = {ABHAYAPALA TD, 2002, ACOUST SPEECH SIG PR, P1949. Barabell A. J., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P336. Boukerche A, 2007, IEEE WIREL COMMUN, V14, P6, DOI 10.1109/MWC.2007.4407221. Choi CH, 1999, J CHEM PHYS, V111, P8825, DOI 10.1063/1.480229. Dai JS, 2013, IEEE ANTENN WIREL PR, V12, P376, DOI 10.1109/LAWP.2013.2252415. Dalenback B., CATT ACOUSTIC. DEGROAT RD, 1992, CONFERENCE RECORD OF THE TWENTY-SIXTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P581, DOI 10.1109/ACSSC.1992.269205. Demmel J. W., 1990, SIAM J MATRIX ANAL A, V14, P1. Fisher E, 2011, IEEE T AUDIO SPEECH, V19, P256, DOI 10.1109/TASL.2010.2047421. FU L, 1990, IEEE T AERO ELEC SYS, V26, P976, DOI 10.1109/7.62249. Garofolo J. S., 1993, DAPRA TIMIT ACOUSTIC. Gershman AB, 1998, IEEE T SIGNAL PROCES, V46, P1351, DOI 10.1109/78.668797. Golub G. H., 1996, MATRIX COMPUTATIONS. Goossens R, 2009, IET SIGNAL PROCESS, V3, P221, DOI 10.1049/iet-spr.2008.0101. Goossens R, 2007, IEEE T ANTENN PROPAG, V55, P841, DOI 10.1109/TAP.2007.891848. HAARDT M, 1995, IEEE T SIGNAL PROCES, V43, P1232, DOI 10.1109/78.382406. Huang QH, 2017, IEEE-ACM T AUDIO SPE, V25, P2045, DOI 10.1109/TASLP.2017.2737235. Huang QH, 2017, SIGNAL PROCESS, V131, P441, DOI 10.1016/j.sigpro.2016.09.002. HUARNG KC, 1991, IEEE T SIGNAL PROCES, V39, P975, DOI 10.1109/78.80927. Ivanic J, 1996, J PHYS CHEM-US, V100, P6342, DOI 10.1021/jp953350u. Jiang J., 2012, Progress In Electromagnetics Research C, V29, P219. KAVEH M, 1986, IEEE T ACOUST SPEECH, V34, P331, DOI 10.1109/TASSP.1986.1164815. KNIGHT WC, 1981, P IEEE, V69, P1451, DOI 10.1109/PROC.1981.12186. Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899. Kumar L, 2016, INT CONF ACOUST SPEE, P3046, DOI 10.1109/ICASSP.2016.7472237. Kumar L, 2015, IEEE SIGNAL PROC LET, V22, P1030, DOI 10.1109/LSP.2014.2381361. Lancaster P., 1985, THEORY MATRICES. Leong PH, 2014, IEEE T SIGNAL PROCES, V62, P6003, DOI 10.1109/TSP.2014.2360155. LI F, 1993, IEEE T AERO ELEC SYS, V29, P1170, DOI 10.1109/7.259520. Li X, 2011, APPL ACOUST, V72, P646, DOI 10.1016/j.apacoust.2011.02.010. LINEBARGER DA, 1994, IEEE T SIGNAL PROCES, V42, P2136, DOI 10.1109/78.301848. LONGSTAFF ID, 1967, P I ELECTR ENG, V114, P713, DOI 10.1049/piee.1967.0142. MATHEWS CP, 1994, IEEE T SIGNAL PROCES, V42, P2395, DOI 10.1109/78.317861. Nadiri O, 2014, IEEE-ACM T AUDIO SPE, V22, P1494, DOI 10.1109/TASLP.2014.2337846. Pesavento M, 2000, IEEE T SIGNAL PROCES, V48, P1306, DOI 10.1109/78.839978. Priyadarshini MP, 2012, 2012 INTERNATIONAL CONFERENCE ON RADAR, COMMUNICATION AND COMPUTING (ICRCC), P120, DOI 10.1109/ICRCC.2012.6450560. Rafaely B, 2005, IEEE T SPEECH AUDI P, V13, P135, DOI 10.1109/TSA.2004.839244. Rafaely B, 2004, J ACOUST SOC AM, V116, P2149, DOI 10.1121/1.1792643. Rafaely B, 2008, IEEE T AUDIO SPEECH, V16, P740, DOI 10.1109/TASL.2008.920059. RAO BD, 1989, IEEE T ACOUST SPEECH, V37, P1939, DOI 10.1109/29.45540. RAO BD, 1989, IEEE T ACOUST SPEECH, V37, P1990, DOI 10.1109/29.45548. Roy R, 1986, 30 ANN TECHN S INT S, V3, P94. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. SEIDMAN LP, 1971, IEEE T ACOUST SPEECH, VAU19, P147, DOI 10.1109/TAU.1971.1162169. Sun HH, 2011, INT CONF ACOUST SPEE, P117. Tao Song, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P2418, DOI 10.1109/CISP.2011.6100767. Teutsch H., 2005, P JOINT WORKSH HANDS, pc. Watson G. N., 1927, COURSE MODERN ANAL.},
  da = {2018-10-18},
  doc-delivery-number = {GK8FP},
  funding-acknowledgement = {National Natural Science Foundation of China [61571279]; Shanghai Science and Technology Commission Scientific Research Project of China [16010500100]},
  funding-text = {This work was supported in part by the National Natural Science Foundation of China (61571279) and in part by the Shanghai Science and Technology Commission Scientific Research Project (16010500100) of China. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Huseyin Hacihabiboglu. (Corresponding author: Qinghua Huang.)},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Direction-of-arrival (DOA) estimation,estimation of signal parameter  via rotational invariance technique (ESPRIT),mean square error (MSE),spherical array),two-step method,unitary  transformation},
  keywords-plus = {OF-ARRIVAL ESTIMATION; UNIFORM CIRCULAR ARRAYS; 2-D ANGLE ESTIMATION; MICROPHONE ARRAY; ROTATION MATRICES; DOA ESTIMATION; PLANE-WAVES; SOUND FIELD; ROOT-MUSIC; LOCALIZATION},
  langid = {english},
  number = {9},
  number-of-cited-references = {48},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000436454100006},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@inproceedings{huEnergyBasedCollaborative2002,
  title = {Energy Based Collaborative Source Localization Using Acoustic Micro-Sensor Array},
  booktitle = {2002 {{IEEE Workshop}} on {{Multimedia Signal Processing}}.},
  author = {Hu, Yu Hen and Li, Dan},
  date = {2002-12},
  pages = {371--375},
  doi = {10.1109/MMSP.2002.1203323},
  abstract = {A novel sensor network source localization method based on acoustic energy measurements is presented. This method makes use of the characteristics that the acoustic energy decays exponentially with respect to the distance from an omni-directional acoustic source. By comparing energy readings measured at surrounding acoustic sensors during the same time interval can be accurately estimated. We show that the potential target location is restricted to a hyper-sphere in the sensor field given the acoustic energy reading at a pair of sensors. Given multiple sensor acoustic energy readings, the target location is solved as the location that is closest (in the least square sense) to all the corresponding hyper-spheres. We further simplified this nonlinear least square problem to an unconstrained linear least square problem that yields a closed form solution. Experiment results using military vehicle acoustic data show great promise of this novel approach.},
  eventtitle = {2002 {{IEEE Workshop}} on {{Multimedia Signal Processing}}.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2REIVNSF\\Hu and Li - 2002 - Energy based collaborative source localization usi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BIWP46RH\\1203323.html},
  keywords = {acoustic arrays,acoustic energy measurement,Acoustic measurements,acoustic microsensor array,Acoustic sensors,acoustic signal processing,array signal processing,closed form solution,Closed-form solution,Collaboration,collaborative signal processing,distributed sensors,energy based collaborative source localization,Energy measurement,hyper-sphere,least squares approximations,Least squares methods,microsensors,military vehicle acoustic data,nonlinear least square problem,novel sensor network source localization method,omnidirectional acoustic source,Sensor phenomena and characterization,target localization,Time measurement,Vehicles}
}

@inproceedings{HuEnergybasedcollaborative2002,
  title = {Energy {{Based Collaborative Source Localization Using Acoustic Micro}}-{{Sensor Array}}},
  booktitle = {2002 {{IEEE Workshop}} on {{Multimedia Signal Processing}}.},
  author = {Hu, Yu Hen and Li, Dan},
  date = {2002-12},
  pages = {371--375},
  doi = {10.1109/MMSP.2002.1203323},
  abstract = {A novel sensor network source localization method based on acoustic energy measurements is presented. This method makes use of the characteristics that the acoustic energy decays exponentially with respect to the distance from an omni-directional acoustic source. By comparing energy readings measured at surrounding acoustic sensors during the same time interval can be accurately estimated. We show that the potential target location is restricted to a hyper-sphere in the sensor field given the acoustic energy reading at a pair of sensors. Given multiple sensor acoustic energy readings, the target location is solved as the location that is closest (in the least square sense) to all the corresponding hyper-spheres. We further simplified this nonlinear least square problem to an unconstrained linear least square problem that yields a closed form solution. Experiment results using military vehicle acoustic data show great promise of this novel approach.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7NR4K5F8\\Hu and Li - 2002 - Energy based collaborative source localization usi.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BVBE6SNF\\1203323.html},
  keywords = {acoustic arrays,acoustic energy measurement,Acoustic measurements,acoustic microsensor array,Acoustic sensors,acoustic signal processing,array signal processing,closed form solution,Closed-form solution,Collaboration,collaborative signal processing,distributed sensors,energy based collaborative source localization,Energy measurement,hyper-sphere,least squares approximations,Least squares methods,microsensors,military vehicle acoustic data,nonlinear least square problem,novel sensor network source localization method,omnidirectional acoustic source,Sensor phenomena and characterization,target localization,Time measurement,Vehicles}
}

@article{hurmalainenExemplarbasedRecognitionSpeech2011,
  title = {Exemplar-Based {{Recognition}} of {{Speech}} in {{Highly Variable Noise}}},
  author = {Hurmalainen, Antti and Mahkonen, Katariina and Gemmeke, Jort F and Virtanen, Tuomas},
  date = {2011-09},
  pages = {5},
  abstract = {Robustness against varying background noise is a crucial requirement for the use of automatic speech recognition in everyday situations. In previous work, we proposed an exemplarbased recognition system for tackling the issue at low SNRs. In this work, we compare several exemplar-based factorisation and decoding algorithms in pursuit of higher noise robustness. The algorithms are evaluated using the PASCAL CHiME challenge corpus, which contains multiple speakers and authentic living room noise at six SNRs ranging from 9 to -6 dB. The results show that the proposed exemplar-based techniques offer a substantial improvement in the noise robustness of speech recognition.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JXLDQTA6\\Hurmalainen et al. - Exemplar-based Recognition of Speech in Highly Var.pdf},
  langid = {english}
}

@software{huRuiqiHuARGA2019,
  title = {Ruiqi-{{Hu}}/{{ARGA}}},
  author = {Hu, Ruiqi},
  date = {2019-10-22T06:36:15Z},
  origdate = {2018-07-25T10:35:17Z},
  url = {https://github.com/Ruiqi-Hu/ARGA},
  urldate = {2019-10-24},
  abstract = {This is a TensorFlow implementation of the Adversarially Regularized Graph Autoencoder(ARGA) model as described in our paper:   Pan, S., Hu, R., Long, G., Jiang, J., Yao, L., \&amp; Zhang, C. (2018)...}
}

@article{HyeonwooNoh,
  title = {Hyeonwoo {{Noh}}},
  url = {http://cvlab.postech.ac.kr/~hyeonwoonoh/},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YRZU5XRD\\~hyeonwoonoh.html}
}

@online{HyeonwooNoha,
  title = {Hyeonwoo {{Noh}}},
  url = {http://cvlab.postech.ac.kr/~hyeonwoonoh/},
  urldate = {2017-02-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AVPCNAKM\\~hyeonwoonoh.html}
}

@online{IEEE802AV,
  title = {{{IEEE}} 802.1 {{AV Bridging Task Group}}},
  url = {http://ieee802.org/1/pages/avbridges.html},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VD7QCRZN\\avbridges.html}
}

@online{ImplementingTiedweightsAutoencoders,
  title = {Implementing Tied-Weights Autoencoders in {{Keras}}},
  url = {https://amiralavi.net/blog/2018/08/25/tied-autoencoders},
  urldate = {2019-10-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UKYIU8PY\\tied-autoencoders.html}
}

@inproceedings{imran_methodology_2016-2,
  title = {A Methodology for Sound Source Localization and Tracking: {{Development}} of {{3D}} Microphone Array for near-Field and Far-Field Applications},
  shorttitle = {A Methodology for Sound Source Localization and Tracking},
  booktitle = {2016 13th {{International Bhurban Conference}} on {{Applied Sciences}} and {{Technology}} ({{IBCAST}})},
  author = {Imran, M. and Hussain, A. and Qazi, N. M. and Sadiq, M.},
  date = {2016-01},
  pages = {586--591},
  doi = {10.1109/IBCAST.2016.7429936},
  abstract = {Acoustic source localization and tracking using microphone arrays has become a focus of interest in room acoustics, teleconference systems and tracking of sound producing objects. The current methods to estimate the source localization depend on conventional time-delay estimation techniques between microphone pairs, however, ignoring the ambient noise, reflections from surrounding and reverberation in the closed space. In this study, an acoustic source localizer and tracker (ASLT) based on 3D microphone array is designed and developed for real time source detection and localization. Two practical approaches were examined and evaluated, based on direction of arrival (DOA) estimation techniques and steered power response (SPR) of array algorithms in order to improve the accuracy for tracking multiple sources in full 3D coordinates. Among time delay estimation techniques, generalized cross correlation (GCC) is employed in frequency domain using multiple combinations of microphone pairs of the array with Phase transform (PHAT) weighting function for optimum detection of sources in the presence of reverberant environments. PHAT gives a good performance in the presence of ambience, even when the signal to noise ratio (SNR) is low. For SPR, minimum variance distortion less response (MVDR) beamformer weights are evaluated and purposed for accurate source tracking applications. A microphone array is designed using six transducers in spherical configurations and used for the evaluated for proposed methodology. The measurements are carried out in a reverberant chamber under different noise conditions to validate the practicality of the algorithmic chain and finally, the results are obtained and presented to demonstrate the efficiency of the proposed microphone array design and localization technique.},
  eventtitle = {2016 13th {{International Bhurban Conference}} on {{Applied Sciences}} and {{Technology}} ({{IBCAST}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AN8DGSNV\\Imran et al. - 2016 - A methodology for sound source localization and tr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\B6T68Q7X\\Imran et al. - 2016 - A methodology for sound source localization and tr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NNW83IT2\\7429936.html},
  keywords = {3D microphone array design,accurate source tracking applications,acoustic signal detection,acoustic source localization,acoustic source localizer and tracker,Acoustics,ambient noise,architectural acoustics,array signal processing,ASLT,Correlation,Delay effects,delays,Direction of arrival (DOA),direction of arrival estimation techniques,direction-of-arrival estimation,DOA estimation techniques,Estimation,estimation theory,far-field applications,frequency domain,GCC,generalized cross correlation,Generalized cross correlation (GCC),microphone arrays,microphone pairs,Minimum variance distortion less response (MVDR),minimum variance distortion less response beamformer,MVDR beamformer,near-field applications,object tracking,Phase transform (PHAT),phase transform weighting function,PHAT weighting function,real time source detection,real time source localization,reverberant chamber,reverberation chambers,room acoustics,signal to noise ratio,SNR,sound producing objects tracking,sound source localization,sound source tracking,Spherical microphone array,SPR,steered power response,teleconference systems,time delay estimation techniques,time-delay estimation techniques,tracking multiple sources accuracy}
}

@inproceedings{imranMethodologySoundSource2016,
  title = {A Methodology for Sound Source Localization and Tracking: {{Development}} of {{3D}} Microphone Array for near-Field and Far-Field Applications},
  shorttitle = {A Methodology for Sound Source Localization and Tracking},
  booktitle = {2016 13th {{International Bhurban Conference}} on {{Applied Sciences}} and {{Technology}} ({{IBCAST}})},
  author = {Imran, M. and Hussain, A. and Qazi, N. M. and Sadiq, M.},
  date = {2016-01},
  pages = {586--591},
  doi = {10.1109/IBCAST.2016.7429936},
  abstract = {Acoustic source localization and tracking using microphone arrays has become a focus of interest in room acoustics, teleconference systems and tracking of sound producing objects. The current methods to estimate the source localization depend on conventional time-delay estimation techniques between microphone pairs, however, ignoring the ambient noise, reflections from surrounding and reverberation in the closed space. In this study, an acoustic source localizer and tracker (ASLT) based on 3D microphone array is designed and developed for real time source detection and localization. Two practical approaches were examined and evaluated, based on direction of arrival (DOA) estimation techniques and steered power response (SPR) of array algorithms in order to improve the accuracy for tracking multiple sources in full 3D coordinates. Among time delay estimation techniques, generalized cross correlation (GCC) is employed in frequency domain using multiple combinations of microphone pairs of the array with Phase transform (PHAT) weighting function for optimum detection of sources in the presence of reverberant environments. PHAT gives a good performance in the presence of ambience, even when the signal to noise ratio (SNR) is low. For SPR, minimum variance distortion less response (MVDR) beamformer weights are evaluated and purposed for accurate source tracking applications. A microphone array is designed using six transducers in spherical configurations and used for the evaluated for proposed methodology. The measurements are carried out in a reverberant chamber under different noise conditions to validate the practicality of the algorithmic chain and finally, the results are obtained and presented to demonstrate the efficiency of the proposed microphone array design and localization technique.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QED4T3WV\\7429936.html},
  keywords = {3D microphone array design,accurate source tracking applications,acoustic signal detection,acoustic source localization,acoustic source localizer and tracker,Acoustics,ambient noise,architectural acoustics,array signal processing,ASLT,Correlation,Delay effects,delays,Direction of arrival (DOA),direction of arrival estimation techniques,direction-of-arrival estimation,DOA estimation techniques,Estimation,estimation theory,far-field applications,frequency domain,GCC,generalized cross correlation,Generalized cross correlation (GCC),microphone arrays,microphone pairs,Minimum variance distortion less response (MVDR),minimum variance distortion less response beamformer,MVDR beamformer,near-field applications,object tracking,Phase transform (PHAT),phase transform weighting function,PHAT weighting function,real time source detection,real time source localization,reverberant chamber,reverberation chambers,room acoustics,signal to noise ratio,SNR,sound producing objects tracking,sound source localization,sound source tracking,Spherical microphone array,SPR,steered power response,teleconference systems,time delay estimation techniques,time-delay estimation techniques,tracking multiple sources accuracy}
}

@inproceedings{imranMethodologySoundSource2016a,
  title = {A Methodology for Sound Source Localization and Tracking: {{Development}} of {{3D}} Microphone Array for near-Field and Far-Field Applications},
  shorttitle = {A Methodology for Sound Source Localization and Tracking},
  booktitle = {2016 13th {{International Bhurban Conference}} on {{Applied Sciences}} and {{Technology}} ({{IBCAST}})},
  author = {Imran, M. and Hussain, A. and Qazi, N. M. and Sadiq, M.},
  date = {2016-01},
  pages = {586--591},
  doi = {10.1109/IBCAST.2016.7429936},
  abstract = {Acoustic source localization and tracking using microphone arrays has become a focus of interest in room acoustics, teleconference systems and tracking of sound producing objects. The current methods to estimate the source localization depend on conventional time-delay estimation techniques between microphone pairs, however, ignoring the ambient noise, reflections from surrounding and reverberation in the closed space. In this study, an acoustic source localizer and tracker (ASLT) based on 3D microphone array is designed and developed for real time source detection and localization. Two practical approaches were examined and evaluated, based on direction of arrival (DOA) estimation techniques and steered power response (SPR) of array algorithms in order to improve the accuracy for tracking multiple sources in full 3D coordinates. Among time delay estimation techniques, generalized cross correlation (GCC) is employed in frequency domain using multiple combinations of microphone pairs of the array with Phase transform (PHAT) weighting function for optimum detection of sources in the presence of reverberant environments. PHAT gives a good performance in the presence of ambience, even when the signal to noise ratio (SNR) is low. For SPR, minimum variance distortion less response (MVDR) beamformer weights are evaluated and purposed for accurate source tracking applications. A microphone array is designed using six transducers in spherical configurations and used for the evaluated for proposed methodology. The measurements are carried out in a reverberant chamber under different noise conditions to validate the practicality of the algorithmic chain and finally, the results are obtained and presented to demonstrate the efficiency of the proposed microphone array design and localization technique.},
  eventtitle = {2016 13th {{International Bhurban Conference}} on {{Applied Sciences}} and {{Technology}} ({{IBCAST}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TANQ4P7C\\7429936.html},
  keywords = {3D microphone array design,accurate source tracking applications,acoustic signal detection,acoustic source localization,acoustic source localizer and tracker,Acoustics,ambient noise,architectural acoustics,array signal processing,ASLT,Correlation,Delay effects,delays,Direction of arrival (DOA),direction of arrival estimation techniques,direction-of-arrival estimation,DOA estimation techniques,Estimation,estimation theory,far-field applications,frequency domain,GCC,generalized cross correlation,Generalized cross correlation (GCC),microphone arrays,microphone pairs,Minimum variance distortion less response (MVDR),minimum variance distortion less response beamformer,MVDR beamformer,near-field applications,object tracking,Phase transform (PHAT),phase transform weighting function,PHAT weighting function,real time source detection,real time source localization,reverberant chamber,reverberation chambers,room acoustics,signal to noise ratio,SNR,sound producing objects tracking,sound source localization,sound source tracking,Spherical microphone array,SPR,steered power response,teleconference systems,time delay estimation techniques,time-delay estimation techniques,tracking multiple sources accuracy}
}

@online{InformationEntropybasedViewpoint,
  title = {Information Entropy-Based Viewpoint Planning for 3-{{D}} Object Reconstruction - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/document/1435477},
  urldate = {2020-01-27},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7DVMN6WY\\1435477.html}
}

@online{InnovativeOptoelectronicAcoustic,
  title = {Innovative Optoelectronic and Acoustic Sensing Technologies for Large Scale Forest Fire Long Term Monitoring | {{EU}}-{{FIRE Project}} | {{FP6}} | {{CORDIS}} | {{European Commission}}},
  url = {https://cordis.europa.eu/project/rcn/79404/factsheet/en},
  urldate = {2019-07-29},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5SKDS86H\\en.html}
}

@inreference{InterauralTimeDifference2017,
  title = {Interaural Time Difference},
  booktitle = {Wikipedia},
  date = {2017-12-19T01:28:45Z},
  url = {https://en.wikipedia.org/w/index.php?title=Interaural_time_difference&oldid=816067747},
  urldate = {2018-02-19},
  abstract = {The interaural time difference (or ITD) when concerning humans or animals, is the difference in arrival time of a sound between two ears. It is important in the localization of sounds, as it provides a cue to the direction or angle of the sound source from the head. If a signal arrives at the head from one side, the signal has further to travel to reach the far ear than the near ear. This pathlength difference results in a time difference between the sound's arrivals at the ears, which is detected and aids the process of identifying the direction of sound source.
When a signal is produced in the horizontal plane, its angle in relation to the head is referred to as its azimuth, with 0 degrees (0°) azimuth being directly in front of the listener, 90° to the right, and 180° being directly behind.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N4AZUS7B\\index.html},
  langid = {english}
}

@online{IntroductionLivewireTA15,
  title = {Introduction to {{Livewire}}+ -{{TA15}} 19013-{{Web}}.Pdf},
  url = {https://www.telosalliance.com/images/Axia%20Products/Support%20Documents/Tech%20Tips/Introduction%20to%20Livewire+%20-TA15%2019013-Web.pdf},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RTIR9VEB\\Introduction to Livewire+ -TA15 19013-Web.html}
}

@online{IntroOptimizationDeep2018,
  title = {Intro to Optimization in Deep Learning: {{Momentum}}, {{RMSProp}} and {{Adam}}},
  shorttitle = {Intro to Optimization in Deep Learning},
  date = {2018-06-13T14:30:45.000Z},
  journaltitle = {Paperspace Blog},
  url = {https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/},
  urldate = {2019-12-09},
  abstract = {In this post, we take a look at a problem that plagues training of neural networks, pathological curvature.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B5Y82XCV\\intro-to-optimization-momentum-rmsprop-adam.html},
  langid = {english}
}

@online{Iros09Pdf,
  title = {Iros09.Pdf},
  url = {http://homepages.laas.fr/danes/RESEARCH/RA/iros09.pdf},
  urldate = {2017-01-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KUF2PAQB\\iros09.html}
}

@online{ISDN,
  title = {{{ISDN}}},
  url = {http://hea-www.harvard.edu/~fine/ISDN/},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\F2QPR396\\ISDN.html}
}

@article{jeanSemisupervisedDeepKernel,
  title = {Semi-Supervised {{Deep Kernel Learning}}: {{Regression}} with {{Unlabeled Data}} by {{Minimizing Predictive Variance}}},
  author = {Jean, Neal and Xie, Sang Michael and Ermon, Stefano},
  pages = {12},
  abstract = {Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Y73BJQAQ\\Jean et al. - Semi-supervised Deep Kernel Learning Regression w.pdf},
  langid = {english}
}

@article{jiaMultipletosingleSoundSource2018,
  title = {Multiple-to-Single Sound Source Localization by Applying Single-Source Bins Detection},
  author = {Jia, Maoshen and Sun, Jundai and Bao, Changchun and Ritz, Christian},
  date = {2018-09},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {138},
  pages = {\{28-38\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2018.03.015\}},
  abstract = {This paper proposes a novel localization scheme for multiple sound sources that imposes the relaxed sparsity constrains (not all time-frequency coefficients are overlapped) on the source signals. First, a “DOA convergence” assumption is proposed, which means that if most of the time-frequency (T-F) bins in a T-F zone are derived from only one source- defined as single source bins (SSBs), the corresponding direction of arrival (DOA) estimates are relatively concentrated with a heavy density. This assumption is validated through statistical analysis by applying a quantitative measure of convergence. Accordingly, by applying the “DOA convergence” assumption, the detection of SSBs is converted to a clustering problem, K-means clustering and density-based spatial clustering of applications with noise (DBSCAN) algorithms are utilized to complete the task in this paper. The cross distortions (localization error due to the cocktail party phenomenon) in localization caused by multiple simultaneously occurring sources is significantly weakened by conducting DOA estimation among these SSBs, i.e., the multiple source localization is rewritten to a single source one among these SSBs. Moreover, the proposed SSBs detection is applicable to other localization methods and not limited to specific microphone topology. Experimental results demonstrate the localization accuracy of the proposed method outperforms the state-of-the- art localization approaches which are based on single source zone detection. However the proposed method is capable of realtime processing, the accuracy is insufficient in the current system. If non-real time processing is allowed, our method can be realized with higher accuracy than the conventional ones.},
  affiliation = {Jia, MS (Reprint Author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China. Jia, Maoshen; Sun, Jundai; Bao, Changchun, Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China. Ritz, Christian, Univ Wollongong, ICT Res Inst, Wollongong, NSW 2500, Australia. Ritz, Christian, Univ Wollongong, Sch Elect Comp & Telecommun Engn, Wollongong, NSW 2500, Australia.},
  author-email = {jiamaoshen@bjut.edu.cn},
  cited-references = {Benesty J, 2004, IEEE T SPEECH AUDI P, V12, P509, DOI 10.1109/TSA.2004.833008. Campbell D. R., 2005, Computing and Information Systems, V9, P48. Chen XY, 2015, SPEECH COMMUN, V68, P41, DOI 10.1016/j.specom.2015.01.002. Dmochowski J. P., 2007, IEEE WORKSH APPL SIG, P18. Hornstein J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P1170, DOI 10.1109/IROS.2006.281849. Huang Y, 2000, 2000 IEEE INT C AC S, V2, P11909, DOI [10.1109/ICASSP.2000.859108, DOI 10.1109/ICASSP.2000.859108]. Hyvrinen A., 2001, INDEPENDENT COMPONEN. Ishi CT, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2027, DOI 10.1109/IROS.2009.5354309. Jia MS, 2017, J AMB INTEL HUM COMP, V8, P829, DOI 10.1007/s12652-016-0388-x. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. Nesta F, 2012, IEEE T AUDIO SPEECH, V20, P246, DOI 10.1109/TASL.2011.2160168. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. Shujau M, 2010, INT CONF ACOUST SPEE, P137, DOI 10.1109/ICASSP.2010.5496124. Yao K, 2002, INT CONF ACOUST SPEE, P2949. Yu Y, 2016, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-016-0085-x. Zheng XG, 2013, IEEE SIGNAL PROC LET, V20, P83, DOI 10.1109/LSP.2012.2229977.},
  da = {2018-10-18},
  doc-delivery-number = {GH2ZB},
  eissn = {1872-910X},
  funding-acknowledgement = {China Postdoctoral Science Foundation [2017M610731]; Beijing Postdoctoral Research Foundation; “Ri xin” Training Programme Foundation for the Talents by Beijing University of Technology},
  funding-text = {This work has been supported by China Postdoctoral Science Foundation (2017M610731), the Project supported by Beijing Postdoctoral Research Foundation and “Ri xin” Training Programme Foundation for the Talents by Beijing University of Technology.},
  journal-iso = {Appl. Acoust.},
  keywords = {(Source localization,B-format microphone),Sparsity},
  keywords-plus = {TIME-FREQUENCY MASKING; SOURCE SEPARATION; LOCATION; DELAY},
  langid = {english},
  number-of-cited-references = {18},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000433269400004},
  usage-count-last-180-days = {11},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Acoustics}
}

@article{jiaSeparationMultipleSpeech2018,
  title = {Separation of Multiple Speech Sources by Recovering Sparse and Non-Sparse Components from {{B}}-Format Microphone Recordings},
  author = {Jia, Maoshen and Sun, Jundai and Bao, Changchun and Ritz, Christian},
  date = {2018-02},
  journaltitle = {SPEECH COMMUNICATION},
  volume = {96},
  pages = {\{184-196\}},
  publisher = {ELSEVIER SCIENCE BV},
  location = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
  issn = {0167-6393},
  doi = {\{10.1016/j.specom.2017.12.010\}},
  abstract = {This paper proposes a blind source separation (BSS) method for recovering multiple speech sources from sound fields recorded by a B-format microphone. This microphone provides a four channel representation that can be used to derive the direction of arrival (DOA) of spatially distinct time-frequency (TF) components. Such sparse components correspond to bins where only one speech source is active and are identified based on the inter correlation among the mixture signals. They are recovered via a degenerate unmixing estimation technique (DUET)-like method. Proposed is a “local-zone stationarity” assumption, where the amplitude of a speech signal remains approximately constant within a small band of TF components. This assumption is validated through statistical analysis of a quantitative measure of stationarity. Under this assumption, the non-sparse components (TF points where more than one speech source is active) are recovered via a Wiener-filter-like approach where the separated sparse components is utilized as a guide. The final separated sources are obtained by combining the separated sparse and non-sparse components. Both objective and subjective evaluations show that the proposed method achieves better separation quality compared to some existing BSS approaches where up to six simultaneous speech sources are considered.},
  affiliation = {Jia, MS (Reprint Author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China. Jia, Maoshen; Sun, Jundai; Bao, Changchun, Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China. Ritz, Christian, Univ Wollongong, ICT Res Inst, Wollongong, NSW 2500, Australia. Ritz, Christian, Univ Wollongong, Sch Elect Comp & Telecommun Engn, Wollongong, NSW 2500, Australia.},
  author-email = {jiamaoshen@bjut.edu.cn},
  cited-references = {Abrard F, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 2, PROCEEDINGS, P81, DOI 10.1109/ISSPA.2003.1224820. Abrard F, 2005, SIGNAL PROCESS, V85, P1389, DOI 10.1016/j.sigpro.2005.02.010. [Anonymous], 2005, WIDEBAND EXTENSION R. [Anonymous], 1997, 1534 BS ITR. Araki S, 2003, IEEE T SPEECH AUDI P, V11, P109, DOI 10.1109/TSA.2003.809193. Asaei A, 2016, IEEE T SIGNAL PROCES, V64, P567, DOI 10.1109/TSP.2015.2488598. Benesty J, 2004, IEEE T SPEECH AUDI P, V12, P509, DOI 10.1109/TSA.2004.833008. Campbell D. R., 2005, Computing and Information Systems, V9, P48. Chen JD, 2003, IEEE T SPEECH AUDI P, V11, P549, DOI 10.1109/TSA.2003.818025. Chen XY, 2015, SPEECH COMMUN, V68, P41, DOI 10.1016/j.specom.2015.01.002. Douglas SC, 2007, IEEE T AUDIO SPEECH, V15, P1511, DOI 10.1109/TASL.2007.899176. Gunel B, 2009, INT CONF ACOUST SPEE, P41, DOI 10.1109/ICASSP.2009.4959515. Gunel B, 2008, IEEE T AUDIO SPEECH, V16, P748, DOI 10.1109/TASL.2008.918967. Hyvrinen A., 2001, INDEPENDENT COMPONEN. Jaureguiberry X, 2016, IEEE-ACM T AUDIO SPE, V24, P1266, DOI 10.1109/TASLP.2016.2553441. Jia M., 2016, J AMB INTEL HUM COMP, P1. Jia MS, 2015, IEEE-ACM T AUDIO SPE, V23, P1082, DOI 10.1109/TASLP.2015.2419980. Jourjine A, 2000, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.2000.861162. Liutkus A, 2015, INT CONF ACOUST SPEE, P266, DOI 10.1109/ICASSP.2015.7177973. Liutkus A, 2012, SIGNAL PROCESS, V92, P1937, DOI 10.1016/j.sigpro.2011.09.016. Loesch B., 2008, INT WORKSH AC ECH NO. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. Nesta F, 2013, INT CONF ACOUST SPEE, P86, DOI 10.1109/ICASSP.2013.6637614. Parra LC, 2002, IEEE T SPEECH AUDI P, V10, P352, DOI 10.1109/TSA.2002.803443. Pereira N. C, 1985, AM MATH MONTH, V92, P139. Pulkki V, 2007, J AUDIO ENG SOC, V55, P503. Reju VG, 2009, SIGNAL PROCESS, V89, P1762, DOI 10.1016/j.sigpro.2009.03.017. Schlkopf B., 2007, EM ALGORITHM LOCALIZ, P953. Shujau M., 2011, IEEE 13 INT WORKSH M, P1. Thiagarajan JJ, 2013, DIGIT SIGNAL PROCESS, V23, P9, DOI 10.1016/j.dsp.2012.08.002. Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005. Wang D., 2017, J FRANKLIN I. Wang L, 2011, IEEE T AUDIO SPEECH, V19, P549, DOI 10.1109/TASL.2010.2052244. Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.V8896. Yu Y, 2016, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-016-0085-x. Zheng XG, 2016, MULTIMED TOOLS APPL, V75, P5183, DOI 10.1007/s11042-015-2989-3. Zheng XG, 2013, IEEE SIGNAL PROC LET, V20, P83, DOI 10.1109/LSP.2012.2229977. Zibulevsky M, 2001, NEURAL COMPUT, V13, P863, DOI 10.1162/089976601300014385.},
  da = {2018-10-18},
  doc-delivery-number = {FV6WV},
  eissn = {1872-7182},
  funding-acknowledgement = {China Postdoctoral Science Foundation [2017M610731]; National Natural Science Foundation of China [61471014, 61231015]; Beijing Postdoctoral Research Foundation; Ri xin Training Programme Foundation for the Talents by Beijing University of Technology},
  funding-text = {This work was supported by the China Postdoctoral Science Foundation Funded Project (2017M610731), the National Natural Science Foundation of China under Grants (Nos. 61471014, 61231015), the Project supported by Beijing Postdoctoral Research Foundation and the Ri xin Training Programme Foundation for the Talents by Beijing University of Technology.},
  journal-iso = {Speech Commun.},
  keywords = {(Multiple speech source separation,B-format microphone),Sparsity},
  keywords-plus = {BLIND SOURCE SEPARATION; TIME-FREQUENCY MASKING; MIXING MATRIX ESTIMATION; AUDIO SOURCE SEPARATION; CONVOLUTIVE MIXTURES; DELAY ESTIMATION; LOCALIZATION},
  langid = {english},
  number-of-cited-references = {38},
  research-areas = {Acoustics; Computer Science},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000424723700016},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Acoustics; Computer Science, Interdisciplinary Applications}
}

@article{joDirectionArrivalEstimation2018,
  title = {Direction of Arrival Estimation Using Nonsingular Spherical {{ESPRIT}}},
  author = {Jo, Byeongho and Choi, Jung-Woo},
  date = {2018-03},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{EL181-EL187\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5026122\}},
  abstract = {The eigenbeam-ESPRIT (EB-ESPRIT) is a parametric method that estimates the direction of arrival of source signals using a recurrence relation of spherical harmonics. In EB-ESPRIT, the sound-source elevation angle is estimated from an arctangent function, which diverges near the equator in the spherical coordinate system and inevitably induces an ill-conditioning problem. Here, a nonsingular spherical ESPRIT technique based on sine-based recurrence relations is proposed, in which the elevation angles are estimated from an arcsine function. It is shown that the proposed technique can estimate more sources than the conventional EB-ESPRIT technique while also avoiding ill-conditioning problems. (C) 2018 Acoustical Society of America},
  affiliation = {Choi, JW (Reprint Author), Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon 34141, South Korea. Jo, Byeongho; Choi, Jung-Woo, Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon 34141, South Korea.},
  author-email = {byeongho@kaist.ac.kr jwoo@kaist.ac.kr},
  cited-references = {Goossens R, 2009, IET SIGNAL PROCESS, V3, P221, DOI 10.1049/iet-spr.2008.0101. Gumerov N. A., 2015, FAST MULTIPOLE METHO. Hardin RH, 1996, DISCRETE COMPUT GEOM, V15, P429, DOI 10.1007/BF02711518. Huang QH, 2017, IEEE-ACM T AUDIO SPE, V25, P2045, DOI 10.1109/TASLP.2017.2737235. Jarrett D. P., 2017, THEORY APPL SPHERICA. Kumar L, 2015, IEEE SIGNAL PROC LET, V22, P1030, DOI 10.1109/LSP.2014.2381361. Li X, 2011, APPL ACOUST, V72, P646, DOI 10.1016/j.apacoust.2011.02.010. Rafaely B, 2005, IEEE T SPEECH AUDI P, V13, P135, DOI 10.1109/TSA.2004.839244. Rafaely B., 2015, FUNDAMENTALS SPHERIC. Rafaely B, 2008, IEEE SIGNAL PROC LET, V15, P417, DOI 10.1109/LSP.2008.922288. Rafaely B, 2010, SPRINGER TOP SIGN PR, V3, P281, DOI 10.1007/978-3-642-11130-3_11. Sun HH, 2011, INT CONF ACOUST SPEE, P117. Teutsch H, 2008, INT CONF ACOUST SPEE, P5276, DOI 10.1109/ICASSP.2008.4518850. Teutsch H., 2005, P JOINT WORKSH HANDS, pc.},
  da = {2018-10-18},
  doc-delivery-number = {FZ2JY},
  eissn = {1520-8524},
  funding-acknowledgement = {Agency for Defense Development [UC150002ID]; BK21 Plus program through the National Research Foundation (NRF) - Ministry of Education of Korea},
  funding-text = {This work was supported in part by a grant from the Agency for Defense Development under Contract No. UC150002ID and by the BK21 Plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea.},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {MICROPHONE ARRAYS; LOCALIZATION},
  langid = {english},
  number = {3},
  number-of-cited-references = {14},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000427405300007},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@online{julianiSimpleReinforcementLearning2016,
  title = {Simple {{Reinforcement Learning}} with {{Tensorflow Part}} 0: {{Q}}-{{Learning}} with {{Tables}} and {{Neural Networks}}},
  shorttitle = {Simple {{Reinforcement Learning}} with {{Tensorflow Part}} 0},
  author = {Juliani, Arthur},
  date = {2016-08-25T21:21:20.009Z},
  journaltitle = {Medium},
  url = {https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0},
  urldate = {2017-11-28},
  abstract = {For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms…},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RNJXZ89F\\simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks.html}
}

@article{kaltenbacherInverseSchemeAcoustic2018,
  title = {Inverse {{Scheme}} for {{Acoustic Source Localization}} Using {{Microphone Measurements}} and {{Finite Element Simulations}}},
  author = {Kaltenbacher, Manfred and Kaltenbacher, Barbara and Gombots, Stefan},
  date = {2018-07/2018-08},
  journaltitle = {ACTA ACUSTICA UNITED WITH ACUSTICA},
  volume = {104},
  pages = {\{647-656\}},
  publisher = {S HIRZEL VERLAG},
  location = {POSTFACH 10 10 61, D-70 009 STUTTGART, GERMANY},
  issn = {1610-1928},
  doi = {\{10.3813/AAA.919204\}},
  abstract = {We propose an inverse scheme for acoustic source localization based on solving the corresponding partial differential equation in the frequency domain (Helmholtz equation) by applying the Finite Element (FE) method. This allows us to fully take into account the actual boundary conditions as given in the measurement setup. To recover the source locations, an inverse scheme based on a sparsity promoting Tikhonov functional to match measured (microphone signals) and simulated pressure is proposed. The properties of this inverse scheme and its applicability to source localization in the low frequency range will be demonstrated. (C) 2018 The Author(s). Published by S. Hirzel Verlag . EAA.},
  affiliation = {Kaltenbacher, M (Reprint Author), TU Wien, Inst Mech & Mechatron, Vienna, Austria. Kaltenbacher, Manfred; Gombots, Stefan, TU Wien, Inst Mech & Mechatron, Vienna, Austria. Kaltenbacher, Barbara, Alpen Adria Univ Klagenfurt, Inst Appl Anal, Klagenfurt, Austria.},
  author-email = {manfred.kaltenbacher@tuwien.ac.at barbara.kaltenbacher@aau.at stefan.gombots@tuwien.ac.at},
  cited-references = {Amitt E, 2014, COMPUT MECH, V54, P443, DOI 10.1007/s00466-014-0996-2. Anzengruber SW, 2014, APPL ANAL, V93, P1382, DOI 10.1080/00036811.2013.833326. Bavu E, 2007, ACTA ACUST UNITED AC, V93, P706. Bavu E, 2009, ACTA ACUST UNITED AC, V95, P595, DOI 10.3813/AAA.918188. Brooks TF, 2006, J SOUND VIB, V294, P856, DOI 10.1016/j.jsv.2005.12.046. Christensen J. J., 2004, TECHNICAL REV BEAMFO, V34. Chu ZG, 2014, MECH SYST SIGNAL PR, V48, P404, DOI 10.1016/j.ymssp.2014.03.012. Dougherty RP, 2013, INT J AEROACOUST, V12, P699, DOI 10.1260/1475-472X.12.7-8.699. Fink M, 2003, ANNU REV BIOMED ENG, V5, P465, DOI 10.1146/annurev.bioeng.5.040202.121630. Fink M., 1989, P IEEE ULTR S, P681, DOI 10.1109/ULTSYM.1989.67072. Fink M, 1991, ULTRASONIC IMAGING, V13, P179. Fischer J, 2017, MECH SYST SIGNAL PR, V91, P10, DOI 10.1016/j.ymssp.2016.12.025. Johnson D. H., 1993, FINITE CONTINUOUS AP, P64. Kaltenbacher M., 2015, NUMERICAL SIMULATION, P201. Kim BK, 1996, J ACOUST SOC AM, V100, P3003, DOI 10.1121/1.417112. Kunisch K, 2016, SIAM J CONTROL OPTIM, V54, P1212, DOI 10.1137/141001366. Leclere Q, 2017, INT J AEROACOUST, V16, P431, DOI 10.1177/1475472X17718883. Lu S, 2011, NUMER MATH, V118, P1, DOI 10.1007/s00211-010-0318-3. Martinus F, 2010, NOISE CONTROL ENG J, V58, P83, DOI 10.3397/1.3224003. Mueller T. J., 2002, BEAMFORMING ACOUSTIC, P62. Mueller T. J., 2002, AEROACOUSTIC PHASED, P112. Nolte B, 2005, J COMPUT ACOUST, V13, P187, DOI 10.1142/S0218396X05002608. Padois T, 2017, ACTA ACUST UNITED AC, V103, P392, DOI 10.3813/AAA.919069. Padois T, 2012, J ACOUST SOC AM, V132, P2397, DOI 10.1121/1.4747015. Sarradj E, 2012, ADV ACOUS VIB, DOI 10.1155/2012/292695. Schuhmacher A, 2003, J ACOUST SOC AM, V113, P114, DOI 10.1121/1.1529668. Sijtsma Pieter, 2007, International Journal of Aeroacoustics, V6, P357, DOI 10.1260/147547207783359459. Song HC, 2006, IEEE J OCEANIC ENG, V31, P170, DOI 10.1109/JOE.2005.850911. Suzuki T, 2011, J SOUND VIB, V330, P5835, DOI 10.1016/j.jsv.2011.05.021. van Manen DJ, 2006, GEOPHYSICS, V71, pSI47, DOI 10.1190/1.2213218. Yardibi T, 2008, J ACOUST SOC AM, V123, P2631, DOI 10.1121/1.2896754.},
  da = {2018-10-18},
  doc-delivery-number = {GN9YM},
  eissn = {1861-9959},
  journal-iso = {Acta Acust. United Acust.},
  keywords-plus = {TIME-REVERSAL; SOUND SOURCE; DECONVOLUTION; RECONSTRUCTION; IDENTIFICATION; ARRAYS; FIELD; SINK},
  langid = {english},
  number = {4},
  number-of-cited-references = {31},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000439570100009},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Acoustics}
}

@article{kandasamyNeuralArchitectureSearch2019,
  title = {Neural {{Architecture Search}} with {{Bayesian Optimisation}} and {{Optimal Transport}}},
  author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
  date = {2019-03-15},
  url = {http://arxiv.org/abs/1802.07191},
  urldate = {2019-12-10},
  abstract = {Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.},
  archivePrefix = {arXiv},
  eprint = {1802.07191},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BA3JRL2E\\Kandasamy et al. - 2019 - Neural Architecture Search with Bayesian Optimisat.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{karasuyamaManifoldbasedSimilarityAdaptation,
  title = {Manifold-Based {{Similarity Adaptation}} for {{Label Propagation}}},
  author = {Karasuyama, Masayuki and Mamitsuka, Hiroshi},
  pages = {9},
  abstract = {Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\E2Q9TVMS\\Karasuyama and Mamitsuka - Manifold-based Similarity Adaptation for Label Pro.pdf},
  langid = {english}
}

@article{kawaharaRestructuringSpeechRepresentations1999,
  title = {Restructuring Speech Representations Using a Pitch-Adaptive Time–Frequency Smoothing and an Instantaneous-Frequency-Based {{F0}} Extraction: {{Possible}} Role of a Repetitive Structure in {{sounds1Speech}} Files Available. {{See}} {{http://www.elsevier.nl/locate/specom1}}},
  shorttitle = {Restructuring Speech Representations Using a Pitch-Adaptive Time–Frequency Smoothing and an Instantaneous-Frequency-Based {{F0}} Extraction},
  author = {Kawahara, Hideki and Masuda-Katsuse, Ikuyo and de Cheveigné, Alain},
  date = {1999-04-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {27},
  pages = {187--207},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(98)00085-5},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639398000855},
  urldate = {2019-09-19},
  abstract = {A set of simple new procedures has been developed to enable the real-time manipulation of speech parameters. The proposed method uses pitch-adaptive spectral analysis combined with a surface reconstruction method in the time–frequency region. The method also consists of a fundamental frequency (F0) extraction using instantaneous frequency calculation based on a new concept called `fundamentalness'. The proposed procedures preserve the details of time–frequency surfaces while almost perfectly removing fine structures due to signal periodicity. This close-to-perfect elimination of interferences and smooth F0 trajectory allow for over 600\% manipulation of such speech parameters as pitch, vocal tract length, and speaking rate, while maintaining high reproductive quality.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GDLPVX8H\\Kawahara et al_1999_Restructuring speech representations using a pitch-adaptive time–frequency.pdf;C\:\\Users\\sauli\\Zotero\\storage\\CVU64GCD\\S0167639398000855.html},
  keywords = {F0 extraction,Instantaneous frequency,Pitch-synchronous,Speech analysis,Speech modification,Speech synthesis,Spline smoothing},
  number = {3},
  options = {useprefix=true}
}

@inproceedings{kayserDiscriminativeLearningApproach2014,
  title = {A Discriminative Learning Approach to Probabilistic Acoustic Source Localization},
  booktitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Kayser, H. and Anemüller, J.},
  date = {2014-09},
  pages = {99--103},
  doi = {10.1109/IWAENC.2014.6953346},
  abstract = {Sound source localization algorithms commonly include assessment of inter-sensor (generalized) correlation functions to obtain direction-of-arrival estimates. Here, we present a classification-based method for source localization that uses discriminative support vector machine-learning of correlation patterns that are indicative of source presence or absence. Subsequent probabilistic modeling generates a map of sound source presence probability in given directions. Being data-driven, the method during training adapts to characteristics of the sensor setup, such as convolution effects in non-free-field situations, and to target signal specific acoustic properties. Experimental evaluation was conducted with algorithm training in anechoic single-talker scenarios and test data from several reverberant multi-talker situations, together with diffuse and real-recorded background noise, respectively. Results demonstrate that the method successfully generalizes from training to test conditions. Improvement over the best of five investigated state-of-the-art angular spectrum-based reference methods was on average about 45\% in terms of relative F-measure-related error reduction.},
  eventtitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SGK94859\\Kayser and Anemüller - 2014 - A discriminative learning approach to probabilisti.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SP972NWW\\6953346.html},
  keywords = {acoustic signal processing,Acoustics,anechoic single-talker scenario,Conferences,correlation methods,correlation patterns,direction-of-arrival estimates,direction-of-arrival estimation,discriminative classification,Estimation,learning (artificial intelligence),machine learning,probabilistic acoustic source localization,probability,reverberant multitalker situations,reverberation,signal to noise ratio,sound source localization,Speech,support vector machines,uses discriminative support vector machine learning}
}

@article{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2013-12-20},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2017-03-14},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3DVF922H\\Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\sauli\\Zotero\\storage\\U3T3HJ26\\1312.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{KingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2013-12},
  journaltitle = {arXiv:1312.6114 [cs, stat]},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprintclass = {cs, stat},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YBB35J4V\\Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\sauli\\Zotero\\storage\\FYU56LX2\\1312.html},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@online{kiongMinimumVarianceDistortionless2014,
  title = {Minimum {{Variance Distortionless Response Beamformer}} with {{Enhanced Nulling Level Control}} via {{Dynamic Mutated Artificial Immune System}}},
  author = {Kiong, Tiong Sieh and Salem, S. Balasem and Paw, Johnny Koh Siaw and Sankar, K. Prajindra and Darzi, Soodabeh},
  date = {2014},
  journaltitle = {The Scientific World Journal},
  doi = {10.1155/2014/164053},
  url = {https://www.hindawi.com/journals/tswj/2014/164053/},
  urldate = {2019-09-13},
  abstract = {In smart antenna applications, the adaptive beamforming technique is used to cancel interfering signals (placing nulls) and produce or steer a strong beam toward the target signal according to the calculated weight vectors. Minimum variance distortionless response (MVDR) beamforming is capable of determining the weight vectors for beam steering; however, its nulling level on the interference sources remains unsatisfactory. Beamforming can be considered as an optimization problem, such that optimal weight vector should be obtained through computation. Hence, in this paper, a new dynamic mutated artificial immune system (DM-AIS) is proposed to enhance MVDR beamforming for controlling the null steering of interference and increase the signal to interference noise ratio (SINR) for wanted signals.},
  eprint = {25003136},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HVQXNDGN\\Kiong et al. - 2014 - Minimum Variance Distortionless Response Beamforme.pdf;C\:\\Users\\sauli\\Zotero\\storage\\RU2DFJZT\\164053.html},
  langid = {english},
  type = {Research article}
}

@article{klarLevellingLoudnessProblems2002,
  title = {On Levelling and Loudness Problems at Television and Radio Broadcast Studios},
  author = {Klar, Siegfried and Spikofski, Gerhard},
  date = {2002},
  journaltitle = {PREPRINTS-AUDIO ENGINEERING SOCIETY},
  volume = {51},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GIHJ9CC5\\AES2002OnLevelling.pdf}
}

@article{knappGeneralizedCorrelationMethod1976,
  title = {The Generalized Correlation Method for Estimation of Time Delay},
  author = {Knapp, C. and Carter, G.},
  date = {1976-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {24},
  pages = {320--327},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1976.1162830},
  abstract = {A maximum likelihood (ML) estimator is developed for determining time delay between signals received at two spatially separated sensors in the presence of uncorrelated noise. This ML estimator can be realized as a pair of receiver prefilters followed by a cross correlator. The time argument at which the correlator achieves a maximum is the delay estimate. The ML estimator is compared with several other proposed processors of similar form. Under certain conditions the ML estimator is shown to be identical to one proposed by Hannan and Thomson [10] and MacDonald and Schultheiss [21]. Qualitatively, the role of the prefilters is to accentuate the signal passed to the correlator at frequencies for which the signal-to-noise (S/N) ratio is highest and, simultaneously, to suppress the noise power. The same type of prefiltering is provided by the generalized Eckart filter, which maximizes the S/N ratio of the correlator output. For low S/N ratio, the ML estimator is shown to be equivalent to Eckart prefiltering.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BMSZ6UKF\\Knapp and Carter - 1976 - The generalized correlation method for estimation .pdf;C\:\\Users\\sauli\\Zotero\\storage\\4RX9HIWW\\1162830.html},
  keywords = {Correlation,Correlators,Delay effects,Delay estimation,Frequency,Mathematical model,Maximum likelihood estimation,Propagation delay,signal processing,signal to noise ratio},
  number = {4}
}

@article{Knappgeneralizedcorrelationmethod1976,
  title = {The {{Generalized Correlation Method}} for {{Estimation}} of {{Time Delay}}},
  author = {Knapp, C. and Carter, G.},
  date = {1976-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {24},
  pages = {320--327},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1976.1162830},
  abstract = {A maximum likelihood (ML) estimator is developed for determining time delay between signals received at two spatially separated sensors in the presence of uncorrelated noise. This ML estimator can be realized as a pair of receiver prefilters followed by a cross correlator. The time argument at which the correlator achieves a maximum is the delay estimate. The ML estimator is compared with several other proposed processors of similar form. Under certain conditions the ML estimator is shown to be identical to one proposed by Hannan and Thomson [10] and MacDonald and Schultheiss [21]. Qualitatively, the role of the prefilters is to accentuate the signal passed to the correlator at frequencies for which the signal-to-noise (S/N) ratio is highest and, simultaneously, to suppress the noise power. The same type of prefiltering is provided by the generalized Eckart filter, which maximizes the S/N ratio of the correlator output. For low S/N ratio, the ML estimator is shown to be equivalent to Eckart prefiltering.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4KC7QT6C\\Knapp and Carter - 1976 - The generalized correlation method for estimation .pdf;C\:\\Users\\sauli\\Zotero\\storage\\Z7KMBAXE\\1162830.html},
  keywords = {Correlation,Correlators,Delay effects,Delay estimation,Frequency,Mathematical model,Maximum likelihood estimation,Propagation delay,signal processing,signal to noise ratio},
  number = {4}
}

@article{knappGeneralizedCorrelationMethod1976a,
  title = {The Generalized Correlation Method for Estimation of Time Delay},
  author = {Knapp, C. and Carter, G.},
  date = {1976-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {24},
  pages = {320--327},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1976.1162830},
  abstract = {A maximum likelihood (ML) estimator is developed for determining time delay between signals received at two spatially separated sensors in the presence of uncorrelated noise. This ML estimator can be realized as a pair of receiver prefilters followed by a cross correlator. The time argument at which the correlator achieves a maximum is the delay estimate. The ML estimator is compared with several other proposed processors of similar form. Under certain conditions the ML estimator is shown to be identical to one proposed by Hannan and Thomson [10] and MacDonald and Schultheiss [21]. Qualitatively, the role of the prefilters is to accentuate the signal passed to the correlator at frequencies for which the signal-to-noise (S/N) ratio is highest and, simultaneously, to suppress the noise power. The same type of prefiltering is provided by the generalized Eckart filter, which maximizes the S/N ratio of the correlator output. For low S/N ratio, the ML estimator is shown to be equivalent to Eckart prefiltering.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WBWNSB3J\\1162830.html},
  keywords = {Correlation,Correlators,Delay effects,Delay estimation,Frequency,Mathematical model,Maximum likelihood estimation,Propagation delay,Signal processing,Signal to noise ratio},
  number = {4}
}

@online{KnowingYourNeighbours,
  title = {Knowing {{Your Neighbours}}: {{Machine Learning}} on {{Graphs}}},
  shorttitle = {Knowing {{Your Neighbours}}},
  journaltitle = {KDnuggets},
  url = {https://www.kdnuggets.com/knowing-your-neighbours-machine-learning-on-graphs.html/},
  urldate = {2019-11-11},
  abstract = {Graph Machine Learning uses the network structure of the underlying data to improve predictive outcomes. Learn how to use this modern machine learning method to solve challenges with connected data.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YR35E39E\\neighbours-machine-learning-graphs.html},
  langid = {american}
}

@online{koftinoffGeneralAVBFAQ2014,
  title = {General {{AVB FAQ}}},
  author = {Koftinoff, Jeff},
  date = {2014-02-10T08:08:00-08:00},
  journaltitle = {AVB/TSN},
  url = {https://avb.statusbar.com/page/faq/},
  urldate = {2017-06-13},
  abstract = {General AVB FAQ},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5PNM68IK\\faq.html}
}

@article{kongTransductiveMultilabelLearning2013,
  title = {Transductive {{Multilabel Learning}} via {{Label Set Propagation}}},
  author = {Kong, Xiangnan and Ng, Michael K. and Zhou, Zhi-Hua},
  date = {2013-03},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {25},
  pages = {704--719},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2011.141},
  abstract = {The problem of multilabel classification has attracted great interest in the last decade, where each instance can be assigned with a set of multiple class labels simultaneously. It has a wide variety of real-world applications, e.g., automatic image annotations and gene function analysis. Current research on multilabel classification focuses on supervised settings which assume existence of large amounts of labeled training data. However, in many applications, the labeling of multilabeled data is extremely expensive and time consuming, while there are often abundant unlabeled data available. In this paper, we study the problem of transductive multilabel learning and propose a novel solution, called Trasductive Multilabel Classification (TraM), to effectively assign a set of multiple labels to each instance. Different from supervised multilabel learning methods, we estimate the label sets of the unlabeled instances effectively by utilizing the information from both labeled and unlabeled data. We first formulate the transductive multilabel learning as an optimization problem of estimating label concept compositions. Then, we derive a closed-form solution to this optimization problem and propose an effective algorithm to assign label sets to the unlabeled instances. Empirical studies on several real-world multilabel learning tasks demonstrate that our TraM method can effectively boost the performance of multilabel classification by using both labeled and unlabeled data.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JC87FVDJ\\Kong et al. - 2013 - Transductive Multilabel Learning via Label Set Pro.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3JIAL26C\\5936063.html},
  keywords = {closed-form solution,Closed-form solution,data mining,Data mining,label concept composition estimation,label set propagation,labeled training data,learning (artificial intelligence),Learning systems,machine learning,Machine learning,multilabel classification problem,multilabel learning,multiple class labels,optimisation,Optimization,optimization problem,pattern classification,semi-supervised learning,Semisupervised learning,supervised multilabel learning methods,Training data,TRAM method,transductive learning,transductive multilabel learning,trasductive multilabel classification,unlabeled data,unlabeled instances},
  number = {3}
}

@article{kotus_multiple_2013,
  title = {Multiple Sound Sources Localization in Free Field Using Acoustic Vector Sensor},
  author = {Kotus, Józef},
  date = {2013-06-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {74},
  pages = {4235--4251},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-013-1549-y},
  url = {http://link.springer.com/article/10.1007/s11042-013-1549-y},
  urldate = {2017-01-11},
  abstract = {Method and preliminary results of multiple sound sources localization in free field using the acoustic vector sensor were presented in this study. Direction of arrival (DOA) for considered source was determined based on sound intensity method supported by Fourier analysis. Obtained spectrum components for considered signal allowed to determine the DOA value for the particular frequency independently. The accuracy of the developed and practically implemented algorithm was evaluated on the basis of laboratory tests. Both synthetic acoustic signals (pure tones and noises) and real sounds were used during the measurements. Real signals had the same or different energy distribution both on time and frequency domain. The setup of the experiment and obtained results were described in details in the text. Taking the obtained results into consideration is important to emphasize that the localization of the multiple sound sources using single acoustic vector sensor is possible. The localization accuracy was the best for signals which spectral energy distribution was different.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MJASU8KZ\\Kotus - 2015 - Multiple sound sources localization in free field .pdf;C\:\\Users\\sauli\\Zotero\\storage\\Q3VS6NR4\\s11042-013-1549-y.html},
  langid = {english},
  number = {12}
}

@article{koyamaSparseSoundField2018,
  title = {Sparse Sound Field Decomposition for Super-Resolution in Recording and Reproduction},
  author = {Koyama, Shoichi and Murata, Naoki and Saruwatari, Hiroshi},
  date = {2018-06},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{3780-3795\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5042215\}},
  abstract = {A sound field recording and reproduction method based on sparse sound field decomposition is proposed. Most current methods are based on plane-wave or harmonic decomposition of the pressure distribution obtained by microphones, which leads to spatial aliasing artifacts with severe effects. This paper proposes a method for sound field decomposition based on a generative model of the sound field consisting of near-field source components and far-field plane-wave components. Since the distribution of the near-field source components can be assumed to be spatially sparse, the pressure distribution obtained by the microphones can be decomposed into these two components using sparse decomposition algorithms. Using the proposed method, the sound field can be more accurately interpolated and super-resolution in recording and reproduction can be achieved. Experimental results show that the reproduction accuracy above the spatial Nyquist frequency determined by the microphone intervals was improved compared with that of the current methods. (C) 2018 Author(s).},
  affiliation = {Koyama, S (Reprint Author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138656, Japan. Koyama, S (Reprint Author), Paris Diderot Univ, ESPCI, Inst Langevin, CNRS UMR 7587, F-75005 Paris, France. Koyama, Shoichi; Murata, Naoki; Saruwatari, Hiroshi, Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138656, Japan. Koyama, Shoichi, Paris Diderot Univ, ESPCI, Inst Langevin, CNRS UMR 7587, F-75005 Paris, France.},
  author-email = {koyama.shoichi@ieee.org},
  cited-references = {Ahrens J, 2012, J ACOUST SOC AM, V131, P2190, DOI 10.1121/1.3682036. ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Asaei Afsaneh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1439, DOI 10.1109/ICASSP.2014.6853835. BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852. Chardon G, 2015, IEEE J-STSP, V9, P815, DOI 10.1109/JSTSP.2015.2422673. Chardon G, 2012, J ACOUST SOC AM, V132, P1521, DOI 10.1121/1.4740476. Chartrand R, 2008, INT CONF ACOUST SPEE, P3869, DOI 10.1109/ICASSP.2008.4518498. Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010. Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172. Daniel J., 2003, P 23 AES INT C COP D. de Vries D., 2009, AES MONOGRAPH, P95. Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1. Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837. Figueiredo MAT, 2007, IEEE T IMAGE PROCESS, V16, P2980, DOI 10.1109/TIP.2007.909318. Gauthier PA, 2005, J ACOUST SOC AM, V117, P662, DOI 10.1121/1.1850032. Gemba KL, 2017, J ACOUST SOC AM, V141, P3411, DOI 10.1121/1.4983467. GERZON MA, 1973, J AUDIO ENG SOC, V21, P2. Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475. Gumerov N. A., 2005, FAST MULTIPOLE METHO, P520. HANSEN PC, 1993, SIAM J SCI COMPUT, V14, P1487, DOI 10.1137/0914086. Helwani K., 2012, P INT WORKSH AC SIGN. Huang JZ, 2010, ANN STAT, V38, P1978, DOI 10.1214/09-AOS778. Kameoka H, 2009, INT CONF ACOUST SPEE, P3437, DOI 10.1109/ICASSP.2009.4960364. Kirkeby O, 1996, J ACOUST SOC AM, V100, P1584, DOI 10.1121/1.416060. Kolundzija M, 2011, J AUDIO ENG SOC, V59, P721. Koyama Shoichi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4443, DOI 10.1109/ICASSP.2014.6854442. Koyama S., 2017, P INT C SOUND VIBR I. Koyama S, 2016, J ACOUST SOC AM, V139, P1024, DOI 10.1121/1.4942590. Koyama S, 2015, 2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL SUMMIT AND CONFERENCE (APSIPA), P850, DOI 10.1109/APSIPA.2015.7415391. Koyama S, 2015, INT CONF ACOUST SPEE, P619, DOI 10.1109/ICASSP.2015.7178043. Koyama S, 2015, IEEE J-STSP, V9, P881, DOI 10.1109/JSTSP.2015.2434319. Koyama S, 2014, IEICE T FUND ELECTR, VE97A, P1840, DOI 10.1587/transfun.E97.A.1840. Koyama S, 2014, IEEE-ACM T AUDIO SPE, V22, P1546, DOI 10.1109/TASLP.2014.2339735. Koyama S, 2013, IEEE T AUDIO SPEECH, V21, P685, DOI 10.1109/TASL.2012.2229985. Koyama S, 2012, INT CONF ACOUST SPEE, P381, DOI 10.1109/ICASSP.2012.6287896. LEROUX J, 2013, P IEEE INT C AC SPEE, P4310. Lilis GN, 2010, IEEE T AUDIO SPEECH, V18, P1902, DOI 10.1109/TASL.2010.2040523. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. McCoy MB, 2014, IEEE SIGNAL PROC MAG, V31, P87, DOI 10.1109/MSP.2013.2296605. Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x. PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465. Poletti MA, 2005, J AUDIO ENG SOC, V53, P1004. Qin ZW, 2013, MATH PROGRAM COMPUT, V5, P143, DOI 10.1007/s12532-013-0051-x. Rakotomamonjy A, 2011, SIGNAL PROCESS, V91, P1505, DOI 10.1016/j.sigpro.2011.01.012. Samarasinghe P, 2014, IEEE-ACM T AUDIO SPE, V22, P647, DOI 10.1109/TASLP.2014.2300341. Spors S., 2008, P 124 AES CONV AMST. Spors S., 2009, P 127 AES CONV NEW Y. Stojnic M, 2009, IEEE T SIGNAL PROCES, V57, P3075, DOI 10.1109/TSP.2009.2020754. Sun Y, 2017, IEEE T SIGNAL PROCES, V65, P794, DOI 10.1109/TSP.2016.2601299. Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267. Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030. Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108. Wabnitz A, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P1, DOI 10.1109/ASPAA.2011.6082301. Williams E., 1999, FOURIER ACOUSTICS SO, P306. Wipf DP, 2007, IEEE T SIGNAL PROCES, V55, P3704, DOI 10.1109/TSP.2007.894265. Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016. WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060. Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x.},
  da = {2018-10-18},
  doc-delivery-number = {GL3LI},
  eissn = {1520-8524},
  funding-acknowledgement = {JSPS KAKENHI [JP15H05312]},
  funding-text = {We would like to thank Professor Ken'ichi Furuya for his helpful comments and discussions. This work was supported by JSPS KAKENHI Grant No. JP15H05312.},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {SIGNAL RECONSTRUCTION; SPHERICAL-HARMONICS; SOURCE LOCALIZATION; FREQUENCY-DOMAIN; GROUP LASSO; ALGORITHMS; REGRESSION; SELECTION; PURSUIT; REGULARIZATION},
  langid = {english},
  number = {6},
  number-of-cited-references = {58},
  orcid-numbers = {Koyama, Shoichi/0000-0003-2283-0884},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000437036000069},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@article{kriegelEfficientNextbestscanPlanning2015,
  title = {Efficient Next-Best-Scan Planning for Autonomous {{3D}} Surface Reconstruction of Unknown Objects},
  author = {Kriegel, Simon and Rink, Christian and Bodenmüller, Tim and Suppa, Michael},
  date = {2015-12-01},
  journaltitle = {Journal of Real-Time Image Processing},
  shortjournal = {J Real-Time Image Proc},
  volume = {10},
  pages = {611--631},
  issn = {1861-8219},
  doi = {10.1007/s11554-013-0386-6},
  url = {https://doi.org/10.1007/s11554-013-0386-6},
  urldate = {2020-01-17},
  abstract = {This work focuses on autonomous surface reconstruction of small-scale objects with a robot and a 3D sensor. The aim is a high-quality surface model allowing for robotic applications such as grasping and manipulation. Our approach comprises the generation of next-best-scan (NBS) candidates and selection criteria, error minimization between scan patches and termination criteria. NBS candidates are iteratively determined by a boundary detection and surface trend estimation of the acquired model. To account for both a fast and high-quality model acquisition, that candidate is selected as NBS, which maximizes a utility function that integrates an exploration and a mesh-quality component. The modeling and scan planning methods are evaluated on an industrial robot with a high-precision laser striper system. While performing the new laser scan, data are integrated on-the-fly into both, a triangle mesh and a probabilistic voxel space. The efficiency of the system in fast acquisition of high-quality 3D surface models is proven with different cultural heritage, household and industrial objects.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CDPWFWLZ\\Kriegel et al. - 2015 - Efficient next-best-scan planning for autonomous 3.pdf},
  langid = {english},
  number = {4}
}

@article{krishnaveniBeamformingDirectionofArrivalDOA2013,
  title = {Beamforming for {{Direction}}-of-{{Arrival}} ({{DOA}}) {{Estimation}}-{{A Survey}}},
  author = {Krishnaveni, V and Kesavamurthy, T and B, Aparna.},
  date = {2013-01-18},
  journaltitle = {International Journal of Computer Applications},
  volume = {61},
  pages = {4--11},
  issn = {09758887},
  doi = {10.5120/9970-4758},
  url = {http://research.ijcaonline.org/volume61/number11/pxc3884758.pdf},
  urldate = {2019-03-22},
  abstract = {Direction-of-Arrival (DOA) estimation plays a vital role in many applications. Beamforming is the most prominent technique to estimate DOA. In this survey, a study of various beamforming techniques and algorithms to estimate the direction of arrival of a signal is made. An assessment on the background robust algorithms using Nyquist sampling rate and its Compressive sensing alternative is done. It is known that Bearing estimation algorithms obtain only a small number of direction of arrivals (DOAs) within the entire angle domain, when the sources are spatially sparse. Hence, it may be concluded that, the methods those specifically exploits this spatial sparsity property is advantageous. These methods use a very small number of measurements in the form of random projections of the sensor data along with one full waveform recording at one of the sensors.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9RAD6JWJ\\Krishnaveni et al. - 2013 - Beamforming for Direction-of-Arrival (DOA) Estimat.pdf},
  langid = {english},
  number = {11}
}

@inproceedings{krolikowskiLocalizationSoundSources2000,
  title = {Localization of {{Sound Sources}} by {{Means}} of {{Recurrent Neural Networks}}},
  booktitle = {Rough {{Sets}} and {{Current Trends}} in {{Computing}}},
  author = {Krolikowski, Rafal and Czyzewski, Andrzej and Kostek, Bozena},
  date = {2000-10-16},
  pages = {603--610},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-45554-X_76},
  url = {https://link.springer.com/chapter/10.1007/3-540-45554-X_76},
  urldate = {2018-04-11},
  abstract = {The issue of localization of sound sources for videoconferencing is discussed in the paper. A new algorithm for estimating speaker locations, based on recurrent neural networks (RNN), is introduced and described. The scheme of experiments carried out in an acoustically adopted chamber, exploiting the engineered method is detailed.},
  eventtitle = {International {{Conference}} on {{Rough Sets}} and {{Current Trends}} in {{Computing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8K97BZXC\\10.html},
  isbn = {978-3-540-43074-2 978-3-540-45554-7},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{krolikowskiLocalizationSoundSources2000a,
  title = {Localization of {{Sound Sources}} by {{Means}} of {{Recurrent Neural Networks}}},
  booktitle = {Rough {{Sets}} and {{Current Trends}} in {{Computing}}},
  author = {Krolikowski, Rafal and Czyzewski, Andrzej and Kostek, Bozena},
  date = {2000-10-16},
  pages = {603--610},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-45554-X_76},
  url = {https://link.springer.com/chapter/10.1007/3-540-45554-X_76},
  urldate = {2018-04-02},
  abstract = {The issue of localization of sound sources for videoconferencing is discussed in the paper. A new algorithm for estimating speaker locations, based on recurrent neural networks (RNN), is introduced and described. The scheme of experiments carried out in an acoustically adopted chamber, exploiting the engineered method is detailed.},
  eventtitle = {International {{Conference}} on {{Rough Sets}} and {{Current Trends}} in {{Computing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KLQM2AZI\\3-540-45554-X_76.html},
  isbn = {978-3-540-43074-2 978-3-540-45554-7},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{kubkeDevelopmentAuditoryCenters2005,
  title = {Development of the {{Auditory Centers Responsible}} for {{Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Kubke, M. Fabiana and Carr, Catherine E.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {179--237},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_6},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_6},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7J6Z8NH2\\0-387-28863-5_6.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{KubkeDevelopmentAuditoryCenters2005,
  title = {Development of the {{Auditory Centers Responsible}} for {{Sound Localization}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Kubke, M. Fabiana and Carr, Catherine E.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {179--237},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_6},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\95SHNIST\\0-387-28863-5_6.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@article{kuhneRobustSourceLocalization2009,
  title = {Robust {{Source Localization}} in {{Reverberant Environments Based}} on {{Weighted Fuzzy Clustering}}},
  author = {Kuhne, M. and Togneri, R. and Nordholm, S.},
  date = {2009-02},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {16},
  pages = {85--85},
  issn = {1070-9908},
  doi = {10.1109/LSP.2008.2009833},
  abstract = {Successful localization of sound sources in reverberant enclosures is an important prerequisite for many spatial signal processing algorithms. We investigate the use of a weighted fuzzy c-means cluster algorithm for robust source localization using location cues extracted from a microphone array. In order to increase the algorithm's robustness against sound reflections, we incorporate observation weights to emphasize reliable cues over unreliable ones. The weights are computed from local feature statistics around sound onsets because it is known that these regions are least affected by reverberation. Experimental results illustrate the superiority of the method when compared with standard fuzzy clustering. The proposed algorithm successfully located two speech sources for a range of angular separations in room environments with reverberation times of up to 600 ms.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U3424KGC\\Kuhne et al. - 2009 - Robust Source Localization in Reverberant Environm.pdf;C\:\\Users\\sauli\\Zotero\\storage\\JRJIURD5\\4745930.html},
  keywords = {acoustic signal processing,Australia,Clustering algorithms,fuzzy c-means cluster algorithm,Fuzzy clustering,fuzzy set theory,microphone array,microphone arrays,Partitioning algorithms,pattern clustering,Position measurement,reverberant enclosure,reverberation,robust source localization,Robustness,Signal processing algorithms,sound onset,sound reflection,Source localization,Source separation,Speech,speech sources,weighted fuzzy clustering},
  number = {2}
}

@article{kuhneRobustSourceLocalization2009a,
  title = {Robust {{Source Localization}} in {{Reverberant Environments Based}} on {{Weighted Fuzzy Clustering}}},
  author = {Kuhne, M. and Togneri, R. and Nordholm, S.},
  date = {2009-02},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {16},
  pages = {85--85},
  issn = {1070-9908},
  doi = {10.1109/LSP.2008.2009833},
  abstract = {Successful localization of sound sources in reverberant enclosures is an important prerequisite for many spatial signal processing algorithms. We investigate the use of a weighted fuzzy c-means cluster algorithm for robust source localization using location cues extracted from a microphone array. In order to increase the algorithm's robustness against sound reflections, we incorporate observation weights to emphasize reliable cues over unreliable ones. The weights are computed from local feature statistics around sound onsets because it is known that these regions are least affected by reverberation. Experimental results illustrate the superiority of the method when compared with standard fuzzy clustering. The proposed algorithm successfully located two speech sources for a range of angular separations in room environments with reverberation times of up to 600 ms.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NXC29XCX\\Kuhne et al. - 2009 - Robust Source Localization in Reverberant Environm.pdf;C\:\\Users\\sauli\\Zotero\\storage\\48XPZ2JH\\4745930.html},
  keywords = {acoustic signal processing,Australia,Clustering algorithms,fuzzy c-means cluster algorithm,Fuzzy clustering,fuzzy set theory,microphone array,microphone arrays,Partitioning algorithms,pattern clustering,Position measurement,reverberant enclosure,reverberation,robust source localization,Robustness,Signal processing algorithms,sound onset,sound reflection,Source localization,Source separation,Speech,speech sources,weighted fuzzy clustering},
  number = {2}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  date = {1955-03},
  journaltitle = {Naval Research Logistics Quarterly},
  shortjournal = {Naval Research Logistics},
  volume = {2},
  pages = {83--97},
  issn = {00281441, 19319193},
  doi = {10.1002/nav.3800020109},
  url = {http://doi.wiley.com/10.1002/nav.3800020109},
  urldate = {2019-08-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\C565GHHZ\\Kuhn - 1955 - The Hungarian method for the assignment problem.pdf},
  langid = {english},
  number = {1-2}
}

@article{kumarNearFieldAcousticSource2016,
  title = {Near-{{Field Acoustic Source Localization}} and {{Beamforming}} in {{Spherical Harmonics Domain}}},
  author = {Kumar, L. and Hegde, R. M.},
  date = {2016-07},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {64},
  pages = {3351--3361},
  doi = {10.1109/TSP.2016.2543201},
  abstract = {In this paper, we address the issue of near-field source localization using spherical microphone array. The spherical array has been widely used for far-field source localization due to ease of array processing in spherical harmonics domain. Various methods for far-field source localization has been reformulated in spherical harmonics domain. However, near-field source localization that involves joint estimation of range and bearing of the sources has hitherto not been investigated. In this paper, the near-field data model is developed in spherical harmonics domain. In particular, three methods that jointly estimate the range and bearing of multiple sources in the spherical array framework are proposed. Two subspace-based methods called the Spherical Harmonic MUltiple SIgnal Classification (SH-MUSIC) and the Spherical Harmonics MUSIC-Group Delay (SH-MGD) for near-field source localization are first presented. In addition, a method for near-field source localization and beamforming using Spherical Harmonic MVDR (SH-MVDR) is also formulated. Formulation and analysis of Cramér-Rao bound for near-field sources is presented in spherical harmonics domain. Various source localization experiments were conducted on simulated and signal acquired over spherical microphone array in an anechoic chamber. Root-mean-square error and probability of resolution are utilized as measures to evaluate the proposed methods. The significance and practical application of the proposed methods is discussed using experiment on interference suppression. The near-field SH-MVDR beamforming is utilized in this context.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YTFBF8Y4\\Kumar and Hegde - 2016 - Near-Field Acoustic Source Localization and Beamfo.pdf;C\:\\Users\\sauli\\Zotero\\storage\\7632Q652\\7435322.html},
  keywords = {acoustic radiators,anechoic chamber,anechoic chambers (acoustic),array processing,array signal processing,Array signal processing,Arrays,Covariance matrices,Cramer-Rao bound,Data models,estimation theory,far-field source localization,group delay,Harmonic analysis,interference suppression,mean square error methods,microphone arrays,Microphone arrays,minimum variance distortionless response,MUSIC,MVDR,Near-field,near-field acoustic source localization,near-field data,near-field SH-MVDR beamforming,near-field sources,root mean square error,SH-MUSIC,signal classification,source localization,spherical array framework,spherical harmonic multiple signal classification,spherical harmonics,spherical harmonics domain,spherical microphone array,subspace-based methods},
  number = {13}
}

@article{kumataniMicrophoneArrayProcessing2012,
  title = {Microphone {{Array Processing}} for {{Distant Speech Recognition}}: {{From Close}}-{{Talking Microphones}} to {{Far}}-{{Field Sensors}}},
  shorttitle = {Microphone {{Array Processing}} for {{Distant Speech Recognition}}},
  author = {Kumatani, Kenichi and McDonough, John and Raj, Bhiksha},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {29},
  pages = {127--140},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205285},
  url = {http://ieeexplore.ieee.org/document/6296525/},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SQRXL3TQ\\Kumatani et al. - 2012 - Microphone Array Processing for Distant Speech Rec.pdf},
  langid = {english},
  number = {6}
}

@article{laaksoSplittingUnitDelay1996,
  title = {Splitting the Unit Delay [{{FIR}}/All Pass Filters Design]},
  author = {Laakso, T. I. and Valimaki, V. and Karjalainen, M. and Laine, U. K.},
  date = {1996-01},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {13},
  pages = {30--60},
  issn = {1053-5888},
  doi = {10.1109/79.482137},
  abstract = {A fractional delay filter is a device for bandlimited interpolation between samples. It finds applications in numerous fields of signal processing, including communications, array processing, speech processing, and music technology. We present a comprehensive review of FIR and allpass filter design techniques for bandlimited approximation of a fractional digital delay. Emphasis is on simple and efficient methods that are well suited for fast coefficient update or continuous control of the delay value. Various new approaches are proposed and several examples are provided to illustrate the performance of the methods. We also discuss the implementation complexity of the algorithms. We focus on four applications where fractional delay filters are needed: synchronization of digital modems, incommensurate sampling rate conversion, high-resolution pitch prediction, and sound synthesis of musical instruments.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DBQE563A\\Laakso et al. - 1996 - Splitting the unit delay [FIRall pass filters des.pdf;C\:\\Users\\sauli\\Zotero\\storage\\835F6GHN\\482137.html},
  keywords = {algorithms,all-pass filters,allpass filter design,approximation theory,array processing,Array signal processing,Band pass filters,bandlimited approximation,bandlimited interpolation,Communication system control,communications,continuous control,Delay,delay circuits,digital filters,Digital filters,digital modems synchronisation,fast coefficient update,Finite impulse response filter,FIR filter design,FIR filters,fractional delay filter,fractional digital delay,high-resolution pitch prediction,implementation complexity,incommensurate sampling rate conversion,Interpolation,linear predictive coding,modems,Multiple signal classification,music,music technology,musical instruments,performance,signal processing,Signal processing algorithms,signal resolution,signal sampling,signal synthesis,sound synthesis,speech coding,speech processing,Speech processing,synchronisation,unit delay},
  number = {1}
}

@article{laufer-goldshteinMultiMicrophoneSpeakerLocalization2019,
  title = {Multi-{{Microphone Speaker Localization}} on {{Manifolds}}},
  author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
  date = {2019},
  pages = {206},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VFX8HTIU\\Laufer-Goldshtein et al. - Multi-Microphone Speaker Localization on Manifolds.pdf},
  langid = {english}
}

@article{laufer-goldshteinSemiSupervisedSoundSource2016,
  title = {Semi-{{Supervised Sound Source Localization Based}} on {{Manifold Regularization}}},
  author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
  date = {2016-08},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {24},
  pages = {1393--1407},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2555085},
  url = {http://ieeexplore.ieee.org/document/7458213/},
  urldate = {2019-10-28},
  abstract = {Conventional speaker localization algorithms, based merely on the received microphone signals, are often sensitive to adverse conditions, such as: high reverberation or low signal-to-noise ratio (SNR). In some scenarios, e.g., in meeting rooms or cars, it can be assumed that the source position is confined to a predefined area, and the acoustic parameters of the environment are approximately fixed. Such scenarios give rise to the assumption that the acoustic samples from the region of interest have a distinct geometrical structure. In this paper, we show that the high-dimensional acoustic samples indeed lie on a low-dimensional manifold and can be embedded into a low-dimensional space. Motivated by this result, we propose a semi-supervised source localization algorithm based on two-microphone measurements, which recovers the inverse mapping between the acoustic samples and their corresponding locations. The idea is to use an optimization framework based on manifold regularization, that involves smoothness constraints of possible solutions with respect to the manifold. The proposed algorithm, termed manifold regularization for localization, is adapted while new unlabelled measurements (from unknown source locations) are accumulated during runtime. Experimental results show superior localization performance when compared with a recently presented algorithm based on a manifold learning approach and with the generalized cross-correlation algorithm as a baseline. The algorithm achieves 2◦ accuracy in typical noisy and reverberant environments (reverberation time between 200 and 800 ms and SNR between 5 and 20 dB).},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8CTVP6K7\\Laufer-Goldshtein et al. - 2016 - Semi-Supervised Sound Source Localization Based on.pdf},
  langid = {english},
  number = {8}
}

@article{LearningCompareImage,
  title = {Learning to {{Compare Image Patches}} via {{Convolutional Neural Networks}} - {{Zagoruyko}}\_{{Learning}}\_to\_{{Compare}}\_2015\_{{CVPR}}\_paper.{{Pdf}}},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\J7IWXK9I\\Zagoruyko_Learning_to_Compare_2015_CVPR_paper.html}
}

@online{LearningCompareImagea,
  title = {Learning to {{Compare Image Patches}} via {{Convolutional Neural Networks}} - {{Zagoruyko}}\_{{Learning}}\_to\_{{Compare}}\_2015\_{{CVPR}}\_paper.Pdf},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf},
  urldate = {2017-02-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U9494AC7\\Zagoruyko_Learning_to_Compare_2015_CVPR_paper.html}
}

@article{lee_subspace-based_2015,
  title = {Subspace-Based {{DOA}} with Linear Phase Approximation and Frequency Bin Selection Preprocessing for Interactive Robots in Noisy Environments},
  author = {Lee, Sheng-Chieh and Chen, Bo-Wei and Wang, Jhing-Fa and Liao, Min-Jian and Ji, Wen},
  date = {2015-11},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {34},
  pages = {113--128},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2015.03.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230815000224},
  urldate = {2017-01-10},
  abstract = {This work develops a method of estimating subspace-based direction of arrival (DOA) that uses two proposed preprocesses. The method can be used in applications that involve interactive robots to calculate the direction to a noise-contaminated signal in noisy environments. The proposed method can be divided into two parts, which are linear phase approximation and frequency bin selection. Linear phase approximation rectifies the phases of the two-channel signals that are affected by noise, and reconstructs the covariance matrix of the received signals according to the compensative phases using phase line regression. To increase the accuracy of DOA result, a method of frequency bin selection that is based on eigenvalue decomposition (EVD) is utilized to detect and filter out the noisy frequency bins of the microphone signals. The proposed techniques are adopted in a method of subspace-based DOA estimation that is called multiple signal classification (MUSIC). Experimental results reveal that the mean estimation error obtained using proposed method can be reduced by 7.61° from the conventional MUSIC method. The proposed method is compared with the covariance-based DOA method that is called the minimum variance distortionless response (MVDR). The DOA improves the mean estimation accuracy by 4.98° relative to the conventional MVDR method. The experimental results demonstrate that both subspace-based and covariance-based DOA algorithms with the proposed preprocessing method outperform the DOA estimation in detecting the direction of signal in a noisy environment.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BJCJN3WQ\\Lee et al. - 2015 - Subspace-based DOA with linear phase approximation.pdf;C\:\\Users\\sauli\\Zotero\\storage\\F782EEWC\\S0885230815000224.html},
  keywords = {Angular correction,Covariance matrix reconstruction,Direction of arrival (DOA),Frequency bin selection,Human–robot interaction,Interactive robot,Linear phase approximation,Sound source location},
  number = {1}
}

@article{leeEfficientWidebandSource1994,
  title = {Efficient Wideband Source Localization Using Beamforming Invariance Technique},
  author = {Lee, Ta-Sung},
  date = {1994-06},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {42},
  pages = {1376--1387},
  issn = {1053-587X},
  doi = {10.1109/78.286954},
  abstract = {A novel scheme for wideband direction-of-arrival (DOA) estimation is proposed. The technique performs coherent signal subspace transformation by a set of judiciously constructed beamforming matrices. The beamformers are chosen to transform each of the narrowband array manifold vectors into the one corresponding to the reference frequency, regardless of the actual spatial distribution of the sources. The focused data correlation matrix can thus be obtained without any preliminary DOA estimation or iteration. A simplified version of the beamspace Root-MUSIC algorithm is developed and used in conjunction with the proposed method to efficiently localize multiple wideband sources with a linear, equally spaced array. Numerical simulations are conducted to demonstrate the efficacy of the new scheme},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VKT84VWX\\Lee - 1994 - Efficient wideband source localization using beamf.pdf;C\:\\Users\\sauli\\Zotero\\storage\\E3C766QD\\286954.html},
  keywords = {array signal processing,beamforming invariance technique,beamforming matrices,beamspace root-MUSIC algorithm,coherent signal subspace transformation,computational complexity,Covariance matrix,Direction of arrival estimation,DOA,efficient wideband source localization,focused data correlation matrix,Frequency,matrix algebra,matrix decomposition,multiple wideband sources,Narrowband,narrowband array manifold vector,Numerical simulation,parameter estimation,Position measurement,reference frequency,Sensor arrays,spatial distribution,Wideband,wideband direction-of-arrival estimation},
  number = {6}
}

@article{leeEfficientWidebandSource1994a,
  title = {Efficient Wideband Source Localization Using Beamforming Invariance Technique},
  author = {Lee, Ta-Sung},
  date = {1994-06},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {42},
  pages = {1376--1387},
  issn = {1053-587X},
  doi = {10.1109/78.286954},
  abstract = {A novel scheme for wideband direction-of-arrival (DOA) estimation is proposed. The technique performs coherent signal subspace transformation by a set of judiciously constructed beamforming matrices. The beamformers are chosen to transform each of the narrowband array manifold vectors into the one corresponding to the reference frequency, regardless of the actual spatial distribution of the sources. The focused data correlation matrix can thus be obtained without any preliminary DOA estimation or iteration. A simplified version of the beamspace Root-MUSIC algorithm is developed and used in conjunction with the proposed method to efficiently localize multiple wideband sources with a linear, equally spaced array. Numerical simulations are conducted to demonstrate the efficacy of the new scheme},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2H72ZFCH\\Lee - 1994 - Efficient wideband source localization using beamf.pdf;C\:\\Users\\sauli\\Zotero\\storage\\JEAW5CV8\\286954.html},
  keywords = {array signal processing,beamforming invariance technique,beamforming matrices,beamspace root-MUSIC algorithm,coherent signal subspace transformation,computational complexity,Covariance matrix,Direction of arrival estimation,DOA,efficient wideband source localization,focused data correlation matrix,Frequency,matrix algebra,matrix decomposition,multiple wideband sources,Narrowband,narrowband array manifold vector,Numerical simulation,parameter estimation,Position measurement,reference frequency,Sensor arrays,spatial distribution,Wideband,wideband direction-of-arrival estimation},
  number = {6}
}

@article{lentzBinauralTechnologyVirtual2008,
  title = {Binaural Technology for Virtual Reality},
  author = {Lentz, Tobias},
  date = {2008-12-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {124},
  pages = {3359--3359},
  issn = {0001-4966},
  doi = {10.1121/1.3020604},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.3020604},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZPD289HA\\1.html},
  number = {6}
}

@article{lentzVirtualRealitySystem2007,
  title = {Virtual {{Reality System}} with {{Integrated Sound Field Simulation}} and {{Reproduction}}},
  author = {Lentz, Tobias and Schröder, Dirk and Vorländer, Michael and Assenmacher, Ingo},
  date = {2007-12},
  journaltitle = {EURASIP Journal on Advances in Signal Processing},
  volume = {2007},
  issn = {1687-6180},
  doi = {10.1155/2007/70540},
  url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/70540},
  urldate = {2017-12-09},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5BUA3HEW\\Lentz et al. - 2007 - Virtual Reality System with Integrated Sound Field.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{lerouxMicbotsCollectingLarge2015,
  title = {Micbots: {{Collecting}} Large Realistic Datasets for Speech and Audio Research Using Mobile Robots},
  shorttitle = {Micbots},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Le Roux, Jonathan and Vincent, Emmanuel and Hershey, John R. and Ellis, Daniel P.W.},
  date = {2015-04},
  pages = {5635--5639},
  publisher = {{IEEE}},
  location = {{South Brisbane, Queensland, Australia}},
  doi = {10.1109/ICASSP.2015.7179050},
  url = {http://ieeexplore.ieee.org/document/7179050/},
  urldate = {2019-06-20},
  abstract = {Speech and audio signal processing research is a tale of data collection efforts and evaluation campaigns. Large benchmark datasets for automatic speech recognition (ASR) have been instrumental in the advancement of speech recognition technologies. However, when it comes to robust ASR, source separation, and localization, especially using microphone arrays, the perfect dataset is out of reach, and many different data collection efforts have each made different compromises between the conflicting factors in terms of realism, ground truth, and costs. Our goal here is to escape some of the most difficult trade-offs by proposing MICbots, a low-cost method of collecting large amounts of realistic data where annotations and ground truth are readily available. Our key idea is to use freely moving robots equiped with microphones and loudspeakers, playing recorded utterances from existing (already annotated) speech datasets. We give an overview of previous data collection efforts and the trade-offs they make, and describe the benefits of using our robot-based approach. We finally explain the use of this method to collect room impulse response measurement.},
  eventtitle = {{{ICASSP}} 2015 - 2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XVRV6764\\Le Roux et al. - 2015 - Micbots Collecting large realistic datasets for s.pdf},
  isbn = {978-1-4673-6997-8},
  langid = {english}
}

@inproceedings{lerouxMicbotsCollectingLarge2015a,
  title = {Micbots: {{Collecting}} Large Realistic Datasets for Speech and Audio Research Using Mobile Robots},
  shorttitle = {Micbots},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Le Roux, Jonathan and Vincent, Emmanuel and Hershey, John R. and Ellis, Daniel P.W.},
  date = {2015-04},
  pages = {5635--5639},
  publisher = {{IEEE}},
  location = {{South Brisbane, Queensland, Australia}},
  doi = {10.1109/ICASSP.2015.7179050},
  url = {http://ieeexplore.ieee.org/document/7179050/},
  urldate = {2019-06-19},
  abstract = {Speech and audio signal processing research is a tale of data collection efforts and evaluation campaigns. Large benchmark datasets for automatic speech recognition (ASR) have been instrumental in the advancement of speech recognition technologies. However, when it comes to robust ASR, source separation, and localization, especially using microphone arrays, the perfect dataset is out of reach, and many different data collection efforts have each made different compromises between the conflicting factors in terms of realism, ground truth, and costs. Our goal here is to escape some of the most difficult trade-offs by proposing MICbots, a low-cost method of collecting large amounts of realistic data where annotations and ground truth are readily available. Our key idea is to use freely moving robots equiped with microphones and loudspeakers, playing recorded utterances from existing (already annotated) speech datasets. We give an overview of previous data collection efforts and the trade-offs they make, and describe the benefits of using our robot-based approach. We finally explain the use of this method to collect room impulse response measurement.},
  eventtitle = {{{ICASSP}} 2015 - 2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YUX9SFNH\\Le Roux et al. - 2015 - Micbots Collecting large realistic datasets for s.pdf},
  isbn = {978-1-4673-6997-8},
  langid = {english}
}

@article{liangAuxiliaryFunctionBased,
  title = {Auxiliary Function Based {{IVA}} Using a Source Prior Exploiting Fourth Order Relationships},
  author = {Liang, Yanfeng and Harris, Jack and Chen, Gaojie and Naqvi, Syed Mohsen and Jutten, Christian and Chambers, Jonathon},
  pages = {6},
  abstract = {Independent vector analysis (IVA) can theoretically avoid the permutation ambiguity present in frequency domain independent component analysis by using a multivariate source prior to retain the dependency between different frequency bins of each source. The auxiliary function based independent vector analysis (AuxIVA) is a stable and fast update IVA algorithm which includes no tuning parameters. In this paper, a particular multivariate generalized Gaussian distribution source prior is therefore adopted to derive the AuxIVA algorithm which can exploit fourth order relationships to better preserve the dependency between different frequency bins of speech signals. Experimental results confirm the improved separation performance achieved by using the proposed algorithm.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9DGNNCTK\\Liang et al. - Auxiliary function based IVA using a source prior .pdf}
}

@article{liCascadedMultipleSpeakerLocalization2018,
  title = {A {{Cascaded Multiple}}-{{Speaker Localization}} and {{Tracking System}}},
  author = {Li, Xiaofei and Ban, Yutong and Girin, Laurent and Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2018},
  pages = {6},
  abstract = {This paper presents an online multiple-speaker localization and tracking method, as the INRIA-Perception contribution to the LOCATA Challenge 2018. First, the recursive least-square method is used to adaptively estimate the direct-path relative transfer function as an interchannel localization feature. The feature is assumed to associate with a single speaker at each time-frequency bin. Second, a complex Gaussian mixture model (CGMM) is used as a generative model of the features. The weight of each CGMM component represents the probability that this component corresponds to an active speaker, and is adaptively estimated with an online optimization algorithm. Finally, taking the CGMM component weights as observations, a Bayesian multiple-speaker tracking method based on the variational expectation maximization algorithm is used. The tracker accounts for the variation of active speakers and the localization miss measurements, by introducing speaker birth and sleeping processes. The experiments carried out on the development dataset of the challenge are reported.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PY4S7XM6\\Li et al. - 2018 - A Cascaded Multiple-Speaker Localization and Track.pdf},
  langid = {english}
}

@inproceedings{liDistributedSoundSource2018,
  title = {A Distributed Sound Source Surveillance System Using Autonomous Vehicle Network},
  booktitle = {2018 13th {{IEEE Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Li, X. and Chen, J. and Qi, W. and Zhou, R.},
  date = {2018-05},
  pages = {42--46},
  issn = {2158-2297},
  doi = {10.1109/ICIEA.2018.8397686},
  abstract = {Due to the physical limitation, single autonomous vehicle cannot afford a large aperture microphone array for high performance sound source localization. Motivated by Wireless Sensor Network (WSN) and multiple-base sonar technologies, a distributed sound source localization system prototype is presented in this paper using multiple autonomous vehicles as its mobile platforms. Similar to a typical WSN system, the system includes a base station and multiple sensor nodes. Each node is consisted of a series of sensors, a set of signal acquisition and processing unit, a wireless communication unit and a core MCU as its brain. Adopting TDOA-based direction finding method and LS-based Angle of Arrival localization method, the system operates autonomously and can search/localize the sound source in real time. The optimal sensor deployment technique is also considered which leads to a strategic vehicle control solution. The performance of the system is demonstrated via a series of experiments and the results show that it works well in the designated test field and it can meet the requirements of many application scenarios.},
  keywords = {acoustic generators,acoustic radiators,autonomous vehicle,autonomous vehicle network,base station,Conferences,core MCU,direction finding,direction-of-arrival estimation,distributed sound source localization system,distributed sound source surveillance system,Industrial electronics,large aperture microphone array,least squares approximations,LS-based angle of arrival localization method,microcontrollers,multiple autonomous vehicles,multiple sensor nodes,multiple-base sonar technologies,optimal sensor deployment technique,signal acquisition,signal detection,single autonomous vehicle,sound source localization,strategic vehicle control solution,surveillance,TDOA,TDOA-based direction finding method,vehicular ad hoc networks,wireless communication unit,wireless sensor network,wireless sensor networks,WSN system}
}

@article{liGraphbasedRegularizationRegression2019,
  title = {Graph-Based Regularization for Regression Problems with Alignment and Highly-Correlated Designs},
  author = {Li, Yuan and Mark, Benjamin and Raskutti, Garvesh and Willett, Rebecca and Song, Hyebin and Neiman, David},
  date = {2019-10-13},
  url = {http://arxiv.org/abs/1803.07658},
  urldate = {2019-11-26},
  abstract = {Sparse models for high-dimensional linear regression and machine learning have received substantial attention over the past two decades. Model selection, or determining which features or covariates are the best explanatory variables, is critical to the interpretability of a learned model. Much of the current literature assumes that covariates are only mildly correlated. However, in many modern applications covariates are highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition, restricted isometry property, or other related assumptions). This work considers a high-dimensional regression setting in which a graph governs both correlations among the covariates and the similarity among regression coefficients – meaning there is alignment between the covariates and regression coefficients. Using side information about the strength of correlations among features, we form a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph total variation regularizer that promotes similar weights for correlated features.},
  archivePrefix = {arXiv},
  eprint = {1803.07658},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9VQXLISU\\Li et al. - 2019 - Graph-based regularization for regression problems.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{liInformationEntropybasedViewpoint2005,
  title = {Information Entropy-Based Viewpoint Planning for 3-{{D}} Object Reconstruction},
  author = {Li, Y.F. and Liu, Z.G.},
  date = {2005-06},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {21},
  pages = {324--337},
  issn = {1941-0468},
  doi = {10.1109/TRO.2004.837239},
  abstract = {In this paper, we present an information entropy-based viewpoint-planning approach for reconstruction of freeform surfaces of three-dimensional objects. To achieve the reconstruction, the object is first sliced into a series of cross section curves, with each curve to be reconstructed by a closed B-spline curve. In the framework of Bayesian statistics, we propose an improved Bayesian information criterion (BIC) for determining the B-spline model complexity. Then, we analyze the uncertainty of the model using entropy as the measurement. Based on this analysis, we predict the information gain for each cross section curve for the next measurement. After predicting the information gain of each curve, we obtain the information change for all the B-spline models. This information gain is then mapped into the view space. The viewpoint that contains maximal information gain about the object is selected as the next best view. Experimental results show successful implementation of our view planning method for digitization and reconstruction of freeform objects.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WQTQPBSF\\1435477.html},
  keywords = {3-D reconstruction,3D object reconstruction,B-spline,B-spline curve,Bayesian information criterion,Bayesian methods,Bayesian statistics,belief networks,Data acquisition,entropy,Entropy,image reconstruction,Image reconstruction,information entropy,Manufacturing,maximal information gain,Research and development management,Spline,splines (mathematics),Statistics,Surface reconstruction,Three dimensional displays,uncertainty-driven,viewpoint planning},
  number = {3}
}

@article{liInformationEntropybasedViewpoint2005a,
  title = {Information Entropy-Based Viewpoint Planning for 3-{{D}} Object Reconstruction},
  author = {Li, Y.F. and Liu, Z.G.},
  date = {2005-06},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {21},
  pages = {324--337},
  issn = {1552-3098},
  doi = {10.1109/TRO.2004.837239},
  url = {http://ieeexplore.ieee.org/document/1435477/},
  urldate = {2020-01-27},
  abstract = {In this paper, we present an information entropybased viewpoint-planning approach for reconstruction of freeform surfaces of three-dimensional objects. To achieve the reconstruction, the object is first sliced into a series of cross section curves, with each curve to be reconstructed by a closed B-spline curve. In the framework of Bayesian statistics, we propose an improved Bayesian information criterion (BIC) for determining the B-spline model complexity. Then, we analyze the uncertainty of the model using entropy as the measurement. Based on this analysis, we predict the information gain for each cross section curve for the next measurement. After predicting the information gain of each curve, we obtain the information change for all the B-spline models. This information gain is then mapped into the view space. The viewpoint that contains maximal information gain about the object is selected as the next best view. Experimental results show successful implementation of our view planning method for digitization and reconstruction of freeform objects.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LL7NGREG\\Li and Liu - 2005 - Information entropy-based viewpoint planning for 3.pdf},
  langid = {english},
  number = {3}
}

@inproceedings{limParsingIKEAObjects2013,
  title = {Parsing {{IKEA Objects}}: {{Fine Pose Estimation}}},
  shorttitle = {Parsing {{IKEA Objects}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Lim, J. J. and Pirsiavash, H. and Torralba, A.},
  date = {2013-12},
  pages = {2992--2999},
  doi = {10.1109/ICCV.2013.372},
  abstract = {We address the problem of localizing and estimating the fine-pose of objects in the image with exact 3D models. Our main focus is to unify contributions from the 1970s with recent advances in object detection: use local keypoint detectors to find candidate poses and score global alignment of each candidate pose to the image. Moreover, we also provide a new dataset containing fine-aligned objects with their exactly matched 3D models, and a set of models for widely used objects. We also evaluate our algorithm both on object detection and fine pose estimation, and show that our method outperforms state-of-the art algorithms.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5JGB4HS3\\Lim et al. - 2013 - Parsing IKEA Objects Fine Pose Estimation.pdf;C\:\\Users\\sauli\\Zotero\\storage\\4GUKBXSR\\6751483.html},
  keywords = {candidate pose,Computational modeling,Design automation,Estimation,exact 3D model matching,fine-aligned objects,global alignment,IKEA object parsing,Image edge detection,image matching,local keypoint detectors,object detection,object fine-pose estimation,objects fine-pose localization,pose estimation,Shape,Solid modeling,solid modelling,Three-dimensional displays}
}

@inproceedings{limParsingIKEAObjects2013a,
  title = {Parsing {{IKEA Objects}}: {{Fine Pose Estimation}}},
  shorttitle = {Parsing {{IKEA Objects}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Lim, J. J. and Pirsiavash, H. and Torralba, A.},
  date = {2013-12},
  pages = {2992--2999},
  doi = {10.1109/ICCV.2013.372},
  abstract = {We address the problem of localizing and estimating the fine-pose of objects in the image with exact 3D models. Our main focus is to unify contributions from the 1970s with recent advances in object detection: use local keypoint detectors to find candidate poses and score global alignment of each candidate pose to the image. Moreover, we also provide a new dataset containing fine-aligned objects with their exactly matched 3D models, and a set of models for widely used objects. We also evaluate our algorithm both on object detection and fine pose estimation, and show that our method outperforms state-of-the art algorithms.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PQVIIPGH\\Lim et al. - 2013 - Parsing IKEA Objects Fine Pose Estimation.pdf;C\:\\Users\\sauli\\Zotero\\storage\\KU2HAIH9\\6751483.html},
  keywords = {candidate pose,Computational modeling,Design automation,Estimation,exact 3D model matching,fine-aligned objects,global alignment,IKEA object parsing,Image edge detection,image matching,local keypoint detectors,object detection,object fine-pose estimation,objects fine-pose localization,pose estimation,Shape,Solid modeling,solid modelling,Three-dimensional displays}
}

@inproceedings{linanSoundSourceTarget2010,
  title = {Sound Source Target Localization System of Mobile Robot},
  booktitle = {2010 11th {{International Conference}} on {{Control Automation Robotics Vision}}},
  author = {Linan, Z. and Peng, Y. and Hao, S. and Lingling, C.},
  date = {2010-12},
  pages = {2289--2294},
  doi = {10.1109/ICARCV.2010.5707825},
  abstract = {Target localization system is one of the basic aspects for mobile robot navigation. One of the most popular navigation way is the navigation based on visual. But, this way is limited by the light and the field of view so that it can't accurately detect the targets in special environments. To resolve the above problem of target localization way based on visual, this paper has studied the sound source target localization system. The key technologies of this system have been discussed. They are the structure of the microphone array, the multi acoustic source separation in dynamic environment, sound source localization method in mixed noise environment, the time delay estimation method in the mixed-noise environment, multi-sensor information fusion, and so on. Finally, an auditory system based on microphone array was found to locate the sound source target of 3-D space and some emulational experiments validate the correctness of this auditory system and the accuracy and realtime of the sound source localization.},
  eventtitle = {2010 11th {{International Conference}} on {{Control Automation Robotics Vision}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VPAUN5Q7\\Linan et al_2010_Sound source target localization system of mobile robot.pdf;C\:\\Users\\sauli\\Zotero\\storage\\WHP7HDJU\\5707825.html},
  keywords = {acoustic signal processing,Arrays,audition perception,auditory system,Auditory system,Bionics,Delay effects,delay estimation,dynamic environment,microphone array,microphone arrays,Microphones,mixed noise environment,mobile robot navigation,mobile robots,multiacoustic source separation,multisensor information fusion,Noise,path planning,Robots,sensor fusion,Sensors,sound source target localization system,source separation,target detection,target localization,target tracking,time delay estimation,time delay of arrival,visual navigation}
}

@article{linReverberationRobustLocalizationSpeakers2018,
  title = {Reverberation-{{Robust Localization}} of {{Speakers Using Distinct Speech Onsets}} and {{Multichannel Cross Correlations}}},
  author = {Lin, Shoufeng},
  date = {2018-11},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{2098-2111\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2854871\}},
  abstract = {Many speaker localization methods can he found in the literature. However, speaker localization under strong reverberation still remains a major challenge in the real-world applications. This paper proposes two algorithms for localizing speakers using microphone array recordings of reverberated sounds. To separate concurrent speakers, the first algorithm decomposes microphone signals spectrotemporally into subbands via an auditory filterbank. To suppress reverberation, we propose a novel speech onset detection approach derived from the speech signal and impulse response models, and further propose to formulate the multichannel cross-correlation coefficient of encoded speech onsets in each subband. The subband results are combined to estimate the directions-of-arrival of speakers. The second algorithm extends the generalized cross-correlation phase transform method by using redundant information of multiple microphones to address the reverberation problem. The proposed methods have been evaluated under adverse conditions using not only simulated signals (reverberation time T-60 of up to 1 s) but also recordings in a real reverberant room (T-60 approximate to 0.65 s). Comparing with some state-of-the-art localization methods, experimental results confirm that the proposed methods can reliably locate static and moving speakers, in presence of reverberation.},
  affiliation = {Lin, SF (Reprint Author), Curtin Univ, Sch Elect Engn Comp & Math Sci, Perth, WA 6102, Australia. Lin, Shoufeng, Curtin Univ, Sch Elect Engn Comp & Math Sci, Perth, WA 6102, Australia.},
  author-email = {shoufeng.lin@postgrad.curtin.edu.au},
  cited-references = {Begault DR, 2000, 3 SOUND VIRTUAL REAL. Benesty J, 2004, IEEE T SPEECH AUDI P, V12, P509, DOI 10.1109/TSA.2004.833008. Benesty J, 2000, J ACOUST SOC AM, V107, P384, DOI 10.1121/1.428310. Benesty J, 2008, SPRINGER TOP SIGN PR, V1, P1. Bregman AS, 1994, AUDITORY SCENE ANAL. Cooke M, 2006, J ACOUST SOC AM, V119, P1562, DOI 10.1121/1.2166600. Deller Jr J. R., 1993, DISCRETE TIME PROCES. Dixon S., 2006, P INT C DIG AUD EFF, V120, P133. Garofolo J. S., 1993, LINGUISTIC DATA CONS, V10. Holdsworth J., 1988, ANNEX C SVOS FINAL A, V1, P1. Huang J, 1997, IEEE T INSTRUM MEAS, V46, P842, DOI 10.1109/19.650785. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Kuhne M, 2009, IEEE SIGNAL PROC LET, V16, P85, DOI 10.1109/LSP.2008.2009833. Lee SY, 2010, IEEE SIGNAL PROC LET, V17, P671, DOI 10.1109/LSP.2010.2051050. Lehmann EA, 2008, J ACOUST SOC AM, V124, P269, DOI 10.1121/1.2936367. Lehmann EA, 2010, IEEE T AUDIO SPEECH, V18, P1429, DOI 10.1109/TASL.2009.2035038. Lin S., P IEEE INT C AC SPEE. Litovsky RY, 1999, J ACOUST SOC AM, V106, P1633, DOI 10.1121/1.427914. Lollmann H., 2010, P INT WORKSH AC ECH, P1. Lyon R. F., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P1148. MARAGOS P, 1993, IEEE T SIGNAL PROCES, V41, P3024, DOI 10.1109/78.277799. Meddis R, 1997, J ACOUST SOC AM, V102, P1811, DOI 10.1121/1.420088. MEDDIS R, 1986, J ACOUST SOC AM, V79, P702, DOI 10.1121/1.393460. Nguyen Thi Ngoc Tho, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2287, DOI 10.1109/ICASSP.2014.6854007. OTTERSTEN B, 1990, IEEE T ACOUST SPEECH, V38, P317, DOI 10.1109/29.103067. Parthy A, 2011, J ACOUST SOC AM, V130, P3827, DOI 10.1121/1.3658443. Patterson RD, 1987, M IOC SPEECH GROUP A, V2, P7. Pertila P, 2008, EURASIP J AUDIO SPEE, DOI 10.1155/2008/278185. Pesavento M, 2000, IEEE T SIGNAL PROCES, V48, P1306, DOI 10.1109/78.839978. Plinge A., 2010, P INT WORKSH AC ECH. RAKERD B, 1986, J ACOUST SOC AM, V80, P1695, DOI 10.1121/1.394282. RAO BD, 1989, IEEE T ACOUST SPEECH, V37, P1939, DOI 10.1109/29.45540. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. SCHROEDER MR, 1965, J ACOUST SOC AM, V38, P359, DOI 10.1121/1.1909677. Schuhmacher D, 2008, IEEE T SIGNAL PROCES, V56, P3447, DOI 10.1109/TSP.2008.920469. Smith LS, 2007, IEEE T AUDIO SPEECH, V15, P2278, DOI 10.1109/TASL.2007.904212. Teutsch H, 2006, J ACOUST SOC AM, V120, P2724, DOI 10.1121/1.2346089. Tiana-Roig E, 2010, J ACOUST SOC AM, V128, P3535, DOI 10.1121/1.3500669. Tolonen T, 2000, IEEE T SPEECH AUDI P, V8, P708, DOI 10.1109/89.876309. Torres AM, 2012, J ACOUST SOC AM, V132, P1511, DOI 10.1121/1.4740503. Wang D. L., 2006, COMPUTATIONAL AUDITO. Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.V8896.},
  da = {2018-10-18},
  doc-delivery-number = {GQ1VQ},
  funding-acknowledgement = {Australian Postgraduate Award; Australian Government Research Training Program Scholarship; Curtin Research Scholarship},
  funding-text = {This work was supported in part by an Australian Postgraduate Award, in part by an Australian Government Research Training Program Scholarship, and in part by a Curtin Research Scholarship.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {Acoustic multisource localization,CASA,concurrent speakers,cross-correlation,multichannel cross correlation,multiple source,onset detection,reverberation},
  keywords-plus = {ACOUSTIC SOURCE LOCALIZATION; CIRCULAR MICROPHONE ARRAYS; ROOM IMPULSE RESPONSES; SOUND LOCALIZATION; TIME-DELAY; MODEL; PERFORMANCE; PERCEPTION; SIMULATION; SEPARATION},
  langid = {english},
  number = {11},
  number-of-cited-references = {43},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000441430600012},
  usage-count-last-180-days = {5},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{liOnlineLocalizationTracking2018,
  title = {Online {{Localization}} and {{Tracking}} of {{Multiple Moving Speakers}} in {{Reverberant Environments}}},
  author = {Li, Xiaofei and Ban, Yutong and Girin, Laurent and Alameda-Pineda, Xavier and Horaud, Radu},
  date = {2018-09-28},
  url = {http://arxiv.org/abs/1809.10936},
  urldate = {2019-03-27},
  abstract = {We address the problem of online localization and tracking of multiple moving speakers in reverberant environments. The paper has the following contributions. We use the direct-path relative transfer function (DP-RTF), an inter-channel feature that encodes acoustic information robust against reverberation, and we propose an online algorithm well suited for estimating DP-RTFs associated with moving audio sources. Another crucial ingredient of the proposed method is its ability to properly assign DP-RTFs to audio-source directions. Towards this goal, we adopt a maximum-likelihood formulation and we propose to use an exponentiated gradient (EG) to efficiently update source-direction estimates starting from their currently available values. The problem of multiple speaker tracking is computationally intractable because the number of possible associations between observed source directions and physical speakers grows exponentially with time. We adopt a Bayesian framework and we propose a variational approximation of the posterior filtering distribution associated with multiple speaker tracking, as well as an efficient variational expectation-maximization (VEM) solver. The proposed online localization and tracking method is thoroughly evaluated using two datasets that contain recordings performed in real environments.},
  archivePrefix = {arXiv},
  eprint = {1809.10936},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4AQ8JZCV\\Li et al. - 2018 - Online Localization and Tracking of Multiple Movin.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3H62M7CH\\1809.html},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{liuInformationEntropyBased2003,
  title = {Information Entropy Based Viewpoint Planning for Digitization of {{3D}} Objects},
  booktitle = {Proceedings 2003 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}}. {{Computational Intelligence}} in {{Robotics}} and {{Automation}} for the {{New Millennium}} ({{Cat}}. {{No}}.{{03EX694}})},
  author = {Liu, Z.G. and Li, Y.F.},
  date = {2003-07},
  volume = {3},
  pages = {1509-1514 vol.3},
  issn = {null},
  doi = {10.1109/CIRA.2003.1222221},
  abstract = {In this paper, we present an information entropy based viewpoint planning approach for digitalization of 3D freeform objects. The object is firstly sliced into a number of cross section curves, with each cross-section to be reconstructed by a closed B-spline curve. Then, we propose an improved Bayesian information criterion (BIC) for selecting the control point number of B-spline models. Based on the selected model, we use entropy as the measurement of uncertainty of B-spline model to predict the information gain for each cross section curve. After obtaining the predicted information gain of all the B-spline models, we can map the information gain of these B-spline model into the view space. The viewpoint that contains maximal information gain for the object is then selected as the next best view. Finally, we show our experimental results for the digitization and reconstruction of freedom objects with our view planning method.},
  eventtitle = {Proceedings 2003 {{IEEE International Symposium}} on {{Computational Intelligence}} in {{Robotics}} and {{Automation}}. {{Computational Intelligence}} in {{Robotics}} and {{Automation}} for the {{New Millennium}} ({{Cat}}. {{No}}.{{03EX694}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UKWMJTQ5\\Liu and Li - 2003 - Information entropy based viewpoint planning for d.pdf;C\:\\Users\\sauli\\Zotero\\storage\\4EMW5QDV\\1222221.html},
  keywords = {3D object digitization,Bayesian information criterion,Bayesian methods,closed B-spline curve,computational geometry,cross-section curves,entropy,freeform object reconstruction,image reconstruction,Image reconstruction,Image sensors,information entropy,Information entropy,information gain,Predictive models,Pulp manufacturing,Research and development management,Spline,splines (mathematics),Surface reconstruction,Uncertainty,uncertainty measurement,viewpoint planning}
}

@article{liuSoundSourceLocalization2018,
  title = {Sound {{Source Localization}} in {{Reverberant Environments Based}} on {{Structural Sparse Bayesian Learning}}},
  author = {Liu, Yanshan and Wang, Lu and Zeng, Xiangyang and Wang, Haitao},
  date = {2018-05/2018-06},
  journaltitle = {ACTA ACUSTICA UNITED WITH ACUSTICA},
  volume = {104},
  pages = {\{528-541\}},
  publisher = {S HIRZEL VERLAG},
  location = {POSTFACH 10 10 61, D-70 009 STUTTGART, GERMANY},
  issn = {1610-1928},
  doi = {\{10.3813/AAA.919188\}},
  abstract = {The sound source localization in reverberant rooms is reformulated as a joint-sparsity support recovery problem in frequency domain under sparse Bayesian learning framework, where the reverberant effect is characterized using the image model. The joint sparsity in different frequencies is imposed by hierarchical probabilistic modeling with its hidden variables estimated by variational Bayesian inference. Numerical simulation results indicate that the proposed method achieves accurate sound source localization under low signal to noise ratio. The algorithm is evaluated by real data experiments using signals recorded in an anechoic chamber with one reflective plate and a rectangular room with strong reverberation. Both the numerical simulations and the real data experiments indicate that the proposed method can be applied in reverberant environments.},
  affiliation = {Wang, L (Reprint Author), Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710072, Shaanxi, Peoples R China. Liu, Yanshan; Wang, Lu; Zeng, Xiangyang; Wang, Haitao, Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710072, Shaanxi, Peoples R China.},
  author-email = {wanglu@nwpu.edu.cn},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Asaei A, 2014, IEEE-ACM T AUDIO SPE, V22, P620, DOI 10.1109/TASLP.2013.2297012. Beal M., 2004, VARIATIONAL ALGORITH. Bishop C., 2006, PATTERN RECOGN, P462. Blandin C., 2011, MULTISOURCE TDOA EST. Chardon G, 2012, INT CONF ACOUST SPEE, P9, DOI 10.1109/ICASSP.2012.6287804. DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157. Dmochowski J. P., 2007, IEEE WORKSH APPL SIG, P18. Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241. Grant M., 2013, CVX MATLAB SOFTWARE. Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7. Ji SH, 2008, IEEE T SIGNAL PROCES, V56, P2346, DOI 10.1109/TSP.2007.914345. Koyama Shoichi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4443, DOI 10.1109/ICASSP.2014.6854442. Koyama S, 2015, 2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL SUMMIT AND CONFERENCE (APSIPA), P850, DOI 10.1109/APSIPA.2015.7415391. Le Roux J, 2013, INT CONF ACOUST SPEE, P4310, DOI 10.1109/ICASSP.2013.6638473. Loesch B., 2010, COMP DIFFERENT ALGOR. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076. Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236. Tzikas DG, 2008, IEEE SIGNAL PROC MAG, V25, P131, DOI 10.1109/MSP.2008.929620.},
  da = {2018-10-18},
  doc-delivery-number = {GJ0KF},
  eissn = {1861-9959},
  funding-acknowledgement = {National Natural Science Foundation of China [61501375, 11374241]; Fundamental Research Funds for the Central Universities [3102016ZY006]; Aeronautical Science Foundation of China [20151553021]},
  funding-text = {This work was supported by the National Natural Science Foundation of China under Grant 61501375 and 11374241, the Fundamental Research Funds for the Central Universities under Grant 3102016ZY006, and the Aeronautical Science Foundation of China under Grant 20151553021.},
  journal-iso = {Acta Acust. United Acust.},
  langid = {english},
  number = {3},
  number-of-cited-references = {20},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000434942500015},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Acoustics}
}

@online{LivewireAES67AoIP,
  title = {Livewire+ {{AES67 AoIP Networking}}},
  url = {https://www.telosalliance.com/Axia/Livewire-AoIP-Networking},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DNBBSXJS\\Livewire-AoIP-Networking.html}
}

@online{LivewireAES67AoIPa,
  title = {Livewire+ {{AES67 AoIP Networking}}},
  url = {https://www.telosalliance.com/Axia/Livewire-AoIP-Networking},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3HGI78G6\\Livewire-AoIP-Networking.html}
}

@article{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018-11-07},
  url = {http://arxiv.org/abs/1712.09913},
  urldate = {2019-12-09},
  abstract = {Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “filter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archivePrefix = {arXiv},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YQTM6V5Y\\Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{LNCS2634Energy,
  title = {{{LNCS}} 2634 - {{Energy Based Acoustic Source Localization}} - {{C36ae3197296e9e37448f96384bd7c9181c3}}.{{Pdf}}},
  url = {https://pdfs.semanticscholar.org/4880/c36ae3197296e9e37448f96384bd7c9181c3.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CBA9CH8B\\c36ae3197296e9e37448f96384bd7c9181c3.html}
}

@online{LNCS2634Energya,
  title = {{{LNCS}} 2634 - {{Energy Based Acoustic Source Localization}} - C36ae3197296e9e37448f96384bd7c9181c3.Pdf},
  url = {https://pdfs.semanticscholar.org/4880/c36ae3197296e9e37448f96384bd7c9181c3.pdf},
  urldate = {2017-05-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\397ZBG2M\\c36ae3197296e9e37448f96384bd7c9181c3.html}
}

@online{LocalizationMultipleSpeakers,
  title = {Localization of Multiple Speakers Based on a Two Step Acoustic Map Analysis - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/4518618},
  urldate = {2019-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JESFMANW\\4518618.html}
}

@online{LocalizingSpeakersMultiple,
  title = {Localizing Speakers in Multiple Rooms by Using {{Deep Neural Networks}}},
  doi = {10.1016/j.csl.2017.12.002},
  url = {https://reader.elsevier.com/reader/sd/pii/S0885230817301377?token=22C139CE6BC2096B022ACE67632095AE3E59E94617FFE0949DBFF54278D96A10CA0EC2E6B37555C7F12603CB3244E5D1},
  urldate = {2018-10-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XN5WFPIZ\\S0885230817301377.html},
  langid = {english}
}

@article{LocatingAcousticSources,
  title = {Locating {{Acoustic Sources}} with {{Multilateration}} - {{Applied}} to {{Stationary}} and {{Moving Sources}}},
  pages = {84},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YNBZG54H\\Locating Acoustic Sources with Multilateration - A.pdf},
  langid = {english}
}

@article{loeschComparisonDifferentAlgorithms,
  title = {Comparison of {{Different Algorithms}} for {{Acoustic Source Localization}}},
  author = {Loesch, Benedikt and Ebrahim, Parisa and Yang, Bin},
  pages = {4},
  abstract = {Recently, position estimation for acoustic signals has been studied intensively and many different algorithms have been proposed. The different methods can be classified into indirect (estimation of time-difference of arrival (TDOA) and then position) and direct (direct estimation of position) methods. Furthermore, they can be classified according to whether they use a model for the mixing channel (model-based) or not. In this paper, we compare three different algorithms for source localization: one non-model based indirect method (DATEMM), one non-model based direct method (SRP-PHAT) and one model-based direct method (ICA-SCT). We compare them with respect to the underlying concept and the localization performance using simulations and real room recordings. We evaluate the influence of number of microphones, number of sources, microphone arrangement and reverberation time.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\47Z77KJ7\\Loesch et al. - Comparison of Different Algorithms for Acoustic So.pdf},
  langid = {english}
}

@inproceedings{lollmannLOCATAChallengeData2018,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Lollmann, Heinrich W. and Evers, Christine and Schmidt, Alexander and Mellmann, Heinrich and Barfuss, Hendrik and Naylor, Patrick A. and Kellermann, Walter},
  date = {2018-07},
  pages = {410--414},
  publisher = {{IEEE}},
  location = {{Sheffield}},
  doi = {10.1109/SAM.2018.8448644},
  url = {https://ieeexplore.ieee.org/document/8448644/},
  urldate = {2019-05-16},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\A4N5WMSZ\\Lollmann et al. - 2018 - The LOCATA Challenge Data Corpus for Acoustic Sour.pdf},
  isbn = {978-1-5386-4752-3},
  langid = {english}
}

@inproceedings{lollmannLOCATAChallengeData2018a,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Lollmann, Heinrich W. and Evers, Christine and Schmidt, Alexander and Mellmann, Heinrich and Barfuss, Hendrik and Naylor, Patrick A. and Kellermann, Walter},
  date = {2018-07},
  pages = {410--414},
  publisher = {{IEEE}},
  location = {{Sheffield}},
  doi = {10.1109/SAM.2018.8448644},
  url = {https://ieeexplore.ieee.org/document/8448644/},
  urldate = {2019-03-27},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3RB4JDIL\\Lollmann et al. - 2018 - The LOCATA Challenge Data Corpus for Acoustic Sour.pdf},
  isbn = {978-1-5386-4752-3},
  langid = {english}
}

@inproceedings{lollmannLOCATAChallengeData2018b,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Löllmann, H. W. and Evers, C. and Schmidt, A. and Mellmann, H. and Barfuss, H. and Naylor, P. A. and Kellermann, W.},
  date = {2018-07},
  pages = {410--414},
  issn = {2151-870X},
  doi = {10.1109/SAM.2018.8448644},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  keywords = {acoustic signal processing,acoustic source localization,Acoustics,Array signal processing,Arrays,audio recording,audio signal processing,autonomous systems,Databases,hearing aids,IEEE-AASP Challenge,LOCATA Challenge data corpus,loudspeakers,microphone array,microphone arrays,Microphone arrays,numerous algorithms,signal classification,sound source localization,spherical microphone array,state-of-the-art algorithms,static microphone array,Task analysis,tele-conferencing systems}
}

@inproceedings{lollmannLOCATAChallengeData2018c,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Löllmann, H. W. and Evers, C. and Schmidt, A. and Mellmann, H. and Barfuss, H. and Naylor, P. A. and Kellermann, W.},
  date = {2018-07},
  pages = {410--414},
  doi = {10.1109/SAM.2018.8448644},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q3XAHIFS\\Löllmann et al_2018_The LOCATA Challenge Data Corpus for Acoustic Source Localization and Tracking.pdf;C\:\\Users\\sauli\\Zotero\\storage\\B36T2UZK\\8448644.html},
  keywords = {acoustic signal processing,acoustic source localization,Acoustics,Array signal processing,Arrays,audio recording,audio signal processing,autonomous systems,Databases,hearing aids,IEEE-AASP Challenge,LOCATA Challenge data corpus,loudspeakers,microphone array,microphone arrays,Microphone arrays,numerous algorithms,signal classification,sound source localization,spherical microphone array,state-of-the-art algorithms,static microphone array,Task analysis,tele-conferencing systems}
}

@inproceedings{lollmannLOCATAChallengeData2018d,
  title = {The {{LOCATA Challenge Data Corpus}} for {{Acoustic Source Localization}} and {{Tracking}}},
  booktitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Lollmann, Heinrich W. and Evers, Christine and Schmidt, Alexander and Mellmann, Heinrich and Barfuss, Hendrik and Naylor, Patrick A. and Kellermann, Walter},
  date = {2018-07},
  pages = {410--414},
  publisher = {{IEEE}},
  location = {{Sheffield}},
  doi = {10.1109/SAM.2018.8448644},
  url = {https://ieeexplore.ieee.org/document/8448644/},
  urldate = {2019-07-29},
  abstract = {Algorithms for acoustic source localization and tracking are essential for a wide range of applications such as personal assistants, smart homes, tele-conferencing systems, hearing aids, or autonomous systems. Numerous algorithms have been proposed for this purpose which, however, are not evaluated and compared against each other by using a common database so far. The IEEE-AASP Challenge on sound source localization and tracking (LOCATA) provides a novel, comprehensive data corpus for the objective benchmarking of state-of-the-art algorithms on sound source localization and tracking. The data corpus comprises six tasks ranging from the localization of a single static sound source with a static microphone array to the tracking of multiple moving speakers with a moving microphone array. It contains real-world multichannel audio recordings, obtained by hearing aids, microphones integrated in a robot head, a planar and a spherical microphone array in an enclosed acoustic environment, as well as positional information about the involved arrays and sound sources represented by moving human talkers or static loudspeakers.},
  eventtitle = {2018 {{IEEE}} 10th {{Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\I46Y58AY\\Lollmann et al. - 2018 - The LOCATA Challenge Data Corpus for Acoustic Sour.pdf},
  isbn = {978-1-5386-4752-3},
  langid = {english}
}

@inproceedings{lollmannMicrophoneArraySignal2017,
  title = {Microphone Array Signal Processing for Robot Audition},
  booktitle = {2017 {{Hands}}-Free {{Speech Communications}} and {{Microphone Arrays}} ({{HSCMA}})},
  author = {Lollmann, Heinrich W. and Moore, AlastairH. and Naylor, Patrick A. and Rafaely, Boaz and Horaud, Radu and Mazel, Alexandre and Kellermann, Walter},
  date = {2017},
  pages = {51--55},
  publisher = {{IEEE}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/HSCMA.2017.7895560},
  url = {http://ieeexplore.ieee.org/document/7895560/},
  urldate = {2019-08-12},
  abstract = {Robot audition for humanoid robots interacting naturally with humans in an unconstrained real-world environment is a hitherto unsolved challenge. The recorded microphone signals are usually distorted by background and interfering noise sources (speakers) as well as room reverberation. In addition, the movements of a robot and its actuators cause ego-noise which degrades the recorded signals significantly. The movement of the robot body and its head also complicates the detection and tracking of the desired, possibly moving, sound sources of interest. This paper presents an overview of the concepts in microphone array processing for robot audition and some recent achievements.},
  eventtitle = {2017 {{Hands}}-Free {{Speech Communications}} and {{Microphone Arrays}} ({{HSCMA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EWWLQ7ME\\Lollmann et al. - 2017 - Microphone array signal processing for robot audit.pdf},
  isbn = {978-1-5090-5925-6},
  langid = {english}
}

@article{lopatka_acceleration_2014,
  title = {Acceleration of Decision Making in Sound Event Recognition Employing Supercomputing Cluster},
  author = {Lopatka, Kuba and Czyzewski, Andrzej},
  date = {2014-11-20},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {285},
  pages = {223--236},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2013.11.030},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025513008414},
  urldate = {2017-01-10},
  abstract = {Parallel processing of audio data streams is introduced to shorten the decision making time in hazardous sound event recognition. A supercomputing cluster environment with a framework dedicated to processing multimedia data streams in real time is used. The sound event recognition algorithms employed are based on detecting foreground events, calculating their features in short time frames, and classifying the events with Support Vector Machine. Different strategies for improving the decision time are introduced. The experiments with the presented strategies are conducted and the results are presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6I9CUZ5B\\Lopatka and Czyzewski - 2014 - Acceleration of decision making in sound event rec.pdf;C\:\\Users\\sauli\\Zotero\\storage\\V2GXBX37\\S0020025513008414.html},
  keywords = {Audio supercomputing,Audio surveillance,Sound event recognition},
  series = {Processing and {{Mining Complex Data Streams}}}
}

@article{lopatka_detection_2016,
  title = {Detection, Classification and Localization of Acoustic Events in the Presence of Background Noise for Acoustic Surveillance of Hazardous Situations},
  author = {Lopatka, K. and Kotus, J. and Czyzewski, A.},
  date = {2016-09-01},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {75},
  pages = {10407--10439},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-015-3105-4},
  url = {http://link.springer.com/article/10.1007/s11042-015-3105-4},
  urldate = {2017-01-10},
  abstract = {Evaluation of sound event detection, classification and localization of hazardous acoustic events in the presence of background noise of different types and changing intensities is presented. The methods for discerning between the events being in focus and the acoustic background are introduced. The classifier, based on a Support Vector Machine algorithm, is described. The set of features and samples used for the training of the classifier are introduced. The sound source localization algorithm based on the analysis of multichannel signals from the Acoustic Vector Sensor is presented. The methods are evaluated in an experiment conducted in the anechoic chamber, in which the representative events are played together with noise of differing intensity. The results of detection, classification and localization accuracy with respect to the Signal to Noise Ratio are discussed. The results show that the recognition and localization accuracy are strongly dependent on the acoustic conditions. We also found that the engineered algorithms provide a sufficient robustness in moderately intense noise in order to be applied to practical audio-visual surveillance systems.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JU9RPBB4\\Lopatka et al. - 2016 - Detection, classification and localization of acou.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IIKGK5H6\\10.html},
  langid = {english},
  number = {17}
}

@article{lopatkaApplicationVectorSensors2011,
  title = {Application of {{Vector Sensors}} to {{Acoustic Surveillance}} of a {{Public Interior Space}}},
  author = {Łopatka, Kuba and Kotus, Józef and Czyżewski, Andrzej},
  date = {2011-01-01},
  journaltitle = {Archives of Acoustics},
  volume = {36},
  issn = {0137-5075},
  doi = {10.2478/v10168-011-0056-2},
  url = {http://journals.pan.pl/dlibra/publication/120021/edition/104445/content},
  urldate = {2019-07-29},
  abstract = {A method for precise sound sources detection and localization in interiors is presented. Acoustic vector sensors, which provide multichannel output signals of acoustic pressure and particle velocity were employed. Methods for detecting acoustic events are introduced. The algorithm for localizing sound events in the audience is presented. The system set up in a lecture hall, which serves as a demonstrator of the proposed technology, is described. The accurracy of the proposed method is evaluated by the described measurement results. The analysis of the results is followed by conclusions pertaining the usability of the proposed system. The concept of the multimodal audio-visual detection of events in the audience is also introduced.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RWW85C6N\\Łopatka et al. - 2011 - Application of Vector Sensors to Acoustic Surveill.pdf},
  langid = {english},
  number = {4}
}

@article{luanDeepPhotoStyle2017,
  title = {Deep {{Photo Style Transfer}}},
  author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
  date = {2017-03-22},
  url = {http://arxiv.org/abs/1703.07511},
  urldate = {2017-03-27},
  abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
  archivePrefix = {arXiv},
  eprint = {1703.07511},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SP4ETERI\\Luan et al. - 2017 - Deep Photo Style Transfer.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EXW6TWR8\\1703.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{LuanDeepPhotoStyle2017,
  title = {Deep {{Photo Style Transfer}}},
  author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
  date = {2017-03},
  journaltitle = {arXiv:1703.07511 [cs]},
  abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
  archivePrefix = {arXiv},
  eprint = {1703.07511},
  eprintclass = {cs},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Z6ZEF2ZA\\Luan et al. - 2017 - Deep Photo Style Transfer.pdf;C\:\\Users\\sauli\\Zotero\\storage\\PK2AUGI4\\1703.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{luoTwoNewShrinkingCircle2018,
  title = {Two {{New Shrinking}}-{{Circle Methods}} for {{Source Localization Based}} on {{TDoA Measurements}}},
  author = {Luo, Mingzhi and Chen, Xiang and Cao, Shuai and Zhang, Xu},
  date = {2018-04},
  journaltitle = {SENSORS},
  volume = {18},
  publisher = {MDPI},
  location = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  issn = {1424-8220},
  doi = {\{10.3390/s18041274\}},
  abstract = {Time difference of arrival (TDoA) measurement is a promising approach for target localization based on a set of nodes with known positions, with high accuracy and low complexity. Common localization algorithms include the maximum-likelihood, non-linear least-squares and weighted least-squares methods. These methods have shortcomings such as high computational complexity, requiring an initial guess position, or having difficulty in finding the optimal solution. From the point of view of geometrical analysis, this study proposes two new shrinking-circle methods (SC-1 and SC-2) to solve the TDoA-based localization problem in a two-dimensional (2-D) space. In both methods, an optimal radius is obtained by shrinking the radius with a dichotomy algorithm, and the position of the target is determined by the optimal radius. The difference of the two methods is that a distance parameter is defined in SC-1, while an error function is introduced in SC-2 to guide the localization procedure. Simulations and indoor-localization experiments based on acoustic transducers were conducted to compare the performance differences between the proposed methods, algorithms based on weighted least-squares as well as the conventional shrinking-circle method. The experimental results demonstrate that the proposed methods can realize high-precision target localization based on TDoA measurements using three nodes, and have the advantages of speed and high robustness.},
  affiliation = {Chen, X (Reprint Author), Univ Sci & Technol China, Dept Elect Sci & Technol, Hefei 230026, Peoples R China. Luo, Mingzhi; Chen, Xiang; Cao, Shuai; Zhang, Xu, Univ Sci & Technol China, Dept Elect Sci & Technol, Hefei 230026, Peoples R China.},
  article-number = {1274},
  author-email = {lmz2514@mail.ustc.edu.cn xch@ustc.edu.cn caoshuai@ustc.edu.cn xuzhang90@ustc.edu.cn},
  cited-references = {Bull J. F., 2015, U. S. Patent, Patent No. [9121923B2, 9121923]. Chan YT, 2006, IEEE T VEH TECHNOL, V55, P10, DOI 10.1109/TVT.2005.861162. CHAN YT, 1994, IEEE T SIGNAL PROCES, V42, P1905, DOI 10.1109/78.301830. Cheung KW, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/20858. Chu YL, 2017, OPTIK, V131, P207, DOI 10.1016/j.ijleo.2016.11.094. Fowler ML, 2008, IEEE T AERO ELEC SYS, V44, P1543, DOI 10.1109/TAES.2008.4667729. FRIEDLANDER B, 1987, IEEE J OCEANIC ENG, V12, P234, DOI 10.1109/JOE.1987.1145216. Ge SS, 2014, IEEE J OCEANIC ENG, V39, P515, DOI 10.1109/JOE.2013.2271957. Gezici S, 2005, IEEE SIGNAL PROC MAG, V22, P70, DOI 10.1109/MSP.2005.1458289. Griffin A, 2015, SIGNAL PROCESS, V107, P54, DOI 10.1016/j.sigpro.2014.08.013. Ho KC, 2012, IEEE T SIGNAL PROCES, V60, P2101, DOI 10.1109/TSP.2012.2187283. Ho KC, 2004, IEEE T SIGNAL PROCES, V52, P2453, DOI 10.1109/TSP.2004.831921. Lin LX, 2013, SIGNAL PROCESS, V93, P2872, DOI 10.1016/j.sigpro.2013.04.004. Mandal A, 2005, CONSUM COMM NETWORK, P348. MARCHAND N, 1964, IEEE T AERO NAV ELEC, VAN11, P96, DOI 10.1109/TANE.1964.4502170. Mensing C., 2006, P IEEE INT C AC SPEE, pIV . Moutinho JN, 2016, PERVASIVE MOB COMPUT, V29, P1, DOI 10.1016/j.pmcj.2015.10.016. Qi YH, 2006, IEEE T WIREL COMMUN, V5, P672, DOI 10.1109/TWC.2006.03024. Qu XM, 2017, IEEE T SIGNAL PROCES, V65, P3990, DOI 10.1109/TSP.2017.2703667. Qu XM, 2016, SIGNAL PROCESS, V119, P142, DOI 10.1016/j.sigpro.2015.08.001. SCHMIDT RO, 1972, IEEE T AERO ELEC SYS, VAES8, P821, DOI 10.1109/TAES.1972.309614. Stoica P, 2006, IEEE SIGNAL PROC MAG, V23, P63, DOI 10.1109/SP-M.2006.248717. TORRIERI DJ, 1984, IEEE T AERO ELEC SYS, V20, P183, DOI 10.1109/TAES.1984.310439. Wang C, 2014, SIGNAL PROCESS, V94, P202, DOI 10.1016/j.sigpro.2013.06.005. Wu RS, 2018, RADIO SCI, V53, P129, DOI 10.1002/2017RS006389. Yeredor A, 2011, IEEE T SIGNAL PROCES, V59, P1612, DOI 10.1109/TSP.2010.2103069. Zekavat R., 2011, HDB POSITION LOCATIO, V27. Zhai XP, 2017, AD HOC SENS WIREL NE, V38, P1. Zhu MY, 2018, MULTIMED TOOLS APPL, V77, P3369, DOI 10.1007/s11042-017-5129-4.},
  da = {2018-10-18},
  doc-delivery-number = {GJ7NS},
  funding-acknowledgement = {National Key Research and Development Program of China [2016YFB0502202]},
  funding-text = {We would like to thank Yuheng Chen, Songyu Cong, ChiWu, and Lei Zhang for their help during the localization experiment. We also appreciate the support of the other fellows in our lab. This study was supported by the National Key Research and Development Program of China (No. 2016YFB0502202).},
  journal-iso = {Sensors},
  keywords = {(target localization,dichotomy),shrinking circle method,TDoA},
  keywords-plus = {FDOA MEASUREMENTS; ALGORITHM; LOCATION; NETWORK; SYSTEMS; SOUND},
  langid = {english},
  number = {4},
  number-of-cited-references = {29},
  oa = {gold},
  orcid-numbers = {Zhang, Xu/0000-0002-1533-4340},
  research-areas = {Chemistry; Electrochemistry; Instruments & Instrumentation},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000435574800346},
  usage-count-last-180-days = {6},
  usage-count-since-2013 = {6},
  web-of-science-categories = {Chemistry, Analytical; Electrochemistry; Instruments & Instrumentation}
}

@article{lurtonModellingSoundField2016,
  title = {Modelling of the Sound Field Radiated by Multibeam Echosounders for Acoustical Impact Assessment},
  author = {Lurton, Xavier},
  date = {2016-01-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {101},
  pages = {201--221},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2015.07.012},
  url = {http://www.sciencedirect.com/science/article/pii/S0003682X15002054},
  abstract = {Multi-Beam Echo-Sounders (MBES) designed for seafloor-mapping applications are today a major tool for ocean exploration and monitoring. Concerns have been raised about their impact towards marine life and especially marine mammals, although their inherent characteristics (high frequencies, short signals and narrow transmitting lobes) actually minimize this possibility. The present paper proposes an analysis of MBES radiation characteristics (pulse design, source level and radiation directivity pattern) accounting for the various geometries met today and expressed according to the metrics used for acoustical impact assessment (maximum Sound Pressure Level, and cumulative Sound Exposure Level). A detailed radiation model is proposed, including the transmission through directivity sidelobes, and applied to three typical MBES examples. A simplified radiation model is then defined, in order to extend it to the case of the cumulative insonification by a MBES moving along a survey line. An approximated analytical model is proposed for the accumulated intensity, showing good agreement with the complete simulation of insonification; it is applied to the worst-case configuration of a low-frequency (12kHz) multi-sector system. The computation of ranges corresponding to impact thresholds accepted today shows that impacts in terms of injury are negligible for both SPL and SEL; however behavioural response impacts cannot be excluded, and should require specific experimentation.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DKCFP5C9\\Lurton - 2016 - Modelling of the sound field radiated by multibeam.pdf;C\:\\Users\\sauli\\Zotero\\storage\\9U4X7JGU\\S0003682X15002054.html},
  issue = {Supplement C},
  keywords = {Directivity pattern,Marine mammals,Multibeam echosounders,Sonar impact assessment,Sound Exposure Level,Sound Pressure Level}
}

@online{MachineLearningSpectrogram,
  title = {Machine Learning Spectrogram Signal Separation\textbackslash{} - „{{Google}}“ Paieška},
  url = {https://www.google.com/search?client=firefox-b-ab&dcr=0&biw=1056&bih=492&tbm=isch&sa=1&ei=v9YdWt_BK8GB6QSHl5v4Cw&q=machine+learning+spectrogram+signal+separation%5C&oq=machine+learning+spectrogram+signal+separation%5C&gs_l=psy-ab.3...7988.32536.0.32549.48.39.3.0.0.0.608.3356.7j6j2j2j0j1.18.0....0...1c.1.64.psy-ab..30.9.990...0i19k1j0i24k1j0i8i30k1.0.j8qPGipBTrE#},
  urldate = {2017-11-28}
}

@article{maCompressionComputationalGrid2018,
  title = {Compression Computational Grid Based on Functional Beamforming for Acoustic Source Localization},
  author = {Ma, Wei and Liu, Xun},
  date = {2018-05-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {134},
  pages = {75--87},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2018.01.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0003682X17303900},
  urldate = {2018-10-19},
  abstract = {Phased microphone arrays have become a standard technique for acoustic source localization. Compared with beamforming algorithms such as the conventional beamforming, deconvolution approaches such as DAMAS successfully improve the spatial resolution. However deconvolution approaches usually require high computational effort compared to beamforming algorithms. Without optimizing deconvolution algorithm, recently DAMAS with compression computational grid based on the conventional beamforming (denoted by DAMAS-CG2) has reduced computational run time of DAMAS in applications (Ma and Liu, 2017). This paper proposes a novel algorithm that DAMAS with a novel compression computational grid based on an advance beamforming algorithm functional beamforming (denoted by DAMAS-CG3). This new algorithm takes advantages of functional beamforming to obtain large compression ratio. Simulated applications and experimental applications of benchmark test DLR1 show that DAMAS-CG3 is one order of magnitude faster than DAMAS-CG2 in most cases. In addition, the advantage of DAMAS-CG3 compared to DAMAS-CG2 is particularly more obvious with the threshold decreasing. However for some extreme situations that very complicated sources distribute to a larger extent relative to the scanning plane, the advantage of DAMAS-CG3 compared to DAMAS-CG2 may disappear. In order to get a large compression ratio in any application, the authors highly recommend compressing computational grid based on not only conventional beamforming but also functional beamforming, and then choosing the compression grid with larger compression ratio.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BIZE4QAU\\Ma and Liu - 2018 - Compression computational grid based on functional.pdf;C\:\\Users\\sauli\\Zotero\\storage\\37CHSMAG\\S0003682X17303900.html},
  keywords = {Acoustic,Beamforming,Compression grid,DAMAS,Microphone arrays}
}

@article{maCompressionComputationalGrid2018a,
  title = {Compression Computational Grid Based on Functional Beamforming for Acoustic Source Localization},
  author = {Ma, Wei and Liu, Xun},
  date = {2018-05},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {134},
  pages = {\{75-87\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2018.01.006\}},
  abstract = {Phased microphone arrays have become a standard technique for acoustic source localization. Compared with beamforming algorithms such as the conventional beamforming, deconvolution approaches such as DAMAS successfully improve the spatial resolution. However deconvolution approaches usually require high computational effort compared to beamforming algorithms. Without optimizing deconvolution algorithm, recently DAMAS with compression computational grid based on the conventional beamforming (denoted by DAMAS-CG2) has reduced computational run time of DAMAS in applications (Ma and Liu, 2017). This paper proposes a novel algorithm that DAMAS with a novel compression computational grid based on an advance beamforming algorithm functional beamforming (denoted by DAMAS-CG3). This new algorithm takes advantages of functional beamforming to obtain large compression ratio. Simulated applications and experimental applications of benchmark test DLR1 show that DAMAS-CG3 is one order of magnitude faster than DAMAS-CG2 in most cases. In addition, the advantage of DAMAS-CG3 compared to DAMAS-CG2 is particularly more obvious with the threshold decreasing. However for some extreme situations that very complicated sources distribute to a larger extent relative to the scanning plane, the advantage of DAMAS-CG3 compared to DAMAS-CG2 may disappear. In order to get a large compression ratio in any application, the authors highly recommend compressing computational grid based on not only conventional beamforming but also functional beamforming, and then choosing the compression grid with larger compression ratio.},
  affiliation = {Ma, W (Reprint Author), Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai, Peoples R China. Ma, Wei, Shanghai Jiao Tong Univ, Sch Aeronaut & Astronaut, Shanghai, Peoples R China. Liu, Xun, Shanghai KeyGo Technol Co Ltd, Shanghai, Peoples R China.},
  author-email = {mawei@sjtu.edu.cn},
  cited-references = {Ahlefeldt T, 2013, AIAA J, V51, P2783, DOI 10.2514/1.J052345. Bahr CJ, 2016, J SOUND VIB, V382, P13, DOI 10.1016/j.jsv.2016.06.044. Bahr CJ, 2017, 20173718 AIAA. Brooks T. F., 2006, AIAA PAPER, V2654, P2006. Brooks TF, 2006, J SOUND VIB, V294, P856, DOI 10.1016/j.jsv.2005.12.046. Brooks TF, 2005, 20052960 AIAA. Brooks TF, 2004, AIAA20042954. Chu ZG, 2017, APPL ACOUST, V123, P64, DOI 10.1016/j.apacoust.2017.03.010. Dougherty R, 2005, 20052961 AIAA. Dougherty RP, 2013, INT J AEROACOUST, V12, P699, DOI 10.1260/1475-472X.12.7-8.699. Dougherty RP, 1998, 4 AIAA CEAS AER C. Dougherty RP., 2016, 6 BERL BEAMF C 2016. Dougherty RP, 2014, 5 BERL BEAMF C 2014. Fernandez-Grande E, 2016, J ACOUST SOC AM, V139. Haxter S., 2016, BEBEC2016D8, P1. Huang X, 2012, J ACOUST SOC AM, V131, P2152, DOI 10.1121/1.3682041. Johnson D., 1993, ARRAY SIGNAL PROCESS. Lylloff O, 2015, J ACOUST SOC AM, V138, P172, DOI 10.1121/1.4922516. Ma W, 2017, J SOUND VIB, V410, P473, DOI 10.1016/j.jsv.2017.03.027. Ma W, 2017, J SOUND VIB, V395, P341, DOI 10.1016/j.jsv.2017.02.005. Michel U, 2006, P 1 BERL BEAMF C, P1. Ning F, 2015, SIGN PROC COMM COMP, P1. Pignier NJ, 2017, J SOUND VIB, V394, P203, DOI 10.1016/j.jsv.2017.01.051. Sarradj E, 2010, J SOUND VIB. Sijtsma Pieter, 2007, International Journal of Aeroacoustics, V6, P357, DOI 10.1260/147547207783359459. Sijtsma P., 2010, NLRTP2010549. Sijtsma P, 2004, NLRTP2004165. Suzuki T, 2011, J SOUND VIB, V330, P5835, DOI 10.1016/j.jsv.2011.05.021. Xenaki A, 2014, J ACOUST SOC AM, V136, P260, DOI 10.1121/1.4883360. Zhang Z, 2017, J SOUND VIB.},
  da = {2018-10-18},
  doc-delivery-number = {FX6WV},
  eissn = {1872-910X},
  funding-acknowledgement = {Natural Science Foundation of China [51506121]; “2011 Aero-Engine Collaboration Innovation Plan” of PR China},
  funding-text = {The authors would like to thank anonymous reviewers for their valuable comments that greatly contributed to improving the manuscript. The authors wish to express the sincere gratitude to Dr. Thomas Geyer of BTU Cottbus-Senftenberg, Germany for providing the login information of the DLR1 benchmark test. This work is supported by the Natural Science Foundation of China (Grant No. 51506121), and “2011 Aero-Engine Collaboration Innovation Plan” of PR China.},
  journal-iso = {Appl. Acoust.},
  keywords = {(Microphone arrays,Acoustic,Beamforming,Compression grid),DAMAS},
  keywords-plus = {SOUND SOURCE LOCALIZATION; PHASED MICROPHONE ARRAYS; AEROACOUSTIC MEASUREMENTS; DECONVOLUTION; DAMAS; EFFICIENCY; ALGORITHM},
  langid = {english},
  number-of-cited-references = {30},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000426228900010},
  usage-count-last-180-days = {13},
  usage-count-since-2013 = {16},
  web-of-science-categories = {Acoustics}
}

@inproceedings{maDepthLGPLearningEmbeddings2018,
  title = {{{DepthLGP}}: {{Learning Embeddings}} of {{Out}}-of-{{Sample Nodes}} in {{Dynamic Networks}}},
  shorttitle = {{{DepthLGP}}},
  booktitle = {{{AAAI}}},
  author = {Ma, Jianxin and Cui, Peng and Zhu, Wenwu},
  date = {2018},
  abstract = {Network embedding algorithms to date are primarily designed for static networks, where all nodes are known before learning. How to infer embeddings for out-of-sample nodes, i.e. nodes that arrive after learning, remains an open problem. The problem poses great challenges to existing methods, since the inferred embeddings should preserve intricate network properties such as high-order proximity, share similar characteristics (i.e. be of a homogeneous space) with in-sample node embeddings, and be of low computational cost. To overcome these challenges, we propose a Deeply Transformed High-order Laplacian Gaussian Process (DepthLGP) method to infer embeddings for out-of-sample nodes. DepthLGP combines the strength of nonparametric probabilistic modeling and deep learning. In particular, we design a high-order Laplacian Gaussian process (hLGP) to encode network properties, which permits fast and scalable inference. In order to further ensure homogeneity, we then employ a deep neural network to learn a nonlinear transformation from latent states of the hLGP to node embeddings. DepthLGP is general, in that it is applicable to embeddings learned by any network embedding algorithms. We theoretically prove the expressive power of DepthLGP, and conduct extensive experiments on real-world networks. Empirical results demonstrate that our approach can achieve significant performance gain over existing approaches.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FERD6E52\\Ma et al. - 2018 - DepthLGP Learning Embeddings of Out-of-Sample Nod.pdf},
  keywords = {Algorithm,Algorithmic efficiency,Artificial neural network,Computation,Deep learning,ENCODE,Experiment,Expressive power (computer science),Gaussian process,Nonlinear system,Scalability,Signalling System No. 7,Vocabulary}
}

@article{maExploitingDeepNeural2017,
  title = {Exploiting {{Deep Neural Networks}} and {{Head Movements}} for {{Robust Binaural Localization}} of {{Multiple Sources}} in {{Reverberant Environments}}},
  author = {Ma, Ning and May, Tobias and Brown, Guy J.},
  date = {2017-12},
  journaltitle = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  volume = {25},
  pages = {2444--2453},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2750760},
  url = {https://doi.org/10.1109/TASLP.2017.2750760},
  urldate = {2018-11-13},
  abstract = {This paper presents a novel machine-hearing system that exploits deep neural networks DNNs and head movements for robust binaural localization of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function CCF and interaural level differences ILDs. In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localization of sound sources in the frontal hemifield. Due to the similarity of binaural cues in the frontal and rear hemifields, front–back confusions often occur. To address this, a head movement strategy is incorporated in the localization model to help reduce the front–back errors. The proposed DNN system is compared to a Gaussian-mixture-model-based system that employs interaural time differences ITDs and ILDs as localization features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localization accuracies under challenging acoustic scenarios, in which multiple talkers and room reverberation are present.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\V2MK8V5L\\Ma et al. - 2017 - Exploiting Deep Neural Networks and Head Movements.pdf},
  number = {12}
}

@article{makhzaniAdversarialAutoencoders2016,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  date = {2016-05-24},
  url = {http://arxiv.org/abs/1511.05644},
  urldate = {2019-10-25},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  archivePrefix = {arXiv},
  eprint = {1511.05644},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6LAELK8V\\Makhzani et al. - 2016 - Adversarial Autoencoders.pdf;C\:\\Users\\sauli\\Zotero\\storage\\UVYSFAW5\\1511.html},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{malioutovSparseSignalReconstruction2005,
  title = {A Sparse Signal Reconstruction Perspective for Source Localization with Sensor Arrays},
  author = {Malioutov, D. and Cetin, M. and Willsky, A. S.},
  date = {2005-08},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {53},
  pages = {3010--3022},
  issn = {1053-587X},
  doi = {10.1109/TSP.2005.850882},
  abstract = {We present a source localization method based on a sparse representation of sensor measurements with an overcomplete basis composed of samples from the array manifold. We enforce sparsity by imposing penalties based on the ℓ1-norm. A number of recent theoretical results on sparsifying properties of ℓ1 penalties justify this choice. Explicitly enforcing the sparsity of the representation is motivated by a desire to obtain a sharp estimate of the spatial spectrum that exhibits super-resolution. We propose to use the singular value decomposition (SVD) of the data matrix to summarize multiple time or frequency samples. Our formulation leads to an optimization problem, which we solve efficiently in a second-order cone (SOC) programming framework by an interior point implementation. We propose a grid refinement method to mitigate the effects of limiting estimates to a grid of spatial locations and introduce an automatic selection criterion for the regularization parameter involved in our approach. We demonstrate the effectiveness of the method on simulated data by plots of spatial spectra and by comparing the estimator variance to the Crame´r-Rao bound (CRB). We observe that our approach has a number of advantages over other source localization techniques, including increased resolution, improved robustness to noise, limitations in data quantity, and correlation of the sources, as well as not requiring an accurate initialization.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5ZB9EHJ6\\Malioutov et al. - 2005 - A sparse signal reconstruction perspective for sou.pdf;C\:\\Users\\sauli\\Zotero\\storage\\379NBHCC\\1468495.html},
  keywords = {Acoustic sensors,array signal processing,automatic selection criterion,data matrix,direction-of-arrival estimation,grid refinement method,Maximum likelihood estimation,Multiple signal classification,Optimization,overcomplete representation,regularization parameter,second-order cone programming framework,sensor array processing,Sensor arrays,signal reconstruction,signal representation,Signal representations,signal resolution,signal sampling,signal superresolution,singular value decomposition,Source localization,source localization method,sparse representation,sparse signal reconstruction perspective,sparse signal representation,spatial resolution,spatial spectrum estimation,superresolution,time-frequency analysis},
  number = {8}
}

@book{malisauskasGarsoTechnika2005,
  title = {Garso technika},
  author = {Mališauskas, Vacius},
  date = {2005},
  publisher = {{Technika}},
  location = {{Vilnius}},
  abstract = {Leidinyje nagrinėjama elektroninė technika, sukurianti ore garso bangas. Pradžioje aptariami garso ore duomenys ir žmonių pojūčiai, atsirandantys veikiant garsui klausos organus. Toliau nagrinėjamos elektroakustinės sistemos bei jų dalys – garso technika, kuriai skiriama didžioji knygos dalis.

 

Knygoje analizuojama garso signalų įrašymo, perdavimo ir atkūrimo profesionalioji bei buitinė analoginė ir skaitmeninė technika. Kompiuterinė garso technika priklauso multimedijos įrenginių grupei ir čia tik paminima. Įvade trumpai aptariama buitinė High End garso technika, tačiau didžiausias dėmesys leidinyje skiriamas populiariosios ir Hi-Fi klasių technikai, su kuria dažniausiai susiduriama. Siekiama kuo paprasčiau ir aiškiau, bet nuosekliai ir gana išsamiai išdėstyti minėtosios garso technikos veikimo principus, remiantis visų pirma struktūrinėmis schemomis. Nemažai vietos skiriama techniniams duomenims paaiškinti, jiems suderinti ir palyginti.

Mokomoji knyga parengta VGTU Elektronikos fakulteto studentams, studijuojantiems garso ir vaizdo techniką (modulis ELREB 8002), bet gali būti naudinga ir kitų aukštųjų mokyklų studentams bei visiems, besidomintiems profesionaliąja ir buitine garso technika.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZNN64KVR\\Mališauskas - 2005 - Garso technika.pdf},
  isbn = {9986-05-665-9},
  langid = {lithuanian}
}

@article{maoWirelessSensorNetwork2007,
  title = {Wireless Sensor Network Localization Techniques},
  author = {Mao, Guoqiang and Fidan, Barış and Anderson, Brian D. O.},
  date = {2007-07-11},
  journaltitle = {Computer Networks},
  shortjournal = {Computer Networks},
  volume = {51},
  pages = {2529--2553},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2006.11.018},
  url = {http://www.sciencedirect.com/science/article/pii/S1389128606003227},
  urldate = {2019-09-19},
  abstract = {Wireless sensor network localization is an important area that attracted significant research interest. This interest is expected to grow further with the proliferation of wireless sensor network applications. This paper provides an overview of the measurement techniques in sensor network localization and the one-hop localization algorithms based on these measurements. A detailed investigation on multi-hop connectivity-based and distance-based localization algorithms are presented. A list of open research problems in the area of distance-based sensor network localization is provided with discussion on possible approaches to them.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B6BNJUW5\\Mao et al_2007_Wireless sensor network localization techniques.pdf;C\:\\Users\\sauli\\Zotero\\storage\\WK32QD99\\S1389128606003227.html},
  keywords = {AOA,Localization,RSS,TDOA,Wireless sensor networks},
  number = {10}
}

@article{maRobustBinauralLocalization2018,
  title = {Robust {{Binaural Localization}} of a {{Target Sound Source}} by {{Combining Spectral Source Models}} and {{Deep Neural Networks}}},
  author = {Ma, N. and Gonzalez, J. A. and Brown, G. J.},
  date = {2018-11},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {2122--2131},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2855960},
  abstract = {Despite there being a clear evidence for top-down (e.g., attentional) effects in biological spatial hearing, relatively few machine hearing systems exploit the top-down model-based knowledge in sound localization. This paper addresses this issue by proposing a novel framework for the binaural sound localization that combines the model-based information about the spectral characteristics of sound sources and deep neural networks (DNNs). A target source model and a background source model are first estimated during a training phase using spectral features extracted from sound signals in isolation. When the identity of the background source is not available, a universal background model can be used. During testing, the source models are used jointly to explain the mixed observations and improve the localization process by selectively weighting source azimuth posteriors output by a DNN-based localization system. To address the possible mismatch between the training and testing, a model adaptation process is further employed the on-the-fly during testing, which adapts the background model parameters directly from the noisy observations in an iterative manner. The proposed system, therefore, combines the model-based and data-driven information flow within a single computational framework. The evaluation task involved localization of a target speech source in the presence of an interfering source and room reverberation. Our experiments show that by exploiting the model-based information in this way, the sound localization performance can be improved substantially under various noisy and reverberant conditions.},
  keywords = {acoustic signal processing,Adaptation models,Auditory system,Azimuth,background model parameters,background source model,binaural sound localization,Binaural source localisation,biological spatial hearing,Biological system modeling,Computational modeling,deep neural networks,DNN-based localization system,feature extraction,hearing,localization process,machine hearing,machine hearing systems,masking,model adaptation process,model-based knowledge,neural nets,reverberation,robust binaural localization,room reverberation,sound localization performance,sound signals,sound source combination,source azimuth posteriors output,spectral analysis,spectral characteristics,spectral feature extraction,spectral source models,Speech processing,target sound source,target speech source,THIS,Time-frequency analysis},
  number = {11}
}

@article{maRobustBinauralLocalization2018a,
  title = {Robust {{Binaural Localization}} of a {{Target Sound Source}} by {{Combining Spectral Source Models}} and {{Deep Neural Networks}}},
  author = {Ma, Ning and Gonzalez, Jose A. and Brown, Guy J.},
  date = {2018-11},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{2122-2131\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2855960\}},
  abstract = {Despite there being a clear evidence for top-down (e.g., attentional) effects in biological spatial hearing, relatively few machine hearing systems exploit the top-down model-based knowledge in sound localization. This paper addresses this issue by proposing a novel framework for the binaural sound localization that combines the model-based information about the spectral characteristics of sound sources and deep neural networks (DNNs). A target source model and a background source model are first estimated during a training phase using spectral features extracted from sound signals in isolation. When the identity of the background source is not available, a universal background model can be used. During testing, the source models are used jointly to explain the mixed observations and improve the localization process by selectively weighting source azimuth posteriors output by a DNN-based localization system. To address the possible mismatch between the training and testing, a model adaptation process is further employed the on-the-fly during testing, which adapts the background model parameters directly from the noisy observations in an iterative manner. The proposed system, therefore, combines the model-based and data-driven information flow within a single computational framework. The evaluation task involved localization of a target speech source in the presence of an interfering source and room reverberation. Our experiments show that by exploiting the model-based information in this way, the sound localization performance can be improved substantially under various noisy and reverberant conditions.},
  affiliation = {Ma, N (Reprint Author), Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England. Ma, Ning; Brown, Guy J., Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England. Gonzalez, Jose A., Univ Sheffield, Sheffield S1 4DP, S Yorkshire, England. Gonzalez, Jose A., Univ Malaga, Dept Languages & Comp Sci, E-29071 Malaga, Spain.},
  author-email = {n.ma@sheffield.ac.uk j.gonzalez@uma.es g.j.brown@sheffield.ac.uk},
  cited-references = {Barker JP, 2005, SPEECH COMMUN, V45, P5, DOI 10.1016/j.specom.2004.05.002. Blauert J., 1997, SPATIAL HEARING PSYC. Bregman A. S., 1990, AUDITORY SCENE ANAL. Bronkhorst AW, 2015, ATTEN PERCEPT PSYCHO, V77, P1465, DOI 10.3758/s13414-015-0882-9. BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016. Christensen H, 2009, INT CONF ACOUST SPEE, P4593, DOI 10.1109/ICASSP.2009.4960653. Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005. DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1. Gaese BH, 2002, ZOOLOGY, V105, P329, DOI 10.1078/0944-2006-00070. Garofolo J. S., 1993, DARPA TIMIT ACOUSTIC, V93, P27403. Gonzalez JA, 2017, CIRC SYST SIGNAL PR, V36, P3731, DOI 10.1007/s00034-016-0480-7. Hummersone C, 2010, IEEE T AUDIO SPEECH, V18, P1867, DOI 10.1109/TASL.2010.2051354. Liu JD, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1251, DOI 10.1109/IJCNN.2011.6033367. Ma N., 2015, P INTERSPEECH 2015 D, P3302. Ma N, 2017, IEEE-ACM T AUDIO SPE, V25, P2444, DOI 10.1109/TASLP.2017.2750760. Ma N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P160. Ma N, 2013, COMPUT SPEECH LANG, V27, P820, DOI 10.1016/j.csl.2012.09.001. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711. May T., 2013, TECHNOLOGY BINAURAL, P397. May T, 2015, INT CONF ACOUST SPEE, P2679, DOI 10.1109/ICASSP.2015.7178457. May T, 2011, IEEE T AUDIO SPEECH, V19, P1, DOI 10.1109/TASL.2010.2042128. Reddy A. M., 2004, P ISCA TUT RES WORKS, P158. Reddy AM, 2007, IEEE T AUDIO SPEECH, V15, P1766, DOI 10.1109/TASL.2007.901310. Remes U, 2016, COMPUT SPEECH LANG, V35, P14, DOI 10.1016/j.csl.2015.06.005. Rennie SJ, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.938081. SPENCE CJ, 1994, J EXP PSYCHOL HUMAN, V20, P555, DOI 10.1037/0096-1523.20.3.555. VARGA AP, 1990, INT CONF ACOUST SPEE, P845, DOI 10.1109/ICASSP.1990.115970. Wang D. L., 2006, COMPUTATIONAL AUDITO. Wierstorf H., 2011, P 130 AUD ENG SOC CO. Winkowski DE, 2006, NATURE, V439, P336, DOI 10.1038/nature04411. Woodruff J, 2013, IEEE T AUDIO SPEECH, V21, DOI 10.1109/TASL.2012.2236316. Woodruff J, 2012, IEEE T AUDIO SPEECH, V20, P1503, DOI 10.1109/TASL.2012.2183869.},
  da = {2018-10-18},
  doc-delivery-number = {GQ1VQ},
  funding-acknowledgement = {EC [618075]},
  funding-text = {This work was supported by the EC FP7 project TWO!EARS under Grant 618075.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {binaural,Binaural source localisation,machine hearing,masking),reverberation,sound  source combination},
  keywords-plus = {REVERBERANT ENVIRONMENTS; SPEECH RECOGNITION; SOURCE SEPARATION; MULTIPLE SOURCES},
  langid = {english},
  number = {11},
  number-of-cited-references = {32},
  oa = {green_published},
  orcid-numbers = {Gonzalez Lopez, Jose Andres/0000-0002-5531-8994 Brown, Guy/0000-0001-8565-5476 Ma, Ning/0000-0002-4112-3109},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000441430600014},
  usage-count-last-180-days = {11},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{martiSpeakerLocalizationDetection2011,
  title = {Speaker {{Localization}} and {{Detection}} in {{Videoconferencing Environments Using}} a {{Modified SRP}}-{{PHAT Algorithm}}},
  author = {Marti, A and Cobos, M and Aguilera, E and Lopez, J J},
  date = {2011},
  pages = {8},
  abstract = {The Steered Response Power - Phase Transform (SRP-PHAT) algorithm has been shown to be one of the most robust sound source localization approaches operating in noisy and reverberant environments. However, its practical implementation is usually based on a costly fine grid-search procedure, making the computational cost of the method a real issue. In this paper, we introduce an effective strategy which performs a full exploration of the sampled space rather than computing the SRP at discrete spatial positions, increasing its robustness and allowing for a coarser spatial grid that reduces the computational cost required in a practical implementation. The modified SRP-PHAT functional has been successfully implemented in a real time speaker localization system for multiparticipant videoconferencing environments. Moreover, a localization-based speech-non speech frame discriminator is presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G73BAHMI\\Marti et al. - 2011 - Speaker Localization and Detection in Videoconfere.pdf},
  langid = {english}
}

@inproceedings{mastersonAcousticImpulseResponse2009,
  title = {Acoustic {{Impulse Response Interpolation}} for {{Multichannel Systems Using Dynamic Time Warping}}},
  author = {Masterson, Claire and Kearney, Gavin and Boland, Frank},
  date = {2009-02-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15188},
  urldate = {2018-09-18},
  abstract = {We present a method of interpolation of room acoustic impulse responses for reducing measurement sets in multichannel convolution systems. The method employs the use of Dynamic Time Warping for the synthesis of early reflections and critical band analysis for diffuse decay decorrelation. An objective study of the reduced data sets in comparison to full spatial resolution measurements taken in a reverberant environment is presented through binaural measurements. Listening tests are conducted...},
  eventtitle = {Audio {{Engineering Society Conference}}: 35th {{International Conference}}: {{Audio}} for {{Games}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UBAUV8MY\\browse.html},
  langid = {english}
}

@article{matassoniFBKSystemCHiME42016,
  title = {The {{FBK}} System for the {{CHiME}}-4 Challenge},
  author = {Matassoni, Marco and Ravanelli, Mirco and Jalalvand, Shahab and Brutti, Alessio and Falavigna, Daniele},
  date = {2016},
  pages = {5},
  abstract = {This paper describes the ASR system submitted by FBK to the CHiME-4 challenge for the single channel track. The proposed solution employs multiple subsystems, whose DNNs are trained with different training criteria and strategies (i.e. diverse training material, with and without batch normalization). A “self” adaptation of acoustic models is applied to each subsystem, relying on a blind estimate of the accuracy of automatic transcriptions. This adaptation, performed in a batch fashion over the entire evaluation set, significantly improves the performance of each subsystem. The final output is obtained by combining the multiple transcriptions through ROVER, which provides a further improvement, reducing the average WER on the evaluation set from 22.3\% to 16.5\%.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZR7VX5AX\\Matassoni et al. - The FBK system for the CHiME-4 challenge.pdf},
  langid = {english}
}

@online{MathInverseBilinear,
  title = {Math - {{Inverse Bilinear Interpolation}}?},
  journaltitle = {Stack Overflow},
  url = {https://stackoverflow.com/questions/808441/inverse-bilinear-interpolation},
  urldate = {2018-06-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7ZUMMMBS\\inverse-bilinear-interpolation.html}
}

@software{matiisenGymminecraftMinecraftEnvironment2017,
  title = {Gym-Minecraft: {{Minecraft}} Environment for {{Open AI Gym}}, Based on {{Microsoft}}'s {{Malmo}}},
  shorttitle = {Gym-Minecraft},
  author = {Matiisen, Tambet},
  date = {2017-11-24T12:22:23Z},
  origdate = {2016-10-01T02:01:18Z},
  url = {https://github.com/tambetm/gym-minecraft},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9Q2ZBTEJ\\gym-minecraft.html}
}

@online{MatricesTrilaterationUsing,
  title = {Matrices - {{Trilateration Using TDOA}}},
  journaltitle = {Mathematics Stack Exchange},
  url = {https://math.stackexchange.com/questions/1722021/trilateration-using-tdoa},
  urldate = {2018-11-23},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XZSKV25J\\trilateration-using-tdoa.html}
}

@article{matsumotoMethodInterpolatingBinaural2003,
  title = {A Method of Interpolating Binaural Impulse Responses for Moving Sound Images},
  author = {Matsumoto, Mitsuo and Tohyama, Mikio and Yanagawa, Hirofumi},
  date = {2003},
  journaltitle = {Acoustical Science and Technology},
  volume = {24},
  pages = {284--292},
  issn = {1346-3969, 1347-5177},
  doi = {10.1250/ast.24.284},
  url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/ast/24.284?from=CrossRef},
  urldate = {2018-09-18},
  abstract = {Previously introduced method of interpolating binaural implse responses and algorithm to simulate a moving sound image were evaluated objectively. The method interpolates the responses taking into account the arrival time difference due to changes in the direction of a moving sound source. For the angular interval of 15 , the average of the SDR values of our method, 23 dB was larger than that of the simple method, 9.9 dB. The variances of the SDR values showed our method interpolated the responses more independently of the azimuths of the sound source than the simple method. The responses interpolated using our method changed smoothly as the source direction changed. We have evaluated the algorithm by comparing a moving sound image simulated using the algorithm with an actual moving sound image recorded using a rotating dummy head and with a moving sound image simulated using a conventional method. The spectrogram of the binaural signal of the moving sound image, and no ripples were seen.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GFI3DP96\\Matsumoto et al. - 2003 - A method of interpolating binaural impulse respons.pdf},
  langid = {english},
  number = {5}
}

@article{mayProbabilisticModelRobust2011,
  title = {A {{Probabilistic Model}} for {{Robust Localization Based}} on a {{Binaural Auditory Front}}-{{End}}},
  author = {May, T. and van de Par, S. and Kohlrausch, A.},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {19},
  pages = {1--13},
  issn = {1558-7916},
  doi = {10.1109/TASL.2010.2042128},
  abstract = {Although extensive research has been done in the field of machine-based localization, the degrading effect of reverberation and the presence of multiple sources on localization performance has remained a major problem. Motivated by the ability of the human auditory system to robustly analyze complex acoustic scenes, the associated peripheral stage is used in this paper as a front-end to estimate the azimuth of sound sources based on binaural signals. One classical approach to localize an acoustic source in the horizontal plane is to estimate the interaural time difference (ITD) between both ears by searching for the maximum in the cross-correlation function. Apart from ITDs, the interaural level difference (ILD) can contribute to localization, especially at higher frequencies where the wavelength becomes smaller than the diameter of the head, leading to ambiguous ITD information. The interdependency of ITD and ILD on azimuth is a complex pattern that depends also on the room acoustics, and is therefore learned by azimuth-dependent Gaussian mixture models (GMMs). Multiconditional training is performed to take into account the variability of the binaural features which results from multiple sources and the effect of reverberation. The proposed localization model outperforms state-of-the-art localization techniques in simulated adverse acoustic conditions.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PBUC4DNB\\May et al. - 2011 - A Probabilistic Model for Robust Localization Base.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SX8FU8EK\\5406118.html},
  keywords = {acoustic source localisation,associated peripheral stage,audio signal processing,auditory scene analysis (ASA),Auditory system,Azimuth,binaural,binaural auditory front-end,correlation methods,cross-correlation function,Degradation,Ear,estimation theory,Frequency,Gaussian mixture models,Gaussian processes,human auditory system,Humans,interaural level difference,interaural level difference (ILD),interaural time difference,interaural time difference (ITD),Layout,Localization,localization performance,machine-based localization,probabilistic model,probability,reverberation,Reverberation,robust localization,Robustness,Signal analysis,sound sources,VERY IMPORTANT},
  number = {1}
}

@article{mccowanMicrophoneArraysTutorial,
  title = {Microphone {{Arrays}} : {{A Tutorial}}},
  author = {McCowan, Iain},
  pages = {36},
  abstract = {This report presents a tutorial of fundamental array processing and beamforming theory relevant to microphone array speech processing. A microphone array consists of multiple microphones placed at different spatial locations. Built upon a knowledge of sound propagation principles, the multiple inputs can be manipulated to enhance or attenuate signals emanating from particular directions. In this way, microphone arrays provide a means of enhancing a desired signal in the presence of corrupting noise sources. Moreover, this enhancement is based purely on knowledge of the source location, and so microphone array techniques are applicable to a wide variety of noise types. Microphone arrays have great potential in practical applications of speech processing, due to their ability to provide both noise robustness and hands-free signal acquisition.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EEJ9TWP3\\microphone_array.pdf},
  langid = {english}
}

@article{mccowanRobustSpeakerRecognition,
  title = {Robust {{Speaker Recognition}} Using {{Microphone Arrays}}},
  author = {McCowan, Iain A and Pelecanos, Jason and Sridharan, Sridha},
  pages = {6},
  abstract = {This paper investigates the use of microphone arrays in handsfree speaker recognition systems. Hands-free operation is preferable in many potential speaker recognition applications, however obtaining acceptable performance with a single distant microphone is problematic in real noise conditions. A possible solution to this problem is the use of microphone arrays, which have the capacity to enhance a signal based purely on knowledge of its direction of arrival. The use of microphone arrays for improving the robustness of speech recognition systems has been studied in recent times, however little research has been conducted in the area of speaker recognition. This paper discusses the application of microphone arrays to speaker recognition applications, and presents an experimental evaluation of a hands-free speaker verification application in noisy conditions.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YDZM58HG\\McCowan et al. - Robust Speaker Recognition using Microphone Arrays.pdf},
  keywords = {***},
  langid = {english}
}

@book{mcdermottAirMonitoringToxic2004,
  title = {Air Monitoring for Toxic Exposures},
  author = {McDermott, Henry J. and Ness, Shirley A.},
  date = {2004},
  edition = {2nd ed},
  publisher = {{Wiley-Interscience}},
  location = {{Hoboken, N.J}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CPCBCWY3\\McDermott and Ness - 2004 - Air monitoring for toxic exposures.pdf},
  isbn = {978-0-471-45435-9},
  keywords = {Air,Air sampling apparatus,Biological monitoring,Pollution Measurement},
  pagetotal = {688}
}

@inproceedings{mehrotraInterpolationCombinedHead2011,
  title = {Interpolation of Combined Head and Room Impulse Response for Audio Spatialization},
  booktitle = {2011 {{IEEE}} 13th {{International Workshop}} on {{Multimedia Signal Processing}}},
  author = {Mehrotra, S. and Chen, W. and Zhang, Z.},
  date = {2011-10},
  pages = {1--6},
  doi = {10.1109/MMSP.2011.6093794},
  abstract = {Audio spatialization is becoming an important part of creating realistic experiences needed for immersive video conferencing and gaming. Using a combined head and room impulse response (CHRIR) has been recently proposed as an alternative to using separate head related transfer functions (HRTF) and room impulse responses (RIR). Accurate measurements of the CHRIR at various source and listener locations and orientations are needed to perform good quality audio spatialization. However, it is infeasible to accurately measure or model the CHRIR for all possible locations and orientations. Therefore, low-complexity and accurate interpolation techniques are needed to perform audio spatialization in real-time. In this paper, we present a frequency domain interpolation technique which naturally interpolates the interaural level difference (ILD) and interaural time difference (ITD) for each frequency component in the spectrum. The proposed technique allows for an accurate and low-complexity interpolation of the CHRIR as well as allowing for a low-complexity audio spatialization technique which can be used for both headphones as well as loudspeakers.},
  eventtitle = {2011 {{IEEE}} 13th {{International Workshop}} on {{Multimedia Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9JHVFTDM\\Mehrotra et al. - 2011 - Interpolation of combined head and room impulse re.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5PCUYFU9\\6093794.html},
  keywords = {audio equipment,CHRIR,Ear,frequency component,Frequency domain analysis,Frequency response,gaming,head related transfer functions,headphones,HRTF,ILD,immersive video con¬ ferencing,interaural level difference,interaural time difference,interpolation,Interpolation,ITD,loudspeakers,Loudspeakers,low-complexity audio spatialization technique,room impulse response interpolation technique,Time domain analysis,transfer functions,transient response}
}

@inproceedings{mengOptimalSensorPairing2012,
  title = {Optimal Sensor Pairing for {{TDOA}} Based Source Localization and Tracking in Sensor Networks},
  booktitle = {2012 15th {{International Conference}} on {{Information Fusion}}},
  author = {Meng, W. and Xie, L. and Xiao, W.},
  date = {2012-07},
  pages = {1897--1902},
  abstract = {Source localization based on time-difference-of-arrival (TDOA) measurements from spatially separated sensors is an important problem in sensor networks. While extensive research has been performed on algorithm development, limited attention has been paid to sensor geometry design. In this paper, we study the optimal sensor pair geometry for the TDOA based source localization problem. Analytic solutions to the optimal sensor pair geometries, for both static and movable source cases, are derived when there exist no communication constraints. Furthermore, in many applications, sensor platforms such as unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) are movable, and their movements and the communications between sensors are constrained. The problem becomes how to optimize the trajectories for the moving platforms such that optimal source localization and tracking can be achieved. We extend our work to optimal sensor path planning and cast it as a constrained nonlinear optimization problem. The sequential quadratic programming (SQP) method is adopted for a solution. Computer simulations demonstrate good localization performance.},
  eventtitle = {2012 15th {{International Conference}} on {{Information Fusion}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\54XEVZYP\\Meng et al. - 2012 - Optimal sensor pairing for TDOA based source local.pdf;C\:\\Users\\sauli\\Zotero\\storage\\RVC4L7SW\\6290532.html},
  keywords = {algorithm development,constrained nonlinear optimization problem,constraint handling,Correlation,Covariance matrix,Cramer-Rao bound (CRB),Estimation,Fisher information matrix,Geometry,moving platforms,optimal sensor pair geometry,optimal sensor pairing,optimal sensor path planning,Optimization,path planning,Position measurement,quadratic programming,sensor fusion,sensor geometry design,sensor networks,sensor placement,sensor platforms,sequential quadratic programming,source cases,source localization,spatially separated sensors,TDOA based source localization,time-difference-of-arrival (TDOA),time-difference-of-arrival measurements,time-of-arrival estimation,tracking,Trajectory,unmanned aerial vehicles,unmanned ground vehicles,Wireless sensor network (WSN)}
}

@article{mengSynthesisModelMoving2018,
  title = {A {{Synthesis Model}} for a {{Moving Sound Source Based}} on {{Beamforming}}},
  author = {Meng, Fanyu and Behler, Gottfried and Vorlaender, Michael},
  date = {2018-03/2018-04},
  journaltitle = {ACTA ACUSTICA UNITED WITH ACUSTICA},
  volume = {104},
  pages = {\{351-362\}},
  publisher = {S HIRZEL VERLAG},
  location = {POSTFACH 10 10 61, D-70 009 STUTTGART, GERMANY},
  issn = {1610-1928},
  doi = {\{10.3813/AAA.919177\}},
  abstract = {Moving sound sources are essential parts of outdoor environmental auralization. The moving vehicles, e.g. cars and trains, can be modeled by separable sound sources, which are represented by their locations and signals. A measurement-based synthesis model is developed to characterize moving sound sources regarding the two representations in this paper. This model utilizes beamforming to obtain the locations and signals of moving sound sources. In this paper, the target sound source is a periodic signal, and a spiral microphone array is used in the model. First of all, the Doppler effect in the recording is eliminated by the time domain de-Dopplerization technique. Furthermore, delay and sum beamforming is modified for moving sound sources to process the de-Dopplerized signals. Subsequently, the source positions are acquired by scanning the reconstruction plane. Thus, with steering the array to the corresponding positions, the beamforming outputs can be regarded as the potential signal s of the sources. Additionally, the model is evaluated by errors of localization, frequency and level with varying parameters, i.e. steering position, steering window length and source speed. Finally, validation is conducted on the model by a pass-by measurement with the periodic signal from a loudspeaker installed on a car.},
  affiliation = {Meng, FY (Reprint Author), Rhein Westfal TH Aachen, Inst Tech Acoust, D-52074 Aachen, Germany. Meng, Fanyu; Behler, Gottfried; Vorlaender, Michael, Rhein Westfal TH Aachen, Inst Tech Acoust, D-52074 Aachen, Germany.},
  author-email = {fme@akustik.rwth-aachen.de},
  cited-references = {Arntzen M, 2014, APPL ACOUST, V84, P99, DOI 10.1016/j.apacoust.2013.09.002. BARSIKOW B, 1987, J SOUND VIB, V118, P99, DOI 10.1016/0022-460X(87)90257-4. Bongini E., 2007, P 9 INT WORKSH RAILW, P320. Bruhl S, 2000, J SOUND VIB, V231, P611, DOI 10.1006/jsvi.1999.2548. Camargo H., 2010, THESIS. Christensen JJ, 2004, TECHNICAL REV BEAMFO. Fleury V, 2011, J ACOUST SOC AM, V129, P1417, DOI 10.1121/1.3531939. HOWELL GP, 1986, J SOUND VIB, V105, P151, DOI 10.1016/0022-460X(86)90227-0. Jagla J, 2012, J ACOUST SOC AM, V132, P3098, DOI 10.1121/1.4754663. Johnson D. H., 1993, ARRAY SIGNAL PROCESS, P111. Kang J., 2006, URBAN SOUND ENV, P199. Klemenz M, 2005, ACTA ACUST UNITED AC, V91, P779. Kook H, 2000, J SOUND VIB, V233, P137, DOI 10.1006/jsvi.1999.2794. Kuttruff H., 2007, ACOUSTICS INTRO, P69. Le Courtois F, 2016, J SOUND VIB, V371, P78, DOI 10.1016/j.jsv.2016.02.004. Meng F., 2015, EURONOISE 2015, P1703. Meng F., 2016, DATA 42 JAHR AK AACH, P42. Miner N., 1998, THESIS. Morse P. M., 1968, THEORETICAL ACOUSTIC, P717. Peplow A., 2011, P EUR C AC FOR AC 20, V27, P665. Pieren Reto, 2016, Applied Sciences, V6, DOI 10.3390/app6010005. Pieren R, 2017, APPL ACOUST, V127, P34, DOI 10.1016/j.apacoust.2017.05.026. Pieren R, 2014, ACTA ACUST UNITED AC, V100, P25, DOI 10.3813/AAA.918683. Rietdijk F., 2015, P 10 EUR C NOIS CONT, V31, P781. Rizzi S. A., 2005, 11 AIAA CEAS AER C, V4, P2291. Sahai A., 2012, P 18 AIAA CEAS AEROA. Sarrazin M., 2012, VIRTUAL CAR SOUND SY. SERRA X, 1990, COMPUT MUSIC J, V14, P12, DOI 10.2307/3680788. Shin H., 2006, 12 AIAA CEAS AER C, P8. Stienen J., 2017, DAGA 43 JAHR AK KIEL, P43. Van Trees H. L., 2002, OPTIMUM ARRAY PROC 4, P17. Vorlander M, 2008, RWTHEDITION, P1. Yang DG, 2011, J SOUND VIB, V330, P1352, DOI 10.1016/j.jsv.2010.10.001. Zwicker E., 2013, PSYCHOACOUSTICS FACT, P175.},
  da = {2018-10-18},
  doc-delivery-number = {GA3VS},
  eissn = {1861-9959},
  journal-iso = {Acta Acust. United Acust.},
  keywords-plus = {AIRCRAFT FLYOVER NOISE; HIGH-SPEED TRAIN; EMISSION SYNTHESIS; ARRAY; AURALIZATION; VEHICLES},
  langid = {english},
  number = {2},
  number-of-cited-references = {34},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000428258600019},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics}
}

@online{MethodologySoundSource,
  title = {A Methodology for Sound Source Localization and Tracking: {{Development}} of {{3D}} Microphone Array for near-Field and Far-Field Applications - {{IEEE Xplore Document}}},
  url = {http://ieeexplore.ieee.org/document/7429936/},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WWPS8P3W\\7429936.html}
}

@online{MethodologySoundSourcea,
  title = {A Methodology for Sound Source Localization and Tracking: {{Development}} of {{3D}} Microphone Array for near-Field and Far-Field Applications - {{IEEE Xplore Document}}},
  url = {http://ieeexplore.ieee.org/document/7429936/},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WIXQHARH\\7429936.html}
}

@article{michelHistoryAcousticBeamforming,
  title = {History of {{Acoustic Beamforming}}},
  author = {Michel, Ulf},
  pages = {17},
  abstract = {The development of the beamforming method (also called microphone antenna, phased array of microphones, acoustic telescope, or acoustic camera) is reviewed in this paper. The microphone antenna was invented by Billingsley (1974) and has since seen dramatic improvements due to the availability of better data acquisition and computing hardware. Recent mathematical and software developments invert the beamforming process and allow a quantitative determination of the sources. Beamforming is indispensable for the localization of sound sources on moving objects, on flying aircraft, on high-speed trains, on motor cars in motion, on open rotors like helicopter and wind turbine rotors. In these applications, the ability to follow the motion of the sources is important. The second important applications are source localization tests in the test sections of open and closed wind tunnels. The background noise suppression capability of the beamforming method is required here. The various applications are discussed with a long list of references.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XR3JSRPE\\Michel - HISTORY OF ACOUSTIC BEAMFORMING.pdf},
  langid = {english}
}

@online{MicrophoneArrayProcessing,
  title = {Microphone {{Array Processing}} for {{Distant Speech Recognition}}: {{From Close}}-{{Talking Microphones}} to {{Far}}-{{Field Sensors}} - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/document/6296525},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EW5X76RL\\6296525.html}
}

@online{MicrophoneArraysSignal,
  title = {Microphone {{Arrays}} - {{Signal Processing Techniques}} and {{Applications}} | {{Michael Brandstein}} | {{Springer}}},
  url = {https://www.springer.com/it/book/9783540419532},
  urldate = {2019-08-05},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9IURUCTQ\\9783540419532.html}
}

@online{MicrophoneArraysSignala,
  title = {Microphone {{Arrays}}– {{Signal Processing Techniques}} and {{Applications}} - {{Google}} Knygos},
  url = {https://books.google.lt/books?hl=lt&lr=&id=JinrCAAAQBAJ&oi=fnd&pg=PA3&dq=microphone+array+beamforming&ots=UjkPNaUwpv&sig=npdqr6jd5rddkXBHbqF-PXQntPY&redir_esc=y#v=onepage&q=microphone%20array%20beamforming&f=false},
  urldate = {2019-08-05},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PDUIKTVB\\books.html}
}

@online{MicrosoftPowerPointCONC,
  title = {Microsoft {{PowerPoint}} - {{CONC CN}}\_{{Presentation}}\_{{AES50}}\_2011-06-24\_{{Rev}}.0 - 07 - {{Al Walker}} - {{Applications}} in {{Live Convert Sound}}.Pdf},
  url = {http://www.aes-media.org/sections/uk/Conf2011/Presentation_PDFs/07%20-%20Al%20Walker%20-%20Applications%20in%20Live%20Convert%20Sound.pdf},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G85NJP3K\\07 - Al Walker - Applications in Live Convert Sound.html}
}

@article{MicrosoftWordCIS2008,
  title = {Microsoft {{Word}} - {{CIS2008}}.{{Rtf}} - {{Download}}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9688&rep=rep1&type=pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BCFUS6MS\\download.html}
}

@online{MicrosoftWordCIS2008a,
  title = {Microsoft {{Word}} - {{CIS2008}}.Rtf - Download},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9688&rep=rep1&type=pdf},
  urldate = {2017-01-30},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VH8SGEJI\\download.html}
}

@article{middlebrooksSoundLocalizationHuman1991,
  title = {Sound {{Localization}} by {{Human Listeners}}},
  author = {Middlebrooks, John C. and Green, David M.},
  date = {1991-01},
  journaltitle = {Annual Review of Psychology},
  volume = {42},
  pages = {135--159},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.ps.42.020191.001031},
  url = {http://www.annualreviews.org/doi/10.1146/annurev.ps.42.020191.001031},
  urldate = {2018-02-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JKJ7RRI5\\Sound-Localization-by-Human-Listeners.pdf},
  langid = {english},
  number = {1}
}

@article{mignotInterpolationRoomImpulse2012,
  title = {Interpolation of Room Impulse Responses in 3d Using Compressed Sensing},
  author = {Mignot, Rémi and Daudet, Laurent and Ollivier, Francois},
  date = {2012},
  pages = {7},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8PTDLGQE\\Mignot et al. - 2012 - Interpolation of room impulse responses in 3d usin.pdf},
  langid = {english}
}

@article{millerDesigningElectronicCircuits,
  title = {Designing {{Electronic Circuits Using Evolutionary Algorithms}}. {{Arithmetic Circuits}}: {{A Case Study}}},
  author = {MILLER, J F and THOMSON, P and FOGARTY, T},
  pages = {24},
  abstract = {A Genetic Algorithm is presented which is capable of evolving 100\% functional arithmetic circuits. Evolved designs are presented for one-bit, two-bit adders with carry, and two and three-bit multipliers and details of the 100\% correct evolution of three and four-bit adders. The largest of these circuits are the most complex digital circuits to have been designed by purely evolutionary means. The algorithm is able to re-discover conventionally optimum designs for the one-bit and two-bit adders, but more significantly is able to improve on the conventional designs for the two-bit multiplier. By analysing the history of an evolving design up to complete functionality it is possible to gain insight into evolutionary process. The technique is based on evolving the functionality and connectivity of a rectangular array of logic cells and is modelled on the resources available on the Xilinx 6216 FPGA device. Further work is described about plans to evolve the designs directly onto this device.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HCIDE7HP\\MILLER et al. - Designing Electronic Circuits Using Evolutionary A.pdf},
  langid = {english}
}

@article{miyatoVirtualAdversarialTraining2018,
  title = {Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi}}-{{Supervised Learning}}},
  shorttitle = {Virtual {{Adversarial Training}}},
  author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  date = {2018-06-27},
  url = {http://arxiv.org/abs/1704.03976},
  urldate = {2019-10-28},
  abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
  archivePrefix = {arXiv},
  eprint = {1704.03976},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3XA3JI29\\Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@online{ModelInterauralTime,
  title = {Model for the Interaural Time Differences in the Azimuthal Plane: {{The Journal}} of the {{Acoustical Society}} of {{America}}: {{Vol}} 62, {{No}} 1},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.381498},
  urldate = {2018-02-05},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XXEGSYNM\\1.html}
}

@inproceedings{mooreRoomGeometryEstimation2013,
  title = {Room Geometry Estimation from a Single Channel Acoustic Impulse Response},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), 2013 {{Proceedings}} of the 21st {{European}}},
  author = {Moore, Alastair H. and Brookes, Mike and Naylor, Patrick A.},
  date = {2013},
  pages = {1--5},
  publisher = {{IEEE}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SUBRM2WP\\Moore et al. - 2013 - Room geometry estimation from a single channel aco.pdf}
}

@online{MOTUComMOTU,
  title = {{{MOTU}}.Com - {{MOTU AVB FAQ}}},
  url = {http://motu.com/avb/avb-faq},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B9S2ISAS\\avb-faq.html}
}

@book{mullerHandbookEngineeringAcoustics2012,
  title = {Handbook of {{Engineering Acoustics}}},
  author = {Müller, Gerhard and Möser, Michael},
  date = {2012-11-06},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This acoustics handbook for mechanical and architectural applications is a translation of the German standard work on the subject. It not only describes the state of art of engineering acoustics but also gives practical help to engineers for solving acoustic problems. It deals with the origin, the transmission and the methods of abatement of air-borne and structure-borne sound of different kinds, from traffic to machinery and flow induced sound.},
  eprint = {1cX4AgfUaSkC},
  eprinttype = {googlebooks},
  isbn = {978-3-540-69460-1},
  keywords = {Science / Acoustics & Sound,Science / Mechanics / Dynamics,Science / Physics / General,Science / Waves & Wave Mechanics,Technology & Engineering / Construction / General,Technology & Engineering / Construction / Heating; Ventilation & Air Conditioning,Technology & Engineering / Electronics / General,Technology & Engineering / Environmental / General,Technology & Engineering / Environmental / Pollution Control,Technology & Engineering / Imaging Systems,Technology & Engineering / Industrial Design / General,Technology & Engineering / Industrial Design / Product,Technology & Engineering / Mechanical},
  langid = {english},
  pagetotal = {704}
}

@online{MultipleSourceLocalization,
  title = {Multiple {{Source Localization Based}} on {{Acoustic Map De}}-{{Emphasis}} | {{EURASIP Journal}} on {{Audio}}, {{Speech}}, and {{Music Processing}} | {{Full Text}}},
  url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1155/2010/147495},
  urldate = {2018-01-03}
}

@inproceedings{murrayRecurrentNeuralNetwork2005,
  title = {A Recurrent Neural Network for Sound-Source Motion Tracking and Prediction},
  author = {Murray, J.C. and Erwin, H. and Wermter, S.},
  date = {2005},
  volume = {4},
  pages = {2232--2236},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2005.1556248},
  url = {http://ieeexplore.ieee.org/document/1556248/},
  urldate = {2018-04-11},
  abstract = {Recurrent neural networks (RNN) have been used in many applications for both pattern detection and prediction. This paper shows the use of RNN’s as a speed classifier and predictor for a robotic sound source tracking system. The system requires extensive training to classify all possible speeds to enable dynamic tracking of the most prominent sound within the environment.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GLMK8M46\\Murray et al. - 2005 - A recurrent neural network for sound-source motion.pdf},
  isbn = {978-0-7803-9048-5}
}

@inproceedings{murrayRecurrentNeuralNetwork2005a,
  title = {A Recurrent Neural Network for Sound-Source Motion Tracking and Prediction},
  author = {Murray, J.C. and Erwin, H. and Wermter, S.},
  date = {2005},
  volume = {4},
  pages = {2232--2236},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2005.1556248},
  url = {http://ieeexplore.ieee.org/document/1556248/},
  urldate = {2018-04-02},
  abstract = {Recurrent neural networks (RNN) have been used in many applications for both pattern detection and prediction. This paper shows the use of RNN’s as a speed classifier and predictor for a robotic sound source tracking system. The system requires extensive training to classify all possible speeds to enable dynamic tracking of the most prominent sound within the environment.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HTATA8W2\\Murray et al. - 2005 - A recurrent neural network for sound-source motion.pdf},
  isbn = {978-0-7803-9048-5}
}

@inproceedings{murrayRecurrentNeuralNetwork2005b,
  title = {A Recurrent Neural Network for Sound-Source Motion Tracking and Prediction},
  booktitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  author = {Murray, J. C. and Erwin, H. and Wermter, S.},
  date = {2005-07},
  volume = {4},
  pages = {2232-2236 vol. 4},
  doi = {10.1109/IJCNN.2005.1556248},
  abstract = {Recurrent neural networks (RNN) have been used in many applications for both pattern detection and prediction. This paper shows the use of RNN's as a speed classifier and predictor for a robotic sound source tracking system. The system requires extensive training to classify all possible speeds to enable dynamic tracking of the most prominent sound within the environment.},
  eventtitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5K9LJ2HH\\1556248.html},
  keywords = {Azimuth,Human robot interaction,Hybrid intelligent systems,Manufacturing,Microphones,mobile robots,motion prediction,Navigation,recurrent neural nets,recurrent neural network,Recurrent neural networks,Robot sensing systems,robotic sound source tracking system,Signal to noise ratio,sound-source motion tracking,speaker recognition,speed classification,Tracking}
}

@online{MUS420EE367ALecture,
  title = {{{MUS420}}/{{EE367A Lecture 4A Interpolated Delay Lines}}, {{Ideal Bandlimited Interpolation}}, and {{Fractional Delay Filter Design}}},
  url = {https://ccrma.stanford.edu/~jos/Interpolation/},
  urldate = {2018-09-18},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6Q75QL8S\\MUS420EE367A Lecture 4A Interpolated Delay Lines,.html}
}

@article{nashLazyThetaAnyAngle,
  title = {Lazy {{Theta}}*: {{Any}}-{{Angle Path Planning}} and {{Path Length Analysis}} in {{3D}}},
  author = {Nash, Alex and Koenig, Sven and Tovey, Craig},
  pages = {8},
  abstract = {Grids with blocked and unblocked cells are often used to represent continuous 2D and 3D environments in robotics and video games. The shortest paths formed by the edges of 8neighbor 2D grids can be up to ≈ 8\% longer than the shortest paths in the continuous environment. Theta* typically finds much shorter paths than that by propagating information along graph edges (to achieve short runtimes) without constraining paths to be formed by graph edges (to find short “any-angle” paths). We show in this paper that the shortest paths formed by the edges of 26-neighbor 3D grids can be ≈ 13\% longer than the shortest paths in the continuous environment, which highlights the need for smart path planning algorithms in 3D. Theta* can be applied to 3D grids in a straight-forward manner, but it performs a line-of-sight check for each unexpanded visible neighbor of each expanded vertex and thus it performs many more line-of-sight checks per expanded vertex on a 26-neighbor 3D grid than on an 8-neighbor 2D grid. We therefore introduce Lazy Theta*, a variant of Theta* which uses lazy evaluation to perform only one line-of-sight check per expanded vertex (but with slightly more expanded vertices). We show experimentally that Lazy Theta* finds paths faster than Theta* on 26-neighbor 3D grids, with one order of magnitude fewer line-of-sight checks and without an increase in path length.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CDYCNUB3\\aaai10b.pdf},
  langid = {english}
}

@online{NearFieldAcousticSource,
  title = {Near-{{Field Acoustic Source Localization}} and {{Beamforming}} in {{Spherical Harmonics Domain}} - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/document/7435322},
  urldate = {2019-09-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TNFE2A38\\7435322.html}
}

@online{NeuralStructuredLearning,
  title = {Neural {{Structured Learning}}},
  journaltitle = {TensorFlow},
  url = {https://www.tensorflow.org/neural_structured_learning},
  urldate = {2019-10-24},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KRALUKGI\\neural_structured_learning.html},
  langid = {english}
}

@online{NewMethodMeasuring,
  title = {New {{Method}} of {{Measuring Reverberation Time}}: {{The Journal}} of the {{Acoustical Society}} of {{America}}: {{Vol}} 37, {{No}} 3},
  url = {https://asa.scitation.org/doi/10.1121/1.1909343},
  urldate = {2019-06-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q9ER846L\\1.html}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  url = {http://neuralnetworksanddeeplearning.com},
  urldate = {2017-02-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\58DXC5PR\\chap6.html}
}

@article{NielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q2AF8CC6\\chap6.html}
}

@article{nikunenDirectionArrivalBased2014,
  title = {Direction of {{Arrival Based Spatial Covariance Model}} for {{Blind Sound Source Separation}}},
  author = {Nikunen, J. and Virtanen, T.},
  date = {2014-03},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  pages = {727--739},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2014.2303576},
  abstract = {This paper addresses the problem of sound source separation from a multichannel microphone array capture via estimation of source spatial covariance matrix (SCM) of a short-time Fourier transformed mixture signal. In many conventional audio separation algorithms the source mixing parameter estimation is done separately for each frequency thus making them prone to errors and leading to suboptimal source estimates. In this paper we propose a SCM model which consists of a weighted sum of direction of arrival (DoA) kernels and estimate only the weights dependent on the source directions. In the proposed algorithm, the spatial properties of the sources become jointly optimized over all frequencies, leading to more coherent source estimates and mitigating the effect of spatial aliasing at high frequencies. The proposed SCM model is combined with a linear model for magnitudes and the parameter estimation is formulated in a complex-valued non-negative matrix factorization (CNMF) framework. Simulations consist of recordings done with a hand-held device sized array having multiple microphones embedded inside the device casing. Separation quality of the proposed algorithm is shown to exceed the performance of existing state of the art separation methods with two sources when evaluated by objective separation quality metrics.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\7INRIMHJ\\Nikunen and Virtanen - 2014 - Direction of Arrival Based Spatial Covariance Mode.pdf;C\:\\Users\\sauli\\Zotero\\storage\\9227G65J\\6728698.html},
  keywords = {array signal processing,Arrays,audio separation algorithms,audio signal processing,blind sound source separation,blind source separation,CNMF framework,complex-valued nonnegative matrix factorization framework,covariance matrices,device casing,direction of arrival based spatial covariance model,Direction of arrival estimation,direction-of-arrival estimation,DoA kernels,Fourier transforms,Frequency estimation,handheld device sized array,Kernel,linear model,Mathematical model,matrix decomposition,microphone arrays,Microphones,multichannel microphone array,multichannel source separation,non-negative matrix factorization,objective separation quality metrics,SCM model,short-time Fourier transformed mixture signal,source mixing parameter estimation,Source separation,source spatial covariance matrix estimation,spatial aliasing effect mitigation,spatial covariance models,spatial property,suboptimal source estimates,weighted sum of direction of arrival kernels},
  number = {3}
}

@inproceedings{nilsen_robust_2013-2,
  title = {Robust 3-{{D Sound Source Localization Using Spherical Microphone Arrays}}},
  author = {Nilsen, Carl-Inge Colombo and Hafizovic, Ines and Holm, Sverre},
  date = {2013-05-04},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=16804},
  urldate = {2017-01-10},
  abstract = {Spherical arrays are gaining increased interest in spatial audio reproduction, especially in Higher Order Ambisonics. In many audio applications the sound source detection and localization is of crucial importance, urging for robust localization methods suitable for spherical arrays. The well-known direction-of-arrival estimator, the ESPRIT algorithm, is not directly applicable to spherical arrays for 3-D applications. The eigenbeam ESPRIT (EB-ESPRIT) is based on the spherical harmonics...},
  eventtitle = {Audio {{Engineering Society Convention}} 134},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KMK7THIG\\Robust 3-D Sound Source Localization Using Spherical Microphone Arrays.pdf;C\:\\Users\\sauli\\Zotero\\storage\\MGN3WTA8\\browse.html},
  langid = {english}
}

@inproceedings{nilsenRobust3DSound2013,
  title = {Robust 3-{{D Sound Source Localization Using Spherical Microphone Arrays}}},
  author = {Nilsen, Carl-Inge Colombo and Hafizovic, Ines and Holm, Sverre},
  date = {2013-05-04},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=16804},
  urldate = {2017-01-10},
  abstract = {Spherical arrays are gaining increased interest in spatial audio reproduction, especially in Higher Order Ambisonics. In many audio applications the sound source detection and localization is of crucial importance, urging for robust localization methods suitable for spherical arrays. The well-known direction-of-arrival estimator, the ESPRIT algorithm, is not directly applicable to spherical arrays for 3-D applications. The eigenbeam ESPRIT (EB-ESPRIT) is based on the spherical harmonics...},
  eventtitle = {Audio {{Engineering Society Convention}} 134},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZMWX34V7\\browse.html},
  langid = {english}
}

@article{nilsenRobust3DSound2013a,
  title = {Robust {{3D}} Sound Source Localization Using Spherical Microphone Arrays},
  author = {Nilsen, C.-I. C. and Hafizovic, I. and Holm, S.},
  date = {2013-01-01},
  journaltitle = {ResearchGate},
  pages = {570--576},
  url = {https://www.researchgate.net/publication/282699170_Robust_3D_sound_source_localization_using_spherical_microphone_arrays},
  urldate = {2017-01-10},
  abstract = {Robust 3D sound source localization using spherical microphone arrays on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ERER78EE\\282699170_Robust_3D_sound_source_localization_using_spherical_microphone_arrays.html}
}

@inproceedings{nishiura_localization_2000,
  title = {Localization of Multiple Sound Sources Based on a {{CSP}} Analysis with a Microphone Array},
  booktitle = {2000 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00CH37100}})},
  author = {Nishiura, T. and Yamada, T. and Nakamura, S. and Shikano, K.},
  date = {2000},
  volume = {2},
  pages = {II1053-II1056 vol.2},
  doi = {10.1109/ICASSP.2000.859144},
  abstract = {Accurate localization of multiple sound sources is indispensable for the microphone array-based high quality sound capture. For single sound source localization, the CSP (cross-power spectrum phase analysis) method has been proposed. The CSP method localizes a sound source as a crossing point of sound directions estimated using different microphone pairs. However, when localizing multiple sound sources, the CSP method has a problem that the localization accuracy is degraded due to cross-correlation among different sound sources. To solve this problem, this paper proposes a new method which suppresses the undesired cross-correlation by synchronous addition of CSP coefficients derived from multiple microphone pairs. Experiment results in a real room showed that the proposed method improves the localization accuracy when increasing the number of the synchronous addition},
  eventtitle = {2000 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00CH37100}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BDWMAXZF\\Nishiura et al. - 2000 - Localization of multiple sound sources based on a .pdf;C\:\\Users\\sauli\\Zotero\\storage\\TCHGJMKQ\\859144.html},
  keywords = {acoustic transducer arrays,array signal processing,Control systems,cross-correlation,cross-power spectrum phase analysis,CSP analysis,Degradation,Delay estimation,Direction of arrival estimation,direction-of-arrival estimation,Discrete Fourier transforms,Equations,high quality sound capture,Information analysis,Information science,microphone array,microphone arrays,microphone pairs,Microphones,multiple sound sources localization,single sound source localization,spectral analysis,speech processing,synchronous addition,Teleconferencing}
}

@article{nix_sound_2006,
  title = {Sound Source Localization in Real Sound Fields Based on Empirical Statistics of Interaural Parameters},
  author = {Nix, Johannes and Hohmann, Volker},
  date = {2006-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {119},
  pages = {463--479},
  issn = {0001-4966},
  doi = {10.1121/1.2139619},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.2139619},
  urldate = {2017-01-11},
  abstract = {The role of temporal fluctuations and systematic variations of interaural parameters in localization of sound sources in spatially distributed, nonstationary noise conditions was investigated. For this, Bayesian estimation was applied to interaural parameters calculated with physiologically plausible time and frequency resolution. Probability density functions (PDFs) of the interaural level differences (ILDs) and phase differences (IPDs) were estimated by measuring histograms for a directional sound source perturbed by several types of interfering noise at signal-to-noise ratios (SNRs) between −5−5{$<$}math display="inline" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}mo{$>$}−{$<$}/mo{$><$}mn{$>$}5{$<$}/mn{$><$}/mrow{$><$}/math{$>$} and +30dB+30dB{$<$}math display="inline" overflow="scroll" altimg="eq-00002.gif"{$><$}mrow{$><$}mo{$>$}+{$<$}/mo{$><$}mn{$>$}30{$<$}/mn{$><$}mspace width="0.3em"{$><$}/mspace{$><$}mi{$>$}dB{$<$}/mi{$><$}/mrow{$><$}/math{$>$}. A moment analysis of the PDFs reveals that the expected values shift and the standard deviations increase considerably with decreasing SNR, and that the PDFs have non-Gaussian shape at medium SNRs. A d′d′{$<$}math display="inline" overflow="scroll" altimg="eq-00003.gif"{$><$}msup{$><$}mi{$>$}d{$<$}/mi{$><$}mo{$>$}′{$<$}/mo{$><$}/msup{$><$}/math{$>$} analysis of the PDFs indicates that elevation discrimination is possible even at low SNRs in the median plane by integrating information across frequency. Absolute sound localization was simulated by a Bayesian maximum a posteriori (MAP) procedure. The simulation is based on frequency integration of broadly tuned “detectors.” Confusion patterns of real and estimated sound source directions are similar to those of human listeners. The results indicate that robust processing strategies are needed to exploit interaural parameters successfully in noise conditions due to their strong temporal fluctuations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\55XTCNRK\\1.html},
  number = {1}
}

@article{nixSoundSourceLocalization2006,
  title = {Sound Source Localization in Real Sound Fields Based on Empirical Statistics of Interaural Parameters},
  author = {Nix, Johannes and Hohmann, Volker},
  date = {2006-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {119},
  pages = {463--479},
  issn = {0001-4966},
  doi = {10.1121/1.2139619},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.2139619},
  urldate = {2017-01-11},
  abstract = {The role of temporal fluctuations and systematic variations of interaural parameters in localization of sound sources in spatially distributed, nonstationary noise conditions was investigated. For this, Bayesian estimation was applied to interaural parameters calculated with physiologically plausible time and frequency resolution. Probability density functions (PDFs) of the interaural level differences (ILDs) and phase differences (IPDs) were estimated by measuring histograms for a directional sound source perturbed by several types of interfering noise at signal-to-noise ratios (SNRs) between −5−5{$<$}math display="inline" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}mo{$>$}−{$<$}/mo{$><$}mn{$>$}5{$<$}/mn{$><$}/mrow{$><$}/math{$>$} and +30dB+30dB{$<$}math display="inline" overflow="scroll" altimg="eq-00002.gif"{$><$}mrow{$><$}mo{$>$}+{$<$}/mo{$><$}mn{$>$}30{$<$}/mn{$><$}mspace width="0.3em"{$><$}/mspace{$><$}mi{$>$}dB{$<$}/mi{$><$}/mrow{$><$}/math{$>$}. A moment analysis of the PDFs reveals that the expected values shift and the standard deviations increase considerably with decreasing SNR, and that the PDFs have non-Gaussian shape at medium SNRs. A d′d′{$<$}math display="inline" overflow="scroll" altimg="eq-00003.gif"{$><$}msup{$><$}mi{$>$}d{$<$}/mi{$><$}mo{$>$}′{$<$}/mo{$><$}/msup{$><$}/math{$>$} analysis of the PDFs indicates that elevation discrimination is possible even at low SNRs in the median plane by integrating information across frequency. Absolute sound localization was simulated by a Bayesian maximum a posteriori (MAP) procedure. The simulation is based on frequency integration of broadly tuned “detectors.” Confusion patterns of real and estimated sound source directions are similar to those of human listeners. The results indicate that robust processing strategies are needed to exploit interaural parameters successfully in noise conditions due to their strong temporal fluctuations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WS47EK4Q\\1.html},
  number = {1}
}

@article{nohLearningDeconvolutionNetwork2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  date = {2015-05-17},
  url = {http://arxiv.org/abs/1505.04366},
  urldate = {2017-02-19},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
  archivePrefix = {arXiv},
  eprint = {1505.04366},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HGI88AUM\\Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf;C\:\\Users\\sauli\\Zotero\\storage\\DEMCJQT7\\1505.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{NohLearningDeconvolutionNetwork2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  date = {2015-05},
  journaltitle = {arXiv:1505.04366 [cs]},
  abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
  archivePrefix = {arXiv},
  eprint = {1505.04366},
  eprintclass = {cs},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JCCUIQLT\\Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NQ5FWISJ\\1505.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@online{NoiseReductionSpherical,
  title = {(2) {{Noise Reduction}} in the {{Spherical Harmonic Domain Using}} a {{Tradeoff Beamformer}} and {{Narrowband DOA Estimates}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/264589798_Noise_Reduction_in_the_Spherical_Harmonic_Domain_Using_a_Tradeoff_Beamformer_and_Narrowband_DOA_Estimates/figures?lo=1},
  urldate = {2018-05-29},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9UFK2FXC\\figures.html},
  langid = {english}
}

@online{NoiseReductionSphericala,
  title = {(2) {{Noise Reduction}} in the {{Spherical Harmonic Domain Using}} a {{Tradeoff Beamformer}} and {{Narrowband DOA Estimates}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/264589798_Noise_Reduction_in_the_Spherical_Harmonic_Domain_Using_a_Tradeoff_Beamformer_and_Narrowband_DOA_Estimates},
  urldate = {2018-05-29},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UWVV9MBP\\(2) Noise Reduction in the Spherical Harmonic Doma.pdf;C\:\\Users\\sauli\\Zotero\\storage\\9ZQVYYPD\\264589798_Noise_Reduction_in_the_Spherical_Harmonic_Domain_Using_a_Tradeoff_Beamformer_and_Narr.html},
  langid = {english}
}

@article{nordeboFundamentalLimitationsDOA,
  title = {Fundamental {{Limitations}} for {{DOA Estimation}} by a {{Sphere}}},
  author = {Nordebo, Sven and Gustafsson, Mats},
  pages = {30},
  abstract = {In this paper we consider fundamental limitations for DOA estimation with arbitrary lossless antennas or antenna arrays inserted inside a sphere. Spherical vector modes and their associated equivalent circuits and Q factor approximations are employed as a general framework for the analysis. The classical broadband matching theory by Fano is extended to a general multiport S–parameter model of the antennas and fundamental bounds are given for the scattering parameters with respect to bandwidth and electrical size of the sphere. Finally, assuming a statistical signal model with Gaussian receiver noise, the Cramer–Rao lower bound is used to derive fundamental upper bounds for the performance of DOA estimation by a sphere.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RN6M4S2E\\Nordebo and Gustafsson - Fundamental Limitations for DOA Estimation by a Sp.pdf}
}

@article{nordeboFundamentalLimitationsDOA2004,
  title = {Fundamental {{Limitations}} for {{DOA Estimation}} by a {{Sphere}}},
  author = {Nordebo, Sven and Gustafsson, Mats},
  date = {2004},
  url = {http://portal.research.lu.se/portal/en/publications/fundamental-limitations-for-doa-estimation-by-a-sphere(400b45ef-54dc-4ba2-90e0-905436ddd8d8).html},
  urldate = {2018-05-29},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WGXJPNFP\\Nordebo and Gustafsson - 2004 - Fundamental Limitations for DOA Estimation by a Sp.pdf;C\:\\Users\\sauli\\Zotero\\storage\\X6EUHFNM\\fundamental-limitations-for-doa-estimation-by-a-sphere(400b45ef-54dc-4ba2-90e0-905436ddd8d8).html},
  langid = {english}
}

@inproceedings{ntalampirasAcousticSurveillanceHazardous2009,
  title = {On Acoustic Surveillance of Hazardous Situations},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Ntalampiras, Stavros and Potamitis, Ilyas and Fakotakis, Nikos},
  date = {2009-04},
  pages = {165--168},
  publisher = {{IEEE}},
  location = {{Taipei, Taiwan}},
  doi = {10.1109/ICASSP.2009.4959546},
  url = {http://ieeexplore.ieee.org/document/4959546/},
  urldate = {2019-07-29},
  abstract = {The present study presents a practical methodology for automatic space monitoring based solely on the perceived acoustic information. We consider the case where atypical situations such as screams, explosions and gunshots take place in a metro station environment. Our approach is based on a two stage recognition schema, each one exploiting HMMs for approximating the density function of the corresponding sound class. The main objective is to detect abnormal events that take place in a noisy environment. A thorough evaluation procedure is carried out under different SNR conditions and we report high detection rates with respect to false alarm and miss probabilities rates.},
  eventtitle = {{{ICASSP}} 2009 - 2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PCECCXJ8\\Ntalampiras et al. - 2009 - On acoustic surveillance of hazardous situations.pdf},
  isbn = {978-1-4244-2353-8},
  langid = {english}
}

@online{NTiAudioAppNoteAES3AESEBUPdf,
  title = {{{NTi}}-{{Audio}}-{{AppNote}}-{{AES3}}-{{AES}}-{{EBU}}.Pdf},
  url = {http://www.nti-audio.com/Portals/0/data/en/NTi-Audio-AppNote-AES3-AES-EBU.pdf},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HA5VAIRK\\NTi-Audio-AppNote-AES3-AES-EBU.html}
}

@article{nyquist_certain_1928,
  title = {Certain {{Topics}} in {{Telegraph Transmission Theory}}},
  author = {Nyquist, H.},
  date = {1928-04},
  journaltitle = {Transactions of the American Institute of Electrical Engineers},
  volume = {47},
  pages = {617--644},
  issn = {0096-3860},
  doi = {10.1109/T-AIEE.1928.5055024},
  abstract = {The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R245RVPE\\5055024.html},
  keywords = {Circuits,Costs,Distortion,Equalizers,Frequency conversion,Interference,Shape,Steady-state,Telegraphy,Telephony},
  number = {2}
}

@article{ohGPUImplementationNeural2004,
  title = {{{GPU}} Implementation of Neural Networks},
  author = {Oh, Kyoung-Su and Jung, Keechul},
  date = {2004-06},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {37},
  pages = {1311--1314},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2004.01.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320304000524},
  urldate = {2017-01-11},
  abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UKQ4S6M6\\Oh and Jung - 2004 - GPU implementation of neural networks.pdf;C\:\\Users\\sauli\\Zotero\\storage\\VAHKV6AA\\S0031320304000524.html},
  keywords = {Graphics processing unit(GPU),Multi-layer perceptron,Neural network(NN),Text detection},
  number = {6}
}

@inproceedings{omologoAcousticEventLocalization1994,
  title = {Acoustic Event Localization Using a Crosspower-Spectrum Phase Based Technique},
  booktitle = {Proceedings of {{ICASSP}} '94. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Omologo, M. and Svaizer, P.},
  date = {1994-04},
  volume = {ii},
  pages = {II/273-II/276 vol.2},
  doi = {10.1109/ICASSP.1994.389667},
  abstract = {Linear microphone arrays can be employed for acoustic event localization in a noisy environment using time delay estimation. Three techniques are investigated that allow delay estimation, namely normalized cross correlation, LMS adaptive filters, crosspower-spectrum phase: they are combined with a bidimensional representation, the coherence measure, in order to emphasize information that can be exploited for estimating position of both non-moving and moving acoustic sources. To compare the given techniques, different acoustic sources were considered, that generated events in different positions in space. Expressing performance in terms of accuracy of the wavefront direction angle, experiments showed that the crosspower-spectrum phase based technique outperforms the other two. This technique provided very promising preliminary results also in terms of source position estimation.{$<>$}},
  eventtitle = {Proceedings of {{ICASSP}} '94. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SCBXPXWK\\Omologo and Svaizer - 1994 - Acoustic event localization using a crosspower-spe.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3XDHJRMP\\389667.html},
  keywords = {Acoustic arrays,acoustic event localization,Acoustic measurements,Acoustic noise,acoustic signal processing,Adaptive filters,bidimensional representation,Coherence,coherence measure,crosspower-spectrum phase based technique,Delay effects,Delay estimation,delays,direction-of-arrival estimation,Least squares approximation,linear microphone arrays,LMS adaptive filters,Microphone arrays,moving acoustic sources,noisy environment,nonmoving acoustic sources,normalized cross correlation,position measurement,source position estimation,spectral analysis,time delay estimation,wavefront direction angle,Working environment noise}
}

@article{omologoUseCrosspowerspectrumPhase1997,
  title = {Use of the Crosspower-Spectrum Phase in Acoustic Event Location},
  author = {Omologo, M. and Svaizer, P.},
  date = {1997-05},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {5},
  pages = {288--292},
  doi = {10.1109/89.568735},
  abstract = {The article reports on the use of crosspower-spectrum phase (CSP) analysis as an accurate time delay estimation (TDE) technique. It is used in a microphone array system for the location of acoustic events in noisy and reverberant environments. A corresponding coherence measure (CM) and its graphical representation are introduced to show the TDE accuracy. Using a two-microphone pair array, real experiments show less than a 10 cm average location error in a 6 m/spl times/6 m area.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CJXKJZVY\\Omologo and Svaizer - 1997 - Use of the crosspower-spectrum phase in acoustic e.pdf;C\:\\Users\\sauli\\Zotero\\storage\\PYQRWIGD\\568735.html},
  keywords = {Acoustic arrays,acoustic event location,acoustic noise,Acoustic noise,Acoustic sensors,acoustic signal processing,acoustic transducer arrays,array signal processing,Array signal processing,crosspower-spectrum phase,crosspower-spectrum phase analysis,Delay effects,Delay estimation,delays,direction-of-arrival estimation,microphone array system,Microphone arrays,microphones,noisy environment,Position measurement,reverberant environment,reverberation,Sensor arrays,spectral analysis,time delay estimation,Working environment noise},
  number = {3}
}

@inproceedings{oumarComparisonMUSICESPRIT2012,
  title = {Comparison between {{MUSIC}} and {{ESPRIT}} Direction of Arrival Estimation Algorithms for Wireless Communication Systems},
  booktitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  author = {Oumar, O. A. and Siyau, M. F. and Sattar, T. P.},
  date = {2012-12},
  pages = {99--103},
  doi = {10.1109/FGCT.2012.6476563},
  abstract = {This paper presents simulation of Angle of Arrival (AOA) estimation using the Multiple Signal Classification (MUSIC) and Estimation of Signal Parameters via Rotational Invariance Technique (ESPRIT) algorithms. We study the localization techniques using the wireless communication systems and there are several algorithms that have the ability in calculate the Direction of Arrival (DOA) of the incident signals. We investigate and compare MUSIC and ESPRIT algorithms. The simulations indicate that the MUSIC algorithm is more accurate and stable compared to the ESPRIT algorithm.},
  eventtitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2KHPRKHW\\Oumar et al. - 2012 - Comparison between MUSIC and ESPRIT direction of a.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3FHGUFTC\\6476563.html},
  keywords = {angle of arrival estimation,Antenna arrays,AOA,AOA estimation,Arrays,direction-of-arrival estimation,DOA,Eigenvalues and eigenfunctions,ESPRIT,ESPRIT direction of arrival estimation algorithm,Estimation,estimation of signal parameters via rotational invariance technique,incident signal,localization technique,MATLAB,Multiple signal classification,MUSIC,MUSIC algorithm,radiocommunication,Receivers,signal classification,wireless communication system}
}

@inproceedings{oumarComparisonMUSICESPRIT2012a,
  title = {Comparison between {{MUSIC}} and {{ESPRIT}} Direction of Arrival Estimation Algorithms for Wireless Communication Systems},
  booktitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  author = {Oumar, O. A. and Siyau, M. F. and Sattar, T. P.},
  date = {2012-12},
  pages = {99--103},
  doi = {10.1109/FGCT.2012.6476563},
  abstract = {This paper presents simulation of Angle of Arrival (AOA) estimation using the Multiple Signal Classification (MUSIC) and Estimation of Signal Parameters via Rotational Invariance Technique (ESPRIT) algorithms. We study the localization techniques using the wireless communication systems and there are several algorithms that have the ability in calculate the Direction of Arrival (DOA) of the incident signals. We investigate and compare MUSIC and ESPRIT algorithms. The simulations indicate that the MUSIC algorithm is more accurate and stable compared to the ESPRIT algorithm.},
  eventtitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\K6G3MEM9\\6476563.html},
  keywords = {angle of arrival estimation,Antenna arrays,AOA,AOA estimation,Arrays,direction-of-arrival estimation,DOA,Eigenvalues and eigenfunctions,ESPRIT,ESPRIT direction of arrival estimation algorithm,Estimation,estimation of signal parameters via rotational invariance technique,incident signal,localization technique,MATLAB,Multiple signal classification,MUSIC,MUSIC algorithm,radiocommunication,Receivers,signal classification,wireless communication system}
}

@inproceedings{oumarComparisonMUSICESPRIT2012b,
  title = {Comparison between {{MUSIC}} and {{ESPRIT}} Direction of Arrival Estimation Algorithms for Wireless Communication Systems},
  booktitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  author = {Oumar, O. A. and Siyau, M. F. and Sattar, T. P.},
  date = {2012-12},
  pages = {99--103},
  doi = {10.1109/FGCT.2012.6476563},
  abstract = {This paper presents simulation of Angle of Arrival (AOA) estimation using the Multiple Signal Classification (MUSIC) and Estimation of Signal Parameters via Rotational Invariance Technique (ESPRIT) algorithms. We study the localization techniques using the wireless communication systems and there are several algorithms that have the ability in calculate the Direction of Arrival (DOA) of the incident signals. We investigate and compare MUSIC and ESPRIT algorithms. The simulations indicate that the MUSIC algorithm is more accurate and stable compared to the ESPRIT algorithm.},
  eventtitle = {The {{First International Conference}} on {{Future Generation Communication Technologies}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4NA3K56C\\6476563.html},
  keywords = {angle of arrival estimation,Antenna arrays,AOA,AOA estimation,Arrays,direction-of-arrival estimation,DOA,Eigenvalues and eigenfunctions,ESPRIT,ESPRIT direction of arrival estimation algorithm,Estimation,estimation of signal parameters via rotational invariance technique,incident signal,localization technique,MATLAB,Multiple signal classification,MUSIC,MUSIC algorithm,radiocommunication,Receivers,signal classification,wireless communication system}
}

@online{OutdoorSoundLocalization,
  title = {Outdoor Sound Localization Using a Tetrahedral Array - {{Project Library}}, {{Aalborg University}}},
  url = {https://projekter.aau.dk/projekter/en/studentthesis/outdoor-sound-localization-using-a-tetrahedral-array(89ad597e-5bb5-4209-acac-5e0a64c38cc8).html},
  urldate = {2019-09-18},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AL93T9UP\\outdoor-sound-localization-using-a-tetrahedral-array(89ad597e-5bb5-4209-acac-5e0a64c38cc8).html}
}

@online{Overview,
  title = {Overview},
  url = {http://www.ravenna-network.com/using-ravenna/overview/},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NV8ZX76V\\overview.html}
}

@inproceedings{ozekiEstimatingDirectionsMultiple2006,
  title = {Estimating {{Directions}} of {{Multiple Sound Sources Using Tetrahedral Microphone Array}}},
  booktitle = {{{TENCON}} 2006 - 2006 {{IEEE Region}} 10 {{Conference}}},
  author = {Ozeki, K. and Hamada, N.},
  date = {2006-11},
  pages = {1--4},
  doi = {10.1109/TENCON.2006.343853},
  abstract = {Localization of sound sources by microphone array is one of the important problems. This work proposes the method for sound localization using tetrahedraly-coordinated microphone arrays. This is based on the method provided by Berdugo et al. which estimates the time delays between each pair of microphones for an individual frequency. We achieved efficient three-dimensional DOA estimation with no bias, including following two improvements: spherical pseudo-histogram defining source-density function (SDF), and using the weighted addition of the past histograms. The performance has been evaluated by performing the simulation experiments for static speakers and the experiments for moving speakers. In the simulation experiments, we assumed one static speaker and additional white noise. We evaluated the performance by calculating the success estimation rate as the direction of the speaker varies. The experiments for moving speakers were performed in typical office. The results in both simulations and experiments show the effectiveness of our proposal},
  eventtitle = {{{TENCON}} 2006 - 2006 {{IEEE Region}} 10 {{Conference}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W47TZT9C\\Ozeki and Hamada - 2006 - Estimating Directions of Multiple Sound Sources Us.pdf;C\:\\Users\\sauli\\Zotero\\storage\\B93Z8QNL\\4142516.html},
  keywords = {acoustic generators,acoustic signal processing,Delay effects,delay estimation,Delay estimation,Direction of arrival estimation,direction-of-arrival estimation,Frequency estimation,Histograms,Loudspeakers,microphone arrays,Microphone arrays,Phase estimation,Phased arrays,SDF,Sensor arrays,sound source localization,source-density function,spherical pseudohistogram,static speaker,tetrahedraly-coordinated microphone arrays,three-dimensional DOA estimation,time delay estimation,white noise}
}

@inproceedings{ozekiEstimatingDirectionsMultiple2006a,
  title = {Estimating {{Directions}} of {{Multiple Sound Sources Using Tetrahedral Microphone Array}}},
  booktitle = {{{TENCON}} 2006 - 2006 {{IEEE Region}} 10 {{Conference}}},
  author = {Ozeki, K. and Hamada, N.},
  date = {2006-11},
  pages = {1--4},
  doi = {10.1109/TENCON.2006.343853},
  abstract = {Localization of sound sources by microphone array is one of the important problems. This work proposes the method for sound localization using tetrahedraly-coordinated microphone arrays. This is based on the method provided by Berdugo et al. which estimates the time delays between each pair of microphones for an individual frequency. We achieved efficient three-dimensional DOA estimation with no bias, including following two improvements: spherical pseudo-histogram defining source-density function (SDF), and using the weighted addition of the past histograms. The performance has been evaluated by performing the simulation experiments for static speakers and the experiments for moving speakers. In the simulation experiments, we assumed one static speaker and additional white noise. We evaluated the performance by calculating the success estimation rate as the direction of the speaker varies. The experiments for moving speakers were performed in typical office. The results in both simulations and experiments show the effectiveness of our proposal},
  eventtitle = {{{TENCON}} 2006 - 2006 {{IEEE Region}} 10 {{Conference}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\I6CNBQWD\\Ozeki_Hamada_2006_Estimating Directions of Multiple Sound Sources Using Tetrahedral Microphone.pdf;C\:\\Users\\sauli\\Zotero\\storage\\62LMSCRH\\4142516.html},
  keywords = {acoustic generators,acoustic signal processing,Delay effects,delay estimation,Delay estimation,Direction of arrival estimation,direction-of-arrival estimation,Frequency estimation,Histograms,Loudspeakers,microphone arrays,Microphone arrays,Phase estimation,Phased arrays,SDF,Sensor arrays,sound source localization,source-density function,spherical pseudohistogram,static speaker,tetrahedraly-coordinated microphone arrays,three-dimensional DOA estimation,time delay estimation,white noise}
}

@article{ozerNoiseRobustSound2018,
  title = {Noise Robust Sound Event Classification with Convolutional Neural Network},
  author = {Ozer, Ilyas and Ozer, Zeynep and Findik, Oguz},
  date = {2018-01-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {272},
  pages = {505--512},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.07.021},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231217312675},
  urldate = {2018-10-19},
  abstract = {Automatic sound recognition (ASR) is a remarkable field of research in recent years. The ability to automatically recognize sound events through computers in a complex audio environment is very useful for machine hearing, acoustic surveillance and multimedia retrieval applications. On the other hand, ASR task become highly difficult as the ambient noise levels increase and many traditional methods show very weak performance under noise. Recent studies has shown that spectrogram image features (SIF) have high performance under noise, while success rates in clean conditions are relatively lower than in the state-of-the-art approaches. In this study, after converting highly overlapped spectrograms into linear quantized images and reducing dimensions by applying various image resizing methods, feature extraction and classification are performed with convolutional neural networks (CNN), which have very high performance in image classification. In the mismatched case, the proposed method achieves a performance improvement of 4.5\%, which is equivalent to a relative error reduction of 63.4\%, with a classification success of 97.4\%, while the multicondition training method achieves an average of 98.63\% success rate.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CCBA5GQA\\Ozer et al. - 2018 - Noise robust sound event classification with convo.pdf;C\:\\Users\\sauli\\Zotero\\storage\\E7SV5Z3F\\S0925231217312675.html},
  keywords = {Convolutional neural networks,Sound event classification,Spectrogram}
}

@article{padoisAcousticSourceLocalization2018,
  title = {Acoustic Source Localization Based on the Generalized Cross-Correlation and the Generalized Mean with Few Microphones},
  author = {Padois, Thomas},
  date = {2018-05},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{EL393-EL398\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5039416\}},
  abstract = {Microphone arrays are an efficient tool for localizing acoustic sources. Time domain beamforming is one of the techniques which can be used to build a sound source map. This technique based on the cross-correlation of the microphone signals can be seen as a broadband beamforming method. The spatial likelihood functions are averaged with the arithmetic mean. Recently, it has been shown that the harmonic and geometric means can improve the quality of the sound source map. In this study, the influence of the exponent value of the generalized mean on the sound source map is investigated. Then it is shown that the proposed technique is less sensitive to the number of microphones and therefore, it can be reduced. (C) 2018 Acoustical Society of America},
  affiliation = {Padois, T (Reprint Author), Ecole Technol Super, Dept Mech Engn, Montreal, PQ H3C 1K3, Canada. Padois, Thomas, Ecole Technol Super, Dept Mech Engn, Montreal, PQ H3C 1K3, Canada.},
  author-email = {thomas.padois@etsmtl.ca},
  cited-references = {Aarabi P, 2003, EURASIP J APPL SIG P, V2003, P338, DOI 10.1155/S1110865703212014. Brooks TF, 2006, J SOUND VIB, V294, P856, DOI 10.1016/j.jsv.2005.12.046. Dougherty R. P., 2014, 5 BERL BEAMF C BEBEC. Johnson D., 1993, ARRAY SIGNAL PROCESS. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Michel U., 2006, 1 BERL BEAMF C BEBEC. Noel C, 2006, J SOUND VIB, V296, P518, DOI 10.1016/j.jsv.2005.12.056. Olver F.W.J., 2010, NIST HDB MATH FUNCTI. Padois T, 2017, MECH SYST SIGNAL PR, V94, P85, DOI 10.1016/j.ymssp.2017.02.035. Padois T, 2017, J SOUND VIB, V386, P82, DOI 10.1016/j.jsv.2016.09.006. Padois T, 2016, J ACOUST SOC AM, V140, pEL56, DOI 10.1121/1.4955007. Padois T, 2014, J SOUND VIB, V333, P6858, DOI 10.1016/j.jsv.2014.07.028. Sijtsma Pieter, 2007, International Journal of Aeroacoustics, V6, P357, DOI 10.1260/147547207783359459. Sijtsma P, 2017, INT J AEROACOUST, V16, P274, DOI 10.1177/1475472X17713034.},
  da = {2018-10-18},
  doc-delivery-number = {GG9WB},
  eissn = {1520-8524},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {TIME-DOMAIN; ARRAY},
  langid = {english},
  number = {5},
  number-of-cited-references = {14},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000433050700014},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@inproceedings{panAdversariallyRegularizedGraph2018,
  title = {Adversarially {{Regularized Graph Autoencoder}} for {{Graph Embedding}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Yao, Lina and Zhang, Chengqi},
  date = {2018-07},
  pages = {2609--2615},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/362},
  url = {https://www.ijcai.org/proceedings/2018/362},
  urldate = {2019-10-24},
  abstract = {Graph embedding is an effective method to represent graph data in a low dimensional space for graph analytics. Most existing embedding algorithms typically focus on preserving the topological structure or minimizing the reconstruction errors of graph data, but they have mostly ignored the data distribution of the latent codes from the graphs, which often results in inferior embedding in realworld graph data. In this paper, we propose a novel adversarial graph embedding framework for graph data. The framework encodes the topological structure and node content in a graph to a compact representation, on which a decoder is trained to reconstruct the graph structure. Furthermore, the latent representation is enforced to match a prior distribution via an adversarial training scheme. To learn a robust embedding, two variants of adversarial approaches, adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA), are developed. Experimental studies on real-world graphs validate our design and demonstrate that our algorithms outperform baselines by a wide margin in link prediction, graph clustering, and graph visualization tasks.},
  eventtitle = {Twenty-{{Seventh International Joint Conference}} on {{Artificial Intelligence}} \{\vphantom\}{{IJCAI}}-18\vphantom\{\}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5LQYC3CE\\Pan et al. - 2018 - Adversarially Regularized Graph Autoencoder for Gr.pdf},
  isbn = {978-0-9992411-2-7},
  langid = {english}
}

@article{panAdversariallyRegularizedGraph2019,
  title = {Adversarially {{Regularized Graph Autoencoder}} for {{Graph Embedding}}},
  author = {Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Yao, Lina and Zhang, Chengqi},
  date = {2019-01-07},
  url = {http://arxiv.org/abs/1802.04407},
  urldate = {2019-10-24},
  abstract = {Graph embedding is an effective method to represent graph data in a low dimensional space for graph analytics. Most existing embedding algorithms typically focus on preserving the topological structure or minimizing the reconstruction errors of graph data, but they have mostly ignored the data distribution of the latent codes from the graphs, which often results in inferior embedding in real-world graph data. In this paper, we propose a novel adversarial graph embedding framework for graph data. The framework encodes the topological structure and node content in a graph to a compact representation, on which a decoder is trained to reconstruct the graph structure. Furthermore, the latent representation is enforced to match a prior distribution via an adversarial training scheme. To learn a robust embedding, two variants of adversarial approaches, adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA), are developed. Experimental studies on real-world graphs validate our design and demonstrate that our algorithms outperform baselines by a wide margin in link prediction, graph clustering, and graph visualization tasks.},
  archivePrefix = {arXiv},
  eprint = {1802.04407},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ARFNZ5JH\\Pan et al. - 2019 - Adversarially Regularized Graph Autoencoder for Gr.pdf;C\:\\Users\\sauli\\Zotero\\storage\\WKXP6CG7\\1802.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{ParallelTransportUnfolding,
  title = {Parallel {{Transport Unfolding}}: {{A Connection}}-Based {{Manifold Learning Approach}}},
  shorttitle = {Parallel {{Transport Unfolding}}},
  journaltitle = {GroundAI},
  url = {https://www.groundai.com/project/parallel-transport-unfolding-a-connection-based-manifold-learning-approach/},
  urldate = {2019-11-11},
  abstract = {Manifold learning offers nonlinear dimensionality reduction of high-dimensional datasets. In this paper, we bring geometry processing to bear on manifold learning by introducing a new approach based on metric connection for generating a quasi-isometric, low-dimensional mapping from a sparse and irregular sampling of an arbitrary manifold embedded in a high-dimensional space. Geodesic distances of discrete paths on the input pointset are evaluated through "parallel transport unfolding" (PTU) to offer robustness to poor sampling and arbitrary topology. Our new geometric procedure exhibits the same strong resilience to noise as one of the staples of manifold learning, the Isomap algorithm, as it …},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X37442KY\\parallel-transport-unfolding-a-connection-based-manifold-learning-approach.html},
  langid = {english}
}

@article{park_sound-field_2005,
  title = {Sound-Field Analysis by Plane-Wave Decomposition Using Spherical Microphone Array},
  author = {Park, Munhum and Rafaely, Boaz},
  date = {2005-10-28},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {118},
  pages = {3094--3103},
  issn = {0001-4966},
  doi = {10.1121/1.2063108},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.2063108},
  urldate = {2017-01-10},
  abstract = {Directional sound-field information is becoming more important in sound-field analysis and auditorium acoustics, and, as a consequence, a variety of microphone arrays have recently been studied that provide such information. In particular, spherical microphone arrays have been proposed that provide three-dimensional information by decomposing the sound field into spherical harmonics. The theoretical formulation of the plane-wave decomposition and array performance analysis were also presented. In this paper, as a direct continuation of the recent work, a spherical microphone array configured around a rigid sphere is designed, analyzed using simulation, and then used experimentally to decompose the sound field in an anechoic chamber and an auditorium into waves. The array employs a maximum of 98 measurement positions around the sphere, and is used to compute spherical harmonics up to order 6. In the current paper we investigate the factors affecting the performance of plane-wave decomposition, showing that the direct sound and several reflections in an auditorium can be identified experimentally. This suggests that the microphone arrays studied here can be employed in various acoustic applications to identify the characteristics of reverberant sound fields.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Z5Z8MN8R\\1.html},
  number = {5}
}

@article{PassiveSourceLocalization,
  title = {Passive {{Source Localization Employing Intersecting Spherical Surfaces}} from {{Time}} of {{Arrival Differences}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/3178418_Passive_Source_Localization_Employing_Intersecting_Spherical_Surfaces_from_Time_of_Arrival_Differences},
  abstract = {Passive Source Localization Employing Intersecting Spherical Surfaces from Time of Arrival Differences on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\M5WIBVEH\\3178418_Passive_Source_Localization_Employing_Intersecting_Spherical_Surfaces_from_Time_of_Arri.html}
}

@online{PassiveSourceLocalizationa,
  title = {Passive {{Source Localization Employing Intersecting Spherical Surfaces}} from {{Time}} of {{Arrival Differences}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/3178418_Passive_Source_Localization_Employing_Intersecting_Spherical_Surfaces_from_Time_of_Arrival_Differences},
  urldate = {2017-05-04},
  abstract = {Passive Source Localization Employing Intersecting Spherical Surfaces from Time of Arrival Differences on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NR7NSFAN\\3178418_Passive_Source_Localization_Employing_Intersecting_Spherical_Surfaces_from_Time_of_Arri.html}
}

@article{pavlidiRealTimeMultipleSound2013,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, D. and Griffin, A. and Puigt, M. and Mouchtaris, A.},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7916},
  doi = {10.1109/TASL.2013.2272524},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AHHKSGI4\\Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf;C\:\\Users\\sauli\\Zotero\\storage\\CP9FM3FJ\\6557035.html},
  keywords = {Arrays,audio signal processing,circular microphone array,computational complexity,direction of arrival,Direction of arrival estimation,direction-of-arrival estimation,DOA estimation,Estimation,linear array,matching pursuit,Matching pursuit algorithms,matching pursuit-based approach,microphone array signal processing,microphone array topology,microphone arrays,Microphones,multiple source localization,real-time localization,real-time multiple sound source counting method,real-time multiple sound source localization method,real-time systems,relaxed sparsity constraint,source counting,sparse component analysis,TF component,time-frequency analysis,time-frequency zone detection,uniform circular microphone array},
  number = {10}
}

@article{PavlidiRealTimeMultipleSound2013,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, D. and Griffin, A. and Puigt, M. and Mouchtaris, A.},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7916},
  doi = {10.1109/TASL.2013.2272524},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LHK2V7DH\\6557035.html},
  keywords = {Arrays,audio signal processing,circular microphone array,computational complexity,direction of arrival,Direction of arrival estimation,direction-of-arrival estimation,DOA estimation,Estimation,linear array,matching pursuit,Matching pursuit algorithms,matching pursuit-based approach,microphone array signal processing,microphone array topology,microphone arrays,Microphones,multiple source localization,real-time localization,real-time multiple sound source counting method,real-time multiple sound source localization method,real-time systems,relaxed sparsity constraint,source counting,sparse component analysis,TF component,time-frequency analysis,time-frequency zone detection,uniform circular microphone array},
  number = {10}
}

@article{pavlidiRealTimeMultipleSound2013a,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, D. and Griffin, A. and Puigt, M. and Mouchtaris, A.},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7916},
  doi = {10.1109/TASL.2013.2272524},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\D5VABZDT\\6557035.html},
  keywords = {Arrays,audio signal processing,circular microphone array,computational complexity,direction of arrival,Direction of arrival estimation,direction-of-arrival estimation,DOA estimation,Estimation,linear array,matching pursuit,Matching pursuit algorithms,matching pursuit-based approach,microphone array signal processing,microphone array topology,microphone arrays,Microphones,multiple source localization,real-time localization,real-time multiple sound source counting method,real-time multiple sound source localization method,real-time systems,relaxed sparsity constraint,source counting,sparse component analysis,TF component,time-frequency analysis,time-frequency zone detection,uniform circular microphone array},
  number = {10}
}

@article{PavlidiRealTimeMultipleSound2013a,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, D. and Griffin, A. and Puigt, M. and Mouchtaris, A.},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7916},
  doi = {10.1109/TASL.2013.2272524},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\T5IZL2F5\\Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf;C\:\\Users\\sauli\\Zotero\\storage\\N748P4NS\\6557035.html},
  keywords = {Arrays,audio signal processing,circular microphone array,computational complexity,direction of arrival,Direction of arrival estimation,direction-of-arrival estimation,DOA estimation,Estimation,linear array,matching pursuit,Matching pursuit algorithms,matching pursuit-based approach,microphone array signal processing,microphone array topology,microphone arrays,Microphones,multiple source localization,real-time localization,real-time multiple sound source counting method,real-time multiple sound source localization method,real-time systems,relaxed sparsity constraint,source counting,sparse component analysis,TF component,time-frequency analysis,time-frequency zone detection,uniform circular microphone array},
  number = {10}
}

@article{pavlidiRealTimeMultipleSound2013b,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, Despoina and Griffin, Anthony and Puigt, Matthieu and Mouchtaris, Athanasios},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2272524},
  url = {http://ieeexplore.ieee.org/document/6557035/},
  urldate = {2019-08-16},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CQJPJZ3R\\Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf},
  keywords = {*****,for multiple ssl assignment,MUST READ},
  langid = {english},
  number = {10}
}

@article{pavlidiRealTimeMultipleSound2013c,
  title = {Real-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  author = {Pavlidi, Despoina and Griffin, Anthony and Puigt, Matthieu and Mouchtaris, Athanasios},
  date = {2013-10},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  pages = {2193--2206},
  issn = {1558-7924},
  doi = {10.1109/TASL.2013.2272524},
  abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being significantly more efficient in terms of computational complexity.},
  eventtitle = {{{IEEE Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HMHPU2RH\\Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZNWWG3BT\\6557035.html},
  keywords = {*****,Arrays,audio signal processing,circular microphone array,computational complexity,direction of arrival,Direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,DOA estimation,Estimation,linear array,matching pursuit,Matching pursuit algorithms,matching pursuit-based approach,microphone array signal processing,microphone array topology,microphone arrays,Microphones,multiple source localization,MUST READ,real-time localization,real-time multiple sound source counting method,real-time multiple sound source localization method,Real-time systems,relaxed sparsity constraint,source counting,sparse component analysis,TF component,time-frequency analysis,Time-frequency analysis,time-frequency zone detection,uniform circular microphone array},
  number = {10}
}

@online{PDFAdvancementsImpulse,
  title = {({{PDF}}) {{Advancements}} in Impulse Response Measurements by Sine Sweeps},
  url = {https://www.researchgate.net/publication/228670187_Advancements_in_impulse_response_measurements_by_sine_sweeps},
  urldate = {2019-06-20},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2YHC7QRS\\228670187_Advancements_in_impulse_response_measurements_by_sine_sweeps.html}
}

@online{PDFInairPassive,
  title = {(7) ({{PDF}}) {{An}} in-Air Passive Acoustic Surveillance System for Air Traffic Control: {{GUARDIAN}} Project},
  url = {https://www.researchgate.net/publication/261169481_An_in-air_passive_acoustic_surveillance_system_for_air_traffic_control_GUARDIAN_project},
  urldate = {2019-07-29},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RZUZTIPP\\261169481_An_in-air_passive_acoustic_surveillance_system_for_air_traffic_control_GUARDIAN_proje.html}
}

@online{PDFMaximumlikelihoodBased,
  title = {(2) ({{PDF}}) {{Maximum}}-Likelihood Based {{3D}} Acoustical Signature Estimation},
  url = {https://www.researchgate.net/publication/271454031_Maximum-likelihood_based_3D_acoustical_signature_estimation},
  urldate = {2019-02-21},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5LUVK5BK\\271454031_Maximum-likelihood_based_3D_acoustical_signature_estimation.html}
}

@online{PDFPOSITIONINGUSING,
  title = {({{PDF}}) {{POSITIONING USING TIME}}-{{DIFFERENCE OF ARRIVAL MEASUREMENT S}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/239542057_POSITIONING_USING_TIME-DIFFERENCE_OF_ARRIVAL_MEASUREMENT_S},
  urldate = {2019-02-13},
  abstract = {PDF | The problem of position estimation from Time Difference Of Arrival (TDOA) measurements occurs in a range of ap- plications from wireless communication networks to elec- tronic warfare positioning. Correlation analysis of the tr ans- mitted signal to two receivers gives rise...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XGC56LLK\\239542057_POSITIONING_USING_TIME-DIFFERENCE_OF_ARRIVAL_MEASUREMENT_S.html},
  langid = {english}
}

@online{PDFRealTimeMultiple,
  title = {(7) ({{PDF}}) {{Real}}-{{Time Multiple Sound Source Localization}} and {{Counting Using}} a {{Circular Microphone Array}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/256275817_Real-Time_Multiple_Sound_Source_Localization_and_Counting_Using_a_Circular_Microphone_Array},
  urldate = {2019-08-16},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TDHH6BA4\\(7) (PDF) Real-Time Multiple Sound Source Localization and Counting Using a.pdf;C\:\\Users\\sauli\\Zotero\\storage\\MGMXGWLX\\256275817_Real-Time_Multiple_Sound_Source_Localization_and_Counting_Using_a_Circular_Microphone.html},
  langid = {english}
}

@inproceedings{perez-lopezAmbiscaperToolAutomatic2018,
  title = {Ambiscaper: {{A Tool}} for {{Automatic Generation}} and {{Annotation}} of {{Reverberant Ambisonics Sound Scenes}}},
  booktitle = {2018 16th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Perez-Lopez, A.},
  date = {2018-09},
  pages = {1--9},
  doi = {10.1109/IWAENC.2018.8521341},
  abstract = {The spatial nature of Ambisonics audio recordings makes them specially interesting for source localization and source separation tasks. Although existing proposals have already shown promising results, there is a lack of annotated databases, required for algorithm design, training and evaluation in this particular scenario. In this paper we present AmbiScaper, a tool for the automatic generation and annotation of reverberant Ambisonics sound scenes, published under an Open Source license to facilitate research reproducibility. As a use case, a database of annotated audio scenes has been created and published, along with an analysis with state-of-the-art sound localization algorithms.},
  keywords = {Acoustics,Ambisonics,Audio recording,Data models,Databases,dataset generation,Direction-of-arrival estimation,reverberation,sound localization,source separation,Source separation,Tools}
}

@inproceedings{petrellaPerformaceAnalysisAcoustic2014,
  title = {Performace Analysis of Acoustic Antennas},
  booktitle = {2014 {{IEEE Metrology}} for {{Aerospace}} ({{MetroAeroSpace}})},
  author = {Petrella, Orsola and Ameduri, Salvatore and Quaranta, Vincenzo and Betta, Giovanni and Laracca, Marco},
  date = {2014-05},
  pages = {560--564},
  publisher = {{IEEE}},
  location = {{Benevento, Italy}},
  doi = {10.1109/MetroAeroSpace.2014.6865988},
  url = {http://ieeexplore.ieee.org/document/6865988/},
  urldate = {2019-07-29},
  abstract = {One of the most common applications of the acoustic antennas is the detection and the localization of acoustic sources. Different parameters (geometric position of the microphones in the array, distance between antenna and the sources, quality of the used microphones, data acquisition system, and so on) influence the quality of the measure of acoustic sources position. In this paper, the authors present an analysis of the effect of the above mentioned parameters to the acoustic antennas performances in terms of uncertainty in the measure of sources position. These information can be used in the design phase allowing the correct choice of the different parts composing the acoustic antenna system in order to ensure the desired measurement uncertainty. The analysis was carried out in a simulation environment (using a dedicated numerical model to theoretically predict the signals produced by each microphone) finally experimentally tested on both linear and bi-dimensional shape acoustic antennas.},
  eventtitle = {2014 {{IEEE Metrology}} for {{Aerospace}} ({{MetroAeroSpace}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PCE59B8Q\\Petrella et al. - 2014 - Performace analysis of acoustic antennas.pdf},
  isbn = {978-1-4799-2069-3},
  langid = {english}
}

@inproceedings{pezzoliEstimationSoundField2018,
  title = {Estimation of the {{Sound Field}} at {{Arbitrary Positions}} in {{Distributed Microphone Networks Based}} on {{Distributed Ray Space Transform}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Pezzoli, M. and Borra, F. and Antonacci, F. and Sarti, A. and Tubaro, S.},
  date = {2018-04},
  pages = {186--190},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462634},
  abstract = {In this paper we propose a parametric sound field reconstruction approach. In particular, the technique is based on the estimation of three parameters for each acoustic source (source position, radiation pattern and source signal) given the signals acquired by few arbitrarily placed microphone arrays. This allows us to synthesize the signal of a virtual microphone placed in any point of the acoustic scene.},
  keywords = {acoustic field,acoustic scene,acoustic signal detection,acoustic signal processing,acoustic source,Acoustics,Antenna radiation patterns,arbitrarily placed microphone arrays,arbitrary positions,array signal processing,distributed microphone networks,distributed ray space transform,Estimation,Indexes,microphone arrays,Microphone arrays,parametric sound field reconstruction approach,radiation pattern,source localization,source position,source signal,Transforms,virtual microphone}
}

@article{picautSoundFieldModeling1999,
  title = {Sound Field Modeling in Streets with a Diffusion Equation},
  author = {Picaut, J. and Simon, L. and Hardy, J.},
  date = {1999-10-28},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {106},
  pages = {2638--2645},
  issn = {0001-4966},
  doi = {10.1121/1.428093},
  url = {http://asa.scitation.org/doi/10.1121/1.428093},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WBSX9CQU\\1.html},
  number = {5}
}

@article{PIIS0167865502,
  title = {{{PII}}: {{S0167}}-8655(02)00204-0 - {{Automatic}}\_identification\_of\_sound\_source.{{Pdf}}},
  url = {http://www.multimed.org/papers/automatic_identification_of_sound_source.pdf}
}

@online{PIIS0167865502a,
  title = {{{PII}}: {{S0167}}-8655(02)00204-0 - Automatic\_identification\_of\_sound\_source.Pdf},
  url = {http://www.multimed.org/papers/automatic_identification_of_sound_source.pdf},
  urldate = {2017-05-03}
}

@article{porteousCorrectionMethodAcoustic2018,
  title = {A Correction Method for Acoustic Source Localisation in Convex Shear Layer Geometries},
  author = {Porteous, Ric and Geyer, Thomas and Moreau, Danielle J. and Doolan, Con J.},
  date = {2018-01-15},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {130},
  pages = {\{128-132\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2017.09.020\}},
  abstract = {In this paper, a new method is presented to correct for the distortions in beamforming maps caused by wind tunnel shear layers during aeroacoustic testing. The shear layer correction method can be used for any convex shear layer shape including circles, ellipses, triangles, rectangles, octagons and squares. After deriving the methodology and proving its equivalence to one-dimensional methods, the shear layer correction method is used to correct the beamforming maps obtained experimentally from an airfoil placed in the potential core of a circular jet. The results show that the method can successfully remove the distorting effects of the shear layer on the airfoil trailing edge source distribution. Thus a new method is available to researchers to correct for the effects of shear layers in three-dimensions, which may also be extended to three-dimensional beamforming.},
  affiliation = {Moreau, DJ (Reprint Author), UNSW Sydney, Sch Mech & Mfg Engn, Sydney, NSW 2052, Australia. Porteous, Ric, Univ Adelaide, Sch Mech Engn, Adelaide, SA 5005, Australia. Geyer, Thomas, Brandenburg Tech Univ Cottbus, Tech Acoust Grp, D-03046 Cottbus, Germany. Moreau, Danielle J.; Doolan, Con J., UNSW Sydney, Sch Mech & Mfg Engn, Sydney, NSW 2052, Australia.},
  author-email = {Porteous.Ric@bcg.com thomas.geyer@b-tu.de d.moreau@unsw.edu.au c.doolan@unsw.edu.au},
  cited-references = {AHUJA KK, 1981, J SOUND VIB, V75, P51, DOI 10.1016/0022-460X(81)90235-2. Amiet R. K., 1975, 2 AIAA AER C. AMIET RK, 1978, J SOUND VIB, V58, P467, DOI 10.1016/0022-460X(78)90353-X. Bahr C, 2010, 16 AIAA CEAS AER C. Bahr CJ, 2014, 20 AIAA CEAS AER C. Candel S, 1976, 3 AER C. Cebeci T, 1977, MOMENTUM TRANSFER BO. Cigada A, 2007, MECH SYST SIGNAL PR, V21, P2645, DOI 10.1016/j.ymssp.2007.01.001. Geyer T, 2010, EXP FLUIDS, V48, P291, DOI 10.1007/s00348-009-0739-x. Koop L, 2005, 11 AIAA CEAS AER C. Moreau DJ, 2014, J SOUND VIB, V333, P6924, DOI 10.1016/j.jsv.2014.08.005. Morfey CL, 2001, J SOUND VIB, V239, P819, DOI 10.1006/jsvi.2000.3218. Mueller TJ, 2013, AEROACOUSTIC MEASURE. Padois T, 2013, 19 AIAA CEAS AER C. Padois T, 2013, APPL ACOUST, V74, P591, DOI 10.1016/j.apacoust.2012.09.013. Pierce A. D., 1989, ACOUSTICS INTRO ITS. Plumblee Jr HEE, 1976, CR2702 NASA. RIBNER HS, 1957, J ACOUST SOC AM, V29, P435, DOI 10.1121/1.1908918. SARRADJ E, 2012, ADV ACOUSTICS VIBRAT. Sarradj E., 2016, 22 AIAA CEAS AER C. Schlinker R. H., 1980, 3371 NASA. TESTER BJ, 1976, J SOUND VIB, V46, P79, DOI 10.1016/0022-460X(76)90819-1. Wei L, 2017, MECH SYST SIGNAL PR, V88, P240, DOI 10.1016/j.ymssp.2016.11.011.},
  da = {2018-10-18},
  doc-delivery-number = {FN7HP},
  eissn = {1872-910X},
  journal-iso = {Appl. Acoust.},
  keywords = {(Aeroacoustics,Beamforming,Shear layer correction,Wind tunnel testing)},
  keywords-plus = {FLOW; NOISE; VALIDATION; SOUND},
  langid = {english},
  number-of-cited-references = {23},
  research-areas = {Acoustics},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000416189600014},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Acoustics}
}

@online{PositioningUsingTimedifference,
  title = {Positioning Using Time-Difference of Arrival Measurements - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/1201741},
  urldate = {2019-02-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Y2LRN2GU\\1201741.html}
}

@inproceedings{postolacheUnderwaterAcousticSource2007,
  title = {Underwater {{Acoustic Source Localization Based}} on {{Passive Sonar}} and {{Intelligent Processing}}},
  booktitle = {2007 {{IEEE Instrumentation Measurement Technology Conference IMTC}} 2007},
  author = {Postolache, O. and Girao, P. and Pereira, M.},
  date = {2007-05},
  pages = {1--4},
  doi = {10.1109/IMTC.2007.379152},
  abstract = {The work presents a distributed measuring system designed and implemented for underwater acoustic source detection and localization. The main part of the system is a sonar unit with three hydrophones connected to a conditioning block controlled by an acquisition, control and processing unit expressed by a real-time controller, a FPGA core and a set of I/O modules. The conditioned signals are acquired and processed at the real time controller level using wavelets filtering algorithms in order to extract the relative time delay information. An intelligent algorithm based on a neural network uses the time delays and the sonar structure azimuth as input values to calculate the range and bearing angle associated with the detected underwater acoustic source. Elements of software implementation that uses LabVIEWreal-time and LabVIEWFPGA modules, wavelet optimal filtering study and localization results are included in the paper.},
  eventtitle = {2007 {{IEEE Instrumentation Measurement Technology Conference IMTC}} 2007},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BBNX87QP\\Postolache et al. - 2007 - Underwater Acoustic Source Localization Based on P.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZQEIHZWM\\4258480.html},
  keywords = {Acoustic measurements,acoustic signal detection,acoustic signal processing,acoustic source tracking,Control systems,Delay effects,distributed measuring system,field programmable gate arrays,filtering theory,FPGA,hydrophones,intelligent processing,neural nets,neural network,passive sonar,Process control,Sonar detection,Sonar equipment,Sonar measurements,Time delay estimation,time delay information,underwater acoustic source detection,underwater acoustic source localization,Underwater acoustics,underwater sound,Underwater tracking,wavelet transforms,wavelets filtering algorithms}
}

@online{POTSPSTNTieline,
  title = {{{POTS}}/{{PSTN}} - {{Tieline Technology}}},
  url = {http://www.tieline.com/Transports/POTSPSTN},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\8C8X5RXU\\POTSPSTN.html}
}

@inproceedings{pulkki_directional_2009,
  title = {Directional Audio Coding-Perception-Based Reproduction of Spatial Sound},
  booktitle = {Int. {{Workshop}} on the {{Principles}} and {{Applications}} of {{Spatial Hearing}}, {{Miyagi}}, {{Japan}}},
  author = {Pulkki, V. and Laitinen, M. V. and Vilkamo, J. and Ahonen, J. and Lokki, T. and Pihlajamäki, T.},
  date = {2009},
  url = {https://mediatech.aalto.fi/~ktlokki/Publs/pulkki_iwpash.pdf},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UQB8C86X\\Directional audio coding - perception-based reproduction of spatial sound.pdf}
}

@incollection{pulkkinenSemisupervisedLearningWLAN2011,
  title = {Semi-Supervised {{Learning}} for {{WLAN Positioning}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2011},
  author = {Pulkkinen, Teemu and Roos, Teemu and Myllymäki, Petri},
  editor = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
  date = {2011},
  volume = {6791},
  pages = {355--362},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21735-7_44},
  url = {http://link.springer.com/10.1007/978-3-642-21735-7_44},
  urldate = {2019-12-02},
  abstract = {Currently the most accurate WLAN positioning systems are based on the fingerprinting approach, where a “radio map” is constructed by modeling how the signal strength measurements vary according to the location. However, collecting a sufficient amount of location-tagged training data is a rather tedious and time consuming task, especially in indoor scenarios — the main application area of WLAN positioning —where GPS coverage is unavailable. To alleviate this problem, we present a semi-supervised manifold learning technique for building accurate radio maps from partially labeled data, where only a small portion of the signal strength measurements need to be tagged with the corresponding coordinates. The basic idea is to construct a non-linear projection that maps high-dimensional signal fingerprints onto a two-dimensional manifold, thereby dramatically reducing the need of location-tagged data. Our results from a deployment in a real-world experiment demonstrate the practical utility of the method.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WSDYV976\\Pulkkinen et al. - 2011 - Semi-supervised Learning for WLAN Positioning.pdf},
  isbn = {978-3-642-21734-0 978-3-642-21735-7},
  langid = {english}
}

@online{PythonModelNot,
  title = {Python - {{Model}} Not Learning - Keras},
  journaltitle = {Stack Overflow},
  url = {https://stackoverflow.com/questions/53177605/model-not-learning-keras},
  urldate = {2019-12-02},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UPG4QE8M\\model-not-learning-keras.html}
}

@inproceedings{qinqixuSoundSourceLocalization2012,
  title = {Sound Source Localization System Based on Mobile Robot},
  booktitle = {2012 24th {{Chinese Control}} and {{Decision Conference}} ({{CCDC}})},
  author = {{Qinqi Xu} and {Peng Yang} and {Jinwei Wang} and {Hao Sun}},
  date = {2012-05},
  pages = {204--207},
  doi = {10.1109/CCDC.2012.6244029},
  abstract = {The method of auditory system design based on time difference is proposed. According to the special character of sound signal and basic need of collection technology, it had been explored to the microphone choice and the signal pretreatment. Sound source position is calculated by the tetrahedron array of four microphones, cross-correlation method is used to count delay. Experiment results show that the design can make the robot locate the azimuth of the sound source accurately and is practical enough for real-time operation.},
  eventtitle = {2012 24th {{Chinese Control}} and {{Decision Conference}} ({{CCDC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WA98NEXT\\Qinqi Xu et al_2012_Sound source localization system based on mobile robot.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3UNFB848\\6244029.html},
  keywords = {acoustic signal processing,Acoustics,Arrays,Auditory,Auditory system,auditory system design,cross-correlation method,Electrets,microphone arrays,microphone choice,Microphones,mobile robot,Mobile robot,mobile robots,Mobile robots,Perception,signal pretreatment,Sound localization,sound source localization system,tetrahedron array}
}

@article{quarantaBeamformingTechniqueAirplane,
  title = {Beam-Forming Technique for Airplane Localization and Tracking},
  author = {Quaranta, Vincenzo and D’Altrui, Gaetano and Calisti, Claudio and Lecce, Leonardo and Napoli, Gianluca Di},
  pages = {11},
  abstract = {The results of a comprehensive investigation on problems, algorithms and solutions, related to the process of acoustic sources localization, are reported in this paper. A review of theoretical and practical aspects of sound sources detection by using microphone array techniques is presented. A dedicated code, based on classical delay and sum beam-forming technique, has been developed within MATLAB environment: starting from the acquisition of the signals, as measured from the array transducers, the propagation direction of the sound emitted by a moving noise source can be identified and tracked. A numerical application of the investigated technique has been carried out by simulating the microphone signals due to an aircraft flyover. The results show that this procedure provides good localization, also in presence of two or more different noise sources and at considerable distance from the array. The error on the estimation of the incoming propagation direction can be minimized with an accurate design of the array size, number of sensors and shape. The design criteria of the array are also reported in details.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4LGGTS78\\Quaranta et al. - Beam-forming technique for airplane localization a.pdf},
  keywords = {**},
  langid = {english}
}

@inproceedings{rajamakiComparisonSparseSensor2017,
  title = {Comparison of Sparse Sensor Array Configurations with Constrained Aperture for Passive Sensing},
  booktitle = {2017 {{IEEE Radar Conference}} ({{RadarConf}})},
  author = {Rajamaki, Robin and Koivunen, Visa},
  date = {2017-05},
  pages = {0797--0802},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/RADAR.2017.7944312},
  url = {http://ieeexplore.ieee.org/document/7944312/},
  urldate = {2019-07-28},
  abstract = {Sparse sensor arrays can match the performance of fully populated arrays in many tasks, such as direction-of-arrival estimation, using substantially fewer elements. However, finding the sparse array configuration that uses the smallest number of elements is generally a hard problem. Consequently, several closed-form, but sub-optimal solutions have been developed in the past. These designs are typically specified for a given number of elements, although when the area occupied by the array is the main limitation, it is more convenient to compare arrays of similar aperture instead. This paper outlines procedures for synthesizing three sparse linear array geometries for a specified aperture, namely the Wichmann, Nested and Super nested array. These configurations are compared to the optimal Minimumredundancy array and their deviation from optimality is quantified in the limit of large apertures.},
  eventtitle = {2017 {{IEEE Radar Conference}} ({{RadarConf17}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\NJBAN3M5\\Rajamaki and Koivunen - 2017 - Comparison of sparse sensor array configurations w.pdf},
  isbn = {978-1-4673-8823-8},
  langid = {english}
}

@inproceedings{rajapakshaGeometricalRoomGeometry2016,
  title = {Geometrical Room Geometry Estimation from Room Impulse Responses},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2016 {{IEEE International Conference}} On},
  author = {Rajapaksha, Tilak and Qiu, Xiaojun and Cheng, Eva and Burnett, Ian},
  date = {2016},
  pages = {331--335},
  publisher = {{IEEE}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G7CN2ZPS\\Rajapaksha et al. - 2016 - Geometrical room geometry estimation from room imp.pdf}
}

@article{ramosAmbientIntelligenceNext2008,
  title = {Ambient {{Intelligence}}—the {{Next Step}} for {{Artificial Intelligence}}},
  author = {Ramos, Carlos and Augusto, Juan Carlos and Shapiro, Daniel},
  date = {2008-03},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {23},
  pages = {15--18},
  issn = {1541-1672},
  doi = {10.1109/MIS.2008.19},
  url = {http://ieeexplore.ieee.org/document/4475854/},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HHVCEJV6\\Ramos et al. - 2008 - Ambient Intelligence—the Next Step for Artificial .pdf},
  langid = {english},
  number = {2}
}

@article{raoPerformanceAnalysisRootMusic1989,
  title = {Performance Analysis of {{Root}}-{{Music}}},
  author = {Rao, B. D. and Hari, K. V. S.},
  date = {1989-12},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  pages = {1939--1949},
  issn = {0096-3518},
  doi = {10.1109/29.45540},
  abstract = {The authors analyze the performance of Root-Music, a variation of the MUSIC algorithm, for estimating the direction of arrival (DOA) of plane waves in white noise in the case of a linear equispaced sensor array. The performance of the method is analyzed by examining the perturbation in the roots of the polynomial formed in the intermediate step of Root-Music. In particular, asymptotic results for the mean squared error in the estimates of the direction of arrival are derived. Simplified expressions are presented for the one- and two-source case and compared to those obtained for least-squares ESPRIT. Computer simulations are also presented, and they are in close agreement with the theory. An important outcome of this analysis is the fact that the error in the signal zeros has a largely radial component. This provides an explanation as to why the Root-Music is superior to the spectral MUSIC algorithm},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9PQ5EM6D\\Rao and Hari - 1989 - Performance analysis of Root-Music.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5PNWJIF7\\45540.html},
  keywords = {Algorithm design and analysis,Computer errors,Computer simulation,direction of arrival,Direction of arrival estimation,least-squares ESPRIT,linear equispaced sensor array,Multiple signal classification,MUSIC algorithm,Performance analysis,plane waves,polynomial,polynomials,radial component,ROOT-MUSIC,Sensor arrays,Signal analysis,signal detection,simulations,spectral analysis,white noise},
  number = {12}
}

@article{RaoPerformanceanalysisRootMusic1989,
  title = {Performance {{Analysis}} of {{Root}}-{{Music}}},
  author = {Rao, B. D. and Hari, K. V. S.},
  date = {1989-12},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  pages = {1939--1949},
  issn = {0096-3518},
  doi = {10.1109/29.45540},
  abstract = {The authors analyze the performance of Root-Music, a variation of the MUSIC algorithm, for estimating the direction of arrival (DOA) of plane waves in white noise in the case of a linear equispaced sensor array. The performance of the method is analyzed by examining the perturbation in the roots of the polynomial formed in the intermediate step of Root-Music. In particular, asymptotic results for the mean squared error in the estimates of the direction of arrival are derived. Simplified expressions are presented for the one- and two-source case and compared to those obtained for least-squares ESPRIT. Computer simulations are also presented, and they are in close agreement with the theory. An important outcome of this analysis is the fact that the error in the signal zeros has a largely radial component. This provides an explanation as to why the Root-Music is superior to the spectral MUSIC algorithm},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X65C4S7J\\Rao and Hari - 1989 - Performance analysis of Root-Music.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SJV22KFX\\45540.html},
  keywords = {Algorithm design and analysis,Computer errors,Computer simulation,direction of arrival,Direction of arrival estimation,least-squares ESPRIT,linear equispaced sensor array,Multiple signal classification,MUSIC algorithm,Performance analysis,plane waves,polynomial,polynomials,radial component,ROOT-MUSIC,Sensor arrays,Signal analysis,signal detection,simulations,spectral analysis,white noise},
  number = {12}
}

@article{rasconLocalizationSoundSources2017,
  title = {Localization of Sound Sources in Robotics: {{A}} Review},
  shorttitle = {Localization of Sound Sources in Robotics},
  author = {Rascon, Caleb and Meza, Ivan},
  date = {2017-10-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {96},
  pages = {184--210},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2017.07.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889016304742},
  urldate = {2018-10-18},
  abstract = {Sound source localization (SSL) in a robotic platform has been essential in the overall scheme of robot audition. It allows a robot to locate a sound source by sound alone. It has an important impact on other robot audition modules, such as source separation, and it enriches human–robot interaction by complementing the robot’s perceptual capabilities. The main objective of this review is to thoroughly map the current state of the SSL field for the reader and provide a starting point to SSL in robotics. To this effect, we present: the evolution and historical context of SSL in robotics; an extensive review and classification of SSL techniques and popular tracking methodologies; different facets of SSL as well as its state-of-the-art; evaluation methodologies used for SSL; and a set of challenges and research motivations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JDSISF8M\\Rascon and Meza - 2017 - Localization of sound sources in robotics A revie.pdf;C\:\\Users\\sauli\\Zotero\\storage\\HXXQ4SNK\\S0921889016304742.html},
  keywords = {Direction-of-arrival,Distance estimation,Robot audition,Sound source localization,Tracking}
}

@article{reche-lopezBinauralLateralLocalization2018,
  title = {Binaural Lateral Localization of Multiple Sources in Real Environments Using a Kurtosis-Driven Split-{{EM}} Algorithm},
  author = {Reche-Lopez, P. and Perez-Lorenzo, J. M. and Rivas, F. and Viciana-Abad, R.},
  date = {2018-03},
  journaltitle = {ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE},
  volume = {69},
  pages = {\{137-146\}},
  publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
  issn = {0952-1976},
  doi = {\{10.1016/j.engappai.2017.12.013\}},
  abstract = {In this work a method for an unsupervised lateral localization of simultaneous sound sources is presented. Following a binaural approach, the kurtosis-driven split-EM algorithm (KDS-EM) implemented is able to estimate the direction of arrival of relevant sound sources without knowing a priori their number. Information about the localization is integrated within a period of observation time to serve as an auditory memory in the context of social robotics. Experiments have been conducted using two types of observation times, one shorter with the purpose of analyzing its performance in a reactive level, and other longer that allows the analysis of its contribution as an input of the building process of the sorroundings auditory models that serves to drive a more deliberative behavior. The system has been tested in real and reverberant environments, achieving a good performance based on an over-modeling process that is able to isolate the location of the relevant sources from adverse acoustic effects, such as reverberations. (C) 2018 Elsevier Ltd. All rights reserved.},
  affiliation = {Reche-Lopez, P (Reprint Author), Univ Jaen, Adv Studies Ctr Informat & Commun Technol CEATIC, Jaen, Spain. Reche-Lopez, P.; Viciana-Abad, R., Univ Jaen, Adv Studies Ctr Informat & Commun Technol CEATIC, Jaen, Spain. Perez-Lorenzo, J. M.; Rivas, F., Univ Jaen, Multimedia & Multimodal Proc Grp M2P, Campus Cient Tecnol Linares,Avda Univ S-N, Jaen 23700, Spain.},
  author-email = {pjreche@ujaen.es},
  cited-references = {Argentieri S, 2015, COMPUT SPEECH LANG, V34, P87, DOI 10.1016/j.csl.2015.03.003. Argentieri S., 2013, TECHNOLOGY BINAURAL, P225. Basiri M, 2016, IEEE ROBOT AUTOM LET, V1, P820, DOI 10.1109/LRA.2016.2527833. Beyer H.-G., 2002, Natural Computing, V1, P3, DOI 10.1023/A:1015059928466. Bregman A. S., 1990, AUDITORY SCENE ANAL. Burgard W, 1999, MACHINE LEARNING, PROCEEDINGS, P67. Calderita L. V., 2014, JMIR REHABIL ASSIST, V1. Cherry E. C, 1957, HUMAN COMMUNICATION. CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229. Cobos M, 2011, DIGIT SIGNAL PROCESS, V21, P66, DOI 10.1016/j.dsp.2010.04.003. DeLiang W., 2006, COMPUTATIONAL AUDITO, P1. DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1. Dietz M, 2011, SPEECH COMMUN, V53, P592, DOI 10.1016/j.specom.2010.05.006. Escolano J, 2014, J ACOUST SOC AM, V135, P742, DOI 10.1121/1.4861356. Ferreira JF, 2013, IEEE T CYBERNETICS, V43, P699, DOI 10.1109/TSMCB.2012.2214477. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Kohlrausch A., 2013, TECHNOVATIO IN PRESS, P1. LU ZL, 1992, SCIENCE, V258, P1668, DOI 10.1126/science.1455246. May T., 2013, TECHNOLOGY BINAURAL, P397. Nikunen J, 2016, SPEECH COMMUN, V76, P157, DOI 10.1016/j.specom.2015.09.005. Perez-Lorenzo JM, 2012, APPL ACOUST, V73, P698, DOI 10.1016/j.apacoust.2012.02.002. REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034. Rissanen J., 1989, STOCHASTIC COMPLEXIT, V15. Roos T., 2002, IEEE Transactions on Mobile Computing, V1, P59, DOI 10.1109/TMC.2002.1011059. SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136. Stern R., 2006, COMPUTATIONAL AUDITO, P147. Thrun S, 1998, AUTON ROBOT, V5, P253, DOI 10.1023/A:1008806205438. Viciana-Abad R, 2014, SENSORS-BASEL, V14, P9522, DOI 10.3390/s140609522. Vlassis N, 1999, IEEE T SYST MAN CY A, V29, P393, DOI 10.1109/3468.769758. Wang L, 2016, IEEE-ACM T AUDIO SPE, V24, P1079, DOI 10.1109/TASLP.2016.2533859. Wei Lu, 2006, WSEAS Transactions on Computers, V5, P1795. Wu Y, 2002, ENG APPL ARTIF INTEL, V15, P139, DOI 10.1016/S0952-1976(02)00025-8. Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.V8896. Zhang WY, 2010, IEEE T AUDIO SPEECH, V18, P1913, DOI 10.1109/TASL.2010.2040525.},
  da = {2018-10-18},
  doc-delivery-number = {FV6VP},
  eissn = {1873-6769},
  funding-acknowledgement = {Economy and Competitiveness Department of the Spanish Government; European Regional Development Fund [TIN2015-65686-C5-2-R]},
  funding-text = {This work has been supported by Economy and Competitiveness Department of the Spanish Government and European Regional Development Fund under the project TIN2015-65686-C5-2-R (MINECO/FEDER, UE). In particular, this work is associated to the development of a person perceptor agent.},
  journal-iso = {Eng. Appl. Artif. Intell.},
  keywords = {(Robot audition,KDS-EM,Laplacian model mixture,Multiple sound localization,Mutational split)},
  keywords-plus = {MAXIMUM-LIKELIHOOD; ARRAY; MODEL; MICROPHONES; PERCEPTION; SOUND},
  langid = {english},
  number-of-cited-references = {34},
  orcid-numbers = {Reche-Lopez, Pedro/0000-0002-5417-3551},
  research-areas = {Automation & Control Systems; Computer Science; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000424720500011},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic}
}

@article{reche-lopezBinauralLateralLocalization2018a,
  title = {Binaural Lateral Localization of Multiple Sources in Real Environments Using a Kurtosis-Driven Split-{{EM}} Algorithm},
  author = {Reche-Lopez, P. and Perez-Lorenzo, J. M. and Rivas, F. and Viciana-Abad, R.},
  date = {2018-03-01},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {69},
  pages = {137--146},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2017.12.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0952197617303160},
  urldate = {2018-10-18},
  abstract = {In this work a method for an unsupervised lateral localization of simultaneous sound sources is presented. Following a binaural approach, the kurtosis-driven split-EM algorithm (KDS-EM) implemented is able to estimate the direction of arrival of relevant sound sources without knowing a priori their number. Information about the localization is integrated within a period of observation time to serve as an auditory memory in the context of social robotics. Experiments have been conducted using two types of observation times, one shorter with the purpose of analyzing its performance in a reactive level, and other longer that allows the analysis of its contribution as an input of the building process of the sorroundings auditory models that servesto drive a more deliberative behavior. The system has been tested in real and reverberant environments, achieving a good performance based on an over-modeling process that is able to isolate the location of the relevant sources from adverse acoustic effects, such as reverberations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\86BDZ36N\\S0952197617303160.html},
  keywords = {KDS-EM,Laplacian model mixture,Multiple sound localization,Mutational split,Robot audition}
}

@article{reedTimeDelayEstimation1981,
  title = {Time Delay Estimation Using the {{LMS}} Adaptive Filter–{{Static}} Behavior},
  author = {Reed, F. and Feintuch, P. and Bershad, N.},
  date = {1981-06},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {29},
  pages = {561--571},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1981.1163614},
  abstract = {A new application of the LMS adaptive filter, that of determining the time delay in a signal between two split-array outputs, is described. In a split array sonar, this time delay can be converted to the bearing of the target radiating the signal. The performance of such a tracker is analyzed for stationary broad-band targets. It is shown that a continuous adaptive tracker performs within 0.5 dB of the Cramér-Rao lower bound. Further, performance predictions are developed for a discrete adaptive tracker which demonstrates excellent agreement with simulations. It is shown that the adaptive tracker can have significantly less sensitivity to changing input spectra than a conventional tracker using a fixed input filter.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JHNW4SCF\\Reed et al. - 1981 - Time delay estimation using the LMS adaptive filte.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5M6H3GUF\\1163614.html},
  keywords = {adaptive filters,array signal processing,Delay effects,Delay estimation,Finite impulse response filter,Least squares approximation,Noise cancellation,Sonar,Target tracking,Wiener filter},
  number = {3}
}

@inproceedings{remaggiAcousticReflectorLocalization2018,
  title = {Acoustic {{Reflector Localization}} and {{Classification}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Remaggi, L. and Kim, H. and Jackson, P. J. B. and Fazi, F. M. and Hilton, A.},
  date = {2018-04},
  pages = {201--205},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462146},
  abstract = {The process of understanding acoustic properties of environments is important for several applications, such as spatial audio, augmented reality and source separation. In this paper, multichannel room impulse responses are recorded and transformed into their direction of arrival (DOA)-time domain, by employing a superdirective beamformer. This domain can be represented as a 2D image. Hence, a novel image processing method is proposed to analyze the DOA-time domain, and estimate the reflection times of arrival and DOAs. The main acoustically reflective objects are then localized. Recent studies in acoustic reflector localization usually assume the room to be free from furniture. Here, by analyzing the scattered reflections, an algorithm is also proposed to binary classify reflectors into room boundaries and interior furniture. Experiments were conducted in four rooms. The classification algorithm showed high quality performance, also improving the localization accuracy, for non-static listener scenarios.},
  keywords = {acoustic reflector localization,acoustic signal processing,Acoustics,array signal processing,augmented reality,Azimuth,Beamforming,binary classify reflectors,direction-of-arrival estimation,Direction-of-arrival estimation,DOA-time domain,Estimation,image processing,image processing method,Microphone arrays,multichannel room impulse responses,reflection times of arrival,Reflector Localization,Reflector Size Classification,Room Impulse Response,source separation,superdirective beamformer,time-of-arrival estimation,Two dimensional displays}
}

@article{Research,
  title = {Research},
  journaltitle = {Sander Dieleman},
  url = {http://benanne.github.io/research/},
  abstract = {Machine learning, deep learning, music information retrieval, recommender systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JTLH3KVI\\research.html}
}

@online{Researcha,
  title = {Research},
  journaltitle = {Sander Dieleman},
  url = {http://benanne.github.io/research/},
  urldate = {2017-02-18},
  abstract = {Machine learning, deep learning, music information retrieval, recommender systems},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GK4PUMSZ\\research.html}
}

@online{Resources,
  title = {Resources},
  url = {http://www.ravenna-network.com/about-ravenna/resources/},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\TMVWGFGR\\resources.html}
}

@inproceedings{ribeiroSupervisedIsomapDissimilarity2008,
  title = {Supervised {{Isomap}} with {{Dissimilarity Measures}} in {{Embedding Learning}}},
  author = {Ribeiro, Bernardete and Vieira, Armando and Carvalho das Neves, Joao},
  date = {2008-09-09},
  pages = {389--396},
  doi = {10.1007/978-3-540-85920-8_48},
  abstract = {In this paper we propose a supervised version of the Isomap algorithm by incorporating class label information into a dissimilarity
matrix in a financial analysis setting. On the credible assumption that corporates financial status lie on a low dimensional
manifold, nonlinear dimensionality reduction based on manifold learning techniques has strong potential for bankruptcy analysis
in financial applications. We apply the method to a real data set of distressed and healthy companies for proper geometric
tunning of similarity cases. We show that the accuracy of the proposed approach is comparable to the state-of-the-art Support
Vector Machines (SVM) and Relevance Vector Machines (RVM) despite the fewer dimensions used resulting from embedding learning.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9JQA639H\\Ribeiro et al. - 2008 - Supervised Isomap with Dissimilarity Measures in E.pdf}
}

@article{rivaAmbientIntelligenceVision,
  title = {Ambient {{Intelligence}}: From {{Vision}} to {{Reality}}},
  author = {Riva, G and Vatalaro, F and Davide, F and Alcañiz, M},
  journaltitle = {Ambient Intelligence},
  pages = {24},
  abstract = {According to the ISTAG vision statement, humans will, in an Ambient Intelligent (AmI) Environment, be surrounded by intelligent interfaces supported by computing and networking technology that is embedded in everyday objects such as furniture, clothes, vehicles, roads and smart materials - even particles of decorative substances like paint. AmI implies a seamless environment of computing, advanced networking technology and specific interfaces. This environment should be aware of the specific characteristics of human presence and personalities; adapt to the needs of users; be capable of responding intelligently to spoken or gestured indications of desire; and even result in systems that are capable of engaging in intelligent dialogue. Ambient Intelligence should also be unobtrusive - interaction should be relaxing and enjoyable for the citizen, and not involve a steep learning curve. Insofar as AmI is “a set of properties of an environment that we are in the process of creating”, ISTAG does not think it necessary to more tightly define the term Ambient Intelligence. But it is important to appreciate that AmI remains an ‘emerging property’ and that future scenario building and iterations of the vision should treat AmI as an ‘imagined concept’ and not as a set of specified requirements.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4ZL9WY8R\\Riva et al. - 4 Ambient Intelligence from Vision to Reality.pdf},
  langid = {english}
}

@incollection{robertDirectionalHearingInsects2005,
  title = {Directional {{Hearing}} in {{Insects}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Robert, Daniel},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {6--35},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_2},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_2},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5FQUWQ4Z\\0-387-28863-5_2.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{RobertDirectionalHearingInsects2005,
  title = {Directional {{Hearing}} in {{Insects}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Robert, Daniel},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {6--35},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_2},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HQM9352F\\0-387-28863-5_2.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@online{RobustIndoorSpeaker,
  title = {Robust Indoor Speaker Recognition in a Network of Audio and Video Sensors - {{ScienceDirect}}},
  url = {https://www.sciencedirect.com/science/article/pii/S0165168416300603},
  urldate = {2019-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FFXZU2Q8\\S0165168416300603.html}
}

@article{rodenSoundSourceLocalization2015,
  title = {On Sound Source Localization of Speech Signals Using Deep Neural Networks},
  author = {Roden, R and Moritz, N and Gerlach, S and Weinzierl, S and Goetze, S},
  date = {2015},
  pages = {4},
  abstract = {In recent years artificial neural networks are successfully applied especially in the context of automatic speech recognition. As information processing systems, neural networks are trained by, e.g., backpropagation or restricted Boltzmann machines to classify patterns at the input of the system. The current work presents the implementation of a deep neural network (DNN) architecture for acoustic source localization.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\F4FH6A7J\\Roden et al. - 2015 - On sound source localization of speech signals usi.pdf},
  keywords = {VERY IMPORTANT},
  langid = {english}
}

@inproceedings{rodenSoundSourceLocalization2015a,
  title = {On Sound Source Localization of Speech Signals Using Deep Neural Networks},
  booktitle = {Fortschritte Der {{Akustik}}. {{DAGA}} 2015},
  author = {Roden, Reinhild and Moritz, Niko and Gerlach, Stephan and {al}, et},
  date = {2015},
  pages = {1510--1513},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UKV4GPFH\\N-349926.html}
}

@article{royESPRITestimationSignalParameters1989,
  title = {{{ESPRIT}}-Estimation of Signal Parameters via Rotational Invariance Techniques},
  author = {Roy, R. and Kailath, T.},
  date = {1989-07},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  pages = {984--995},
  issn = {0096-3518},
  doi = {10.1109/29.32276},
  abstract = {An approach to the general problem of signal parameter estimation is described. The algorithm differs from its predecessor in that a total least-squares rather than a standard least-squares criterion is used. Although discussed in the context of direction-of-arrival estimation, ESPRIT can be applied to a wide variety of problems including accurate detection and estimation of sinusoids in noise. It exploits an underlying rotational invariance among signal subspaces induced by an array of sensors with a translational invariance structure. The technique, when applicable, manifests significant performance and computational advantages over previous algorithms such as MEM, Capon's MLM, and MUSIC},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\C9HQ2MNP\\Roy and Kailath - 1989 - ESPRIT-estimation of signal parameters via rotatio.pdf;C\:\\Users\\sauli\\Zotero\\storage\\42UN44A4\\32276.html},
  keywords = {array,Computational efficiency,Direction of arrival estimation,ESPRIT,Estimation,filtering and prediction theory,Frequency estimation,least-squares,Maximum likelihood estimation,Multiple signal classification,parameter estimation,rotational invariance techniques,Sensor arrays,Sensors,signal parameters,signal processing,Signal processing algorithms,signal subspaces,Time series analysis,translational invariance structure},
  number = {7}
}

@article{saeidi1InstituteCommunicationAcoustics,
  title = {{{1Institute}} of {{Communication Acoustics}}, {{Ruhr}}-{{Universita}}¨t {{Bochum}} 2 {{Spoken Language Laboratory}}, {{INESC}}-{{ID}}, {{Lisbon}} 3 {{School}} of {{Computing}}, {{University}} of {{Eastern Finland}}},
  author = {Saeidi, Rahim and Mowlaee, Pejman},
  pages = {6},
  abstract = {While much progress has been made in designing robust automatic speech recognition (ASR) systems, the combination of high noise levels and reverberant room acoustics still poses a major challenge even to state-of-the-art systems. The following paper describes how robust automatic speech recognition in such difficult environments can be approached by combining beamforming and missing data techniques.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\P4HXK32C\\Saeidi and Mowlaee - 1Institute of Communication Acoustics, Ruhr-Univer.pdf},
  langid = {english}
}

@inproceedings{sainathConvolutionalLongShortTerm2015,
  title = {Convolutional, {{Long Short}}-{{Term Memory}}, Fully Connected {{Deep Neural Networks}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sainath, T. N. and Vinyals, O. and Senior, A. and Sak, H.},
  date = {2015-04},
  pages = {4580--4584},
  doi = {10.1109/ICASSP.2015.7178838},
  abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6\% relative improvement in WER over an LSTM, the strongest of the three individual models.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QV54M7EZ\\7178838.html},
  keywords = {CNN,Context,convolutional memory,convolutional neural networks,DNN,frequency variations,fully connected deep neural networks,Hidden Markov models,long short-term memory,LSTM,neural nets,Neural networks,Noise measurement,Speech,speech recognition,Training}
}

@inproceedings{SainathConvolutionalLongShortTerm2015,
  title = {Convolutional, {{Long Short}}-{{Term Memory}}, {{Fully Connected Deep Neural Networks}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sainath, T. N. and Vinyals, O. and Senior, A. and Sak, H.},
  date = {2015-04},
  pages = {4580--4584},
  doi = {10.1109/ICASSP.2015.7178838},
  abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6\% relative improvement in WER over an LSTM, the strongest of the three individual models.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3FJPFNYV\\7178838.html},
  keywords = {CNN,Context,convolutional memory,convolutional neural networks,DNN,frequency variations,fully connected deep neural networks,Hidden Markov models,long short-term memory,LSTM,neural nets,Neural networks,Noise measurement,Speech,speech recognition,Training}
}

@inproceedings{sakaviciusEstimationSoundSource2019,
  title = {Estimation of {{Sound Source Direction}} of {{Arrival Map Using Convolutional Neural Network}} and {{Cross}}-{{Correlation}} in {{Frequency Bands}}},
  booktitle = {2019 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  author = {Sakavičius, S. and Serackis, A.},
  date = {2019-04},
  pages = {1--6},
  doi = {10.1109/eStream.2019.8732161},
  abstract = {In this paper, we present an evaluation of the usage of a convolutional neural network (CNN) for the estimation of the sound source direction of arrival (DoA) map. Cross-correlations in different frequency bands, calculated for pairs of microphones were used as input features. We propose a technique for generating data for the CNN training, a means of presenting the direction of arrival information for an arbitrary number of sound sources and a viable CNN architecture. In our proposed approach for sound DoA estimation, there is no need for prior knowledge of the number of the active sound sources nor the properties of their signals. We present the results of the evaluation of the two distinct CNN architectures.},
  eventtitle = {2019 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\R6C57STW\\Sakavičius and Serackis - 2019 - Estimation of Sound Source Direction of Arrival Ma.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NR84HTU9\\8732161.html},
  keywords = {Acoustics,Convolutional Neural networks,Direction of arrival map,Direction-of-arrival estimation,Estimation,Microphone arrays,Sound source localization,Training,Training data}
}

@inproceedings{sakaviciusSingleSoundSource2017,
  title = {Single Sound Source Localization Using Multi-Layer Perceptron},
  booktitle = {2017 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  author = {Sakavičius, S. and Plonis, D. and Serackis, A.},
  date = {2017-04},
  pages = {1--4},
  doi = {10.1109/eStream.2017.7950313},
  abstract = {A localization of single sound source is investigated in this paper. The aim of investigation was to propose a technique for single sound source localization based on sound level differences in microphone array. A rectangular array of four microphones was used for sound capturing from single sound source. Location of the sound source is estimated by sending average amplitude of the signal, received from four microphones to a Multi-Layer Perceptron. Different structures of the MultiLayer Perceptron were tested on simulated sound source locations and on the real measured signals. Estimated relative sound source position prediction error was as low as 4 \% in computer simulation and 1.9 \% in practical experimentation. Investigation results presented in this paper shows, that moderate precision of single sound source localization may be received using a MultiLayer Perceptron with only signal amplitude based features sent to the input of the network.},
  eventtitle = {2017 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\G4BWIVGJ\\Sakavičius et al. - 2017 - Single sound source localization using multi-layer.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NCYQAGWM\\7950313.html},
  keywords = {acoustic radiators,acoustic signal processing,microphone array,microphone arrays,Microphone arrays,multi-layer perceptron,perceptrons,Position measurement,rectangular array,relative sound source position prediction error,Sensor arrays,Silicon,simulated sound source locations,single sound source,single sound source localization,sound capturing,sound level microphones,Training}
}

@inproceedings{salvati_use_2016,
  title = {On the Use of Machine Learning in Microphone Array Beamforming for Far-Field Sound Source Localization},
  booktitle = {2016 {{IEEE}} 26th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Salvati, D. and Drioli, C. and Foresti, G. L.},
  date = {2016-09},
  pages = {1--6},
  doi = {10.1109/MLSP.2016.7738899},
  abstract = {This paper presents a weighted minimum variance distortionless response (WMVDR) algorithm for far-field sound source localization in a noisy environment. The broadband beam-forming is computed in the frequency-domain by calculating the response power on each frequency bin and by fusing the narrowband components. A machine learning method based on a support vector machine (SVM) is used for selecting only the narrowband components that positively contribute to the broadband fusion. We investigate the direction of arrival (DOA) estimation problem using a uniform linear array (ULA). The skewness measure of response power function is used as input feature for the supervised SVM learning. Simulations demonstrate the effectiveness of the WMVDR in an outdoor noisy environment.},
  eventtitle = {2016 {{IEEE}} 26th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B9FN4JHV\\Salvati et al. - 2016 - On the use of machine learning in microphone array.pdf;C\:\\Users\\sauli\\Zotero\\storage\\JKGH7RCH\\7738899.html},
  keywords = {array signal processing,broadband beamforming,Broadband communication,broadband fusion,direction of arrival,direction-of-arrival estimation,DOA estimation,estimation theory,far-field sound source localization,learning (artificial intelligence),machine learning,microphone array,microphone array beamforming,microphone arrays,Narrowband,noisy environment,sensor fusion,Sensors,signal to noise ratio,support vector machine,support vector machines,SVM,Training,ULA,uniform linear array,weighted minimum variance distortionless response,WMVDR}
}

@article{salvatiLowComplexityRobustBeamforming2018,
  title = {A {{Low}}-{{Complexity Robust Beamforming Using Diagonal Unloading}} for {{Acoustic Source Localization}}},
  author = {Salvati, D. and Drioli, C. and Foresti, G. L.},
  date = {2018-03},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {609--622},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2789321},
  abstract = {In acoustic array processing, beamforming is a class of algorithms commonly used to estimate the position of a radiating sound source. This paper presents a diagonal unloading (DU) transformation method for the conventional response power beamforming to achieve robust localization with low computational complexity. The transformation is obtained by subtracting an opportune diagonal matrix from the covariance matrix of the array output vector. Specifically, the DU beamformer aims at subtracting the signal subspace from the noisy signal space. It is, hence, a data-dependent covariance matrix conditioning method. We show how to calculate precisely the unloading parameters, and we present a comparison of the proposed DU beamforming, the robust minimum variance distortionless response (MVDR) filter, and the multiple signal classification (MUSIC) method, in terms of their respective eigenanalyses. Theoretical analysis and experiments conducted on both simulated and real acoustic data demonstrate that the DU beamformer localization performance is comparable to that of robust MVDR and MUSIC. Since its computational cost is equivalent to that of a conventional beamformer, the proposed DU beamformer method can, thus, be very attractive due to its effectiveness and computational efficiency.},
  keywords = {acoustic analysis,acoustic array processing,acoustic arrays,acoustic signal processing,acoustic source localization,Acoustics,array output vector,array signal processing,Array signal processing,broadband robust beamforming,Complexity theory,computational complexity,covariance matrices,Covariance matrices,data-dependent covariance matrix conditioning method,Diagonal unloading beamforming,diagonal unloading transformation method,direction of arrival estimation,DU beamformer localization performance,DU beamformer method,DU beamforming,eigenvalues and eigenfunctions,filtering theory,low-complexity robust beamforming,microphone array,Multiple signal classification,multiple signal classification method,noisy signal space,opportune diagonal matrix,radiating sound source,real acoustic data,response power beamforming,robust localization,robust minimum variance distortionless response filter,Robustness,Sensors,signal classification,signal subspace},
  number = {3}
}

@article{salvatiLowComplexityRobustBeamforming2018a,
  title = {A {{Low}}-{{Complexity Robust Beamforming Using Diagonal Unloading}} for {{Acoustic Source Localization}}},
  author = {Salvati, D. and Drioli, C. and Foresti, G. L.},
  date = {2018-03},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {609--622},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2789321},
  abstract = {In acoustic array processing, beamforming is a class of algorithms commonly used to estimate the position of a radiating sound source. This paper presents a diagonal unloading (DU) transformation method for the conventional response power beamforming to achieve robust localization with low computational complexity. The transformation is obtained by subtracting an opportune diagonal matrix from the covariance matrix of the array output vector. Specifically, the DU beamformer aims at subtracting the signal subspace from the noisy signal space. It is, hence, a data-dependent covariance matrix conditioning method. We show how to calculate precisely the unloading parameters, and we present a comparison of the proposed DU beamforming, the robust minimum variance distortionless response (MVDR) filter, and the multiple signal classification (MUSIC) method, in terms of their respective eigenanalyses. Theoretical analysis and experiments conducted on both simulated and real acoustic data demonstrate that the DU beamformer localization performance is comparable to that of robust MVDR and MUSIC. Since its computational cost is equivalent to that of a conventional beamformer, the proposed DU beamformer method can, thus, be very attractive due to its effectiveness and computational efficiency.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DRXCI5U4\\Salvati et al. - 2018 - A Low-Complexity Robust Beamforming Using Diagonal.pdf;C\:\\Users\\sauli\\Zotero\\storage\\DUT75KPR\\8245852.html},
  keywords = {acoustic analysis,acoustic array processing,acoustic arrays,acoustic signal processing,acoustic source localization,Acoustics,array output vector,array signal processing,Array signal processing,broadband robust beamforming,Complexity theory,computational complexity,covariance matrices,Covariance matrices,data-dependent covariance matrix conditioning method,Diagonal unloading beamforming,diagonal unloading transformation method,direction of arrival estimation,DU beamformer localization performance,DU beamformer method,DU beamforming,eigenvalues and eigenfunctions,filtering theory,low-complexity robust beamforming,microphone array,Multiple signal classification,multiple signal classification method,noisy signal space,opportune diagonal matrix,radiating sound source,real acoustic data,response power beamforming,robust localization,robust minimum variance distortionless response filter,Robustness,Sensors,signal classification,signal subspace},
  number = {3}
}

@article{salvatiLowComplexityRobustBeamforming2018b,
  title = {A {{Low}}-{{Complexity Robust Beamforming Using Diagonal Unloading}} for {{Acoustic Source Localization}}},
  author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
  date = {2018-03},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{609-622\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2017.2789321\}},
  abstract = {In acoustic array processing, beamforming is a class of algorithms commonly used to estimate the position of a radiating sound source. This paper presents a diagonal unloading (DU) transformation method for the conventional response power beamforming to achieve robust localization with low computational complexity. The transformation is obtained by subtracting an opportune diagonal matrix from the covariance matrix of the array output vector. Specifically, the DU beamformer aims at subtracting the signal subspace from the noisy signal space. It is, hence, a data dependent covariance matrix conditioning method. We show how to calculate precisely the unloading parameters, and we present a comparison of the proposed DU beam forming, the robust minimum variance distortionless response (MVDR) filter, and the multiple signal classification ( MUSIC) method, in terms of their respective eigenanalyses. Theoretical analysis and experiments conducted on both simulated and real acoustic data demonstrate that the DU beamformer localization performance is comparable to that of robust MVD Rand MUSIC. Since its computational cost is equivalent to that of a conventional beamformer, the proposed DU beamformer method can, thus, be very attractive due to its effectiveness and computational efficiency.},
  affiliation = {Salvati, D (Reprint Author), Univ Udine, Dept Math Comp Sci & Phys, I-33100 Udine, Italy. Salvati, Daniele; Drioli, Carlo; Foresti, Gian Luca, Univ Udine, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.},
  author-email = {daniele.salvati@uniud.it carlo.drioli@uniud.it gianluca.foresti@uniud.it},
  cited-references = {Aarabi P, 2003, EURASIP J APPL SIG P, V2003, P338, DOI 10.1155/S1110865703212014. Abeida H, 2013, IEEE T SIGNAL PROCES, V61, P933, DOI 10.1109/TSP.2012.2231676. ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. BARTLETT MS, 1948, NATURE, V161, P686. Benesty J, 2000, J ACOUST SOC AM, V107, P384, DOI 10.1121/1.428310. Canclini A, 2015, IEEE-ACM T AUDIO SPE, V23, P1563, DOI 10.1109/TASLP.2015.2439040. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278. CARLSON BD, 1988, IEEE T AERO ELEC SYS, V24, P397, DOI 10.1109/7.7181. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. COX H, 1987, IEEE T ACOUST SPEECH, V35, P1365, DOI 10.1109/TASSP.1987.1165054. DiBiase J. H., 2001, MICROPHONE ARRAYS SI. Dmochowski J. P., 2007, IEEE WORKSH APPL SIG, P18. Dmochowski J, 2008, IEEE T AUDIO SPEECH, V16, P1490, DOI 10.1109/TASL.2008.2005029. Du L, 2010, IEEE T AERO ELEC SYS, V46, P449, DOI 10.1109/TAES.2010.5417174. Gerstoft P, 2012, J ACOUST SOC AM, V132, P2388, DOI 10.1121/1.4746024. Gu YJ, 2012, IEEE T SIGNAL PROCES, V60, P3881, DOI 10.1109/TSP.2012.2194289. Harmanci K, 2000, IEEE T SIGNAL PROCES, V48, P1, DOI 10.1109/78.815474. Huang YT, 2001, IEEE T SPEECH AUDI P, V9, P943, DOI 10.1109/89.966097. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Krim H., 1996, IEEE SIGNAL PROCESS, V13, P1053. Kumar L, 2014, IEEE T SIGNAL PROCES, V62, P4627, DOI 10.1109/TSP.2014.2337271. Lehmann EA, 2008, J ACOUST SOC AM, V124, P269, DOI 10.1121/1.2936367. Li J, 2003, IEEE T SIGNAL PROCES, V51, P1702, DOI 10.1109/TSP.2003.812831. Li J, 2006, ROBUST ADAPTIVE BEAMFORMING, P1. Li XF, 2016, IEEE-ACM T AUDIO SPE, V24, P2171, DOI 10.1109/TASLP.2016.2598319. Liu CF, 2010, SIGNAL PROCESS, V90, P1573, DOI 10.1016/j.sigpro.2009.10.027. Liu GH, 2016, IEEE SENS J, V16, P4673, DOI 10.1109/JSEN.2016.2557488. Liu HQ, 2006, SIGNAL PROCESS, V86, P2820, DOI 10.1016/j.sigpro.2005.11.010. Liu WH, 2008, IEEE T SIGNAL PROCES, V56, P6102, DOI 10.1109/TSP.2008.2005862. Lombard A, 2011, IEEE T AUDIO SPEECH, V19, P1490, DOI 10.1109/TASL.2010.2092765. Mestre X, 2006, IEEE T SIGNAL PROCES, V54, P69, DOI 10.1109/TSP.2005.861052. NG BP, 1994, SIGNAL PROCESS, V40, P319, DOI 10.1016/0165-1684(94)90078-7. Nunes LO, 2014, IEEE T SIGNAL PROCES, V62, P5171, DOI 10.1109/TSP.2014.2336636. Omologo M., 1998, SPOKEN DIALOGUE COMP. Pertila P, 2008, EURASIP J AUDIO SPEE, DOI 10.1155/2008/278185. RAO BD, 1989, IEEE T ACOUST SPEECH, V37, P1939, DOI 10.1109/29.45540. Ribeiro F, 2010, IEEE T AUDIO SPEECH, V18, P1781, DOI 10.1109/TASL.2010.2052250. Rieken DW, 2004, IEEE T SIGNAL PROCES, V52, P2396, DOI 10.1109/TSP.2004.832153. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. Salvati D., 2011, P INT C DIG AUD EFF, P177. Salvati D, 2016, PATTERN RECOGN LETT, V84, P15, DOI 10.1016/j.patrec.2016.07.003. Salvati D, 2016, IEEE SIGNAL PROC LET, V23, P1459, DOI 10.1109/LSP.2016.2601878. Salvati D, 2014, IEEE SIGNAL PROC LET, V21, P581, DOI 10.1109/LSP.2014.2311164. Salvati D, 2014, SCI WORLD J, DOI 10.1155/2014/582397. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. Shin JW, 2014, IEEE T SIGNAL PROCES, V62, P6568, DOI 10.1109/TSP.2014.2367454. SMITH JO, 1987, IEEE T ACOUST SPEECH, V35, P1661, DOI 10.1109/TASSP.1987.1165089. Somasundaram SD, 2012, IEEE T GEOSCI REMOTE, V50, P4642, DOI 10.1109/TGRS.2012.2192500. Stoica P., 2005, SPECTRAL ANAL SIGNAL. Stoica P, 2006, IEEE SIGNAL PROC MAG, V23, P63, DOI 10.1109/SP-M.2006.248717. Sun HH, 2011, INT CONF ACOUST SPEE, P117. Synnevag JF, 2007, IEEE T ULTRASON FERR, V54, P1606, DOI 10.1109/TUFFC.2007.431. Taghizadeh MJ, 2015, IEEE J-STSP, V9, P802, DOI 10.1109/JSTSP.2015.2422677. Traa J, 2016, IEEE-ACM T AUDIO SPE, V24, P493, DOI 10.1109/TASLP.2015.2512499. Tzafri L, 2016, IEEE T WIREL COMMUN, V15, P6449, DOI 10.1109/TWC.2016.2585116. Velasco J, 2012, SENSORS-BASEL, V12, P13781, DOI 10.3390/s121013781. Vorobyov SA, 2013, SIGNAL PROCESS, V93, P3264, DOI 10.1016/j.sigpro.2012.10.021. Wang B, 2013, IEEE SIGNAL PROC LET, V20, P311, DOI 10.1109/LSP.2013.2245503. Ward DB, 2003, IEEE T SPEECH AUDI P, V11, P826, DOI 10.1109/TSA.2003.818112. Xu YG, 2014, SIGNAL PROCESS, V94, P670, DOI 10.1016/j.sigpro.2013.07.013. Yardibi T, 2010, IEEE T AERO ELEC SYS, V46, P425, DOI 10.1109/TAES.2010.5417172. Zhang L, 2011, IEEE T SIGNAL PROCES, V59, P2427, DOI 10.1109/TSP.2011.2109957.},
  da = {2018-10-18},
  doc-delivery-number = {FU0GF},
  funding-acknowledgement = {Regione Friuli Venezia-Giulia under the “Proactive vision for advanced UAV systems for the protection of mobile units, control of territory and environmental prevention (SUPReME)” project [FVG L.R. 20/2015]; italian Ministry of Defence under the “PREscriptive Situational awareness for cooperative auto-organizing aerial sensor NETworks (PRESNET)” project [MoD 1889 12/2016]},
  funding-text = {The work was supported in part by the Regione Friuli Venezia-Giulia under the “Proactive vision for advanced UAV systems for the protection of mobile units, control of territory and environmental prevention (SUPReME)” FVG L.R. 20/2015 project and in part by the italian Ministry of Defence under the “PREscriptive Situational awareness for cooperative auto-organizing aerial sensor NETworks (PRESNET)” MoD 1889 12/2016 project. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Simon Doclo.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Diagonal unloading beamforming,acoustic analysis,acoustic source localization,broadband robust beamforming,direction  of arrival estimation,microphone array)},
  keywords-plus = {SOUND-SOURCE LOCALIZATION; RANGE-DIFFERENCE MEASUREMENTS; MICROPHONE ARRAYS; COVARIANCE-MATRIX; LEAST-SQUARES; IMPULSE RESPONSES; LOADING FACTOR; MUSIC; ALGORITHM; MODEL},
  langid = {english},
  number = {3},
  number-of-cited-references = {63},
  orcid-numbers = {Salvati, Daniele/0000-0002-8042-0333},
  research-areas = {Acoustics; Engineering},
  times-cited = {2},
  type = {Article},
  unique-id = {ISI:000423528700012},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{salvatiSensitivitybasedRegionSelection2018,
  title = {Sensitivity-Based Region Selection in the Steered Response Power Algorithm},
  author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
  date = {2018-12},
  journaltitle = {SIGNAL PROCESSING},
  volume = {153},
  pages = {\{1-10\}},
  publisher = {ELSEVIER SCIENCE BV},
  location = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
  issn = {0165-1684},
  doi = {\{10.1016/j.sigpro.2018.07.002\}},
  abstract = {The steered response power (SRP) algorithm is a well-studied method for acoustic source localization using a microphone array. Recently, different improvements based on the accumulation of all time difference of arrival (TDOA) information have been proposed in order to achieve spatial resolution scalability of the grid search map and reduce the computational cost. However, the TDOA information distribution is not uniform with respect to the search grid, as it depends on the geometry of the array, the sampling frequency, and the spatial resolution. In this paper, we propose a sensitivity-based region selection SRP (R-SRP) algorithm that exploits the nonuniform TDOA information accumulation on the search grid. First, high and low sensitivity regions of the search space are identified using an array sensitivity estimation procedure; then, through the formulation of a peak-to-peak ratio (PPR) measuring the peak energy distribution in the two regions, the source is classified to belong to a high or to a low sensitivity region, and this information is used to design an ad hoc weighting function of the acoustic power map on which the grid search is performed. Simulated and real experiments show that the proposed method improves the localization performance in comparison to the state-of-the-art. (C) 2018 Elsevier B.V. All rights reserved.},
  affiliation = {Salvati, D (Reprint Author), Univ Udine, Dept Math Comp Sci & Phys, Udine, Italy. Salvati, Daniele; Drioli, Carlo; Foresti, Gian Luca, Univ Udine, Dept Math Comp Sci & Phys, Udine, Italy.},
  author-email = {daniele.salvati@uniud.it carlo.drioli@uniud.it gianluca.foresti@uniud.it},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. BARTLETT MS, 1948, NATURE, V161, P686. Benesty J, 2000, J ACOUST SOC AM, V107, P384, DOI 10.1121/1.428310. Bertrand A, 2010, IEEE T SIGNAL PROCES, V58, P5277, DOI 10.1109/TSP.2010.2052612. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. Cobos M., 2017, WIRELESS COMMUNICATI, V2017, P1. Cobos M, 2011, IEEE SIGNAL PROC LET, V18, P71, DOI 10.1109/LSP.2010.2091502. DiBiase J. H., 2001, MICROPHONE ARRAYS SI. Donohue KD, 2007, SIGNAL PROCESS, V87, P1677, DOI 10.1016/j.sigpro.2007.01.013. Griffin A, 2015, SIGNAL PROCESS, V107, P54, DOI 10.1016/j.sigpro.2014.08.013. Harmanci K, 2000, IEEE T SIGNAL PROCES, V48, P1, DOI 10.1109/78.815474. Huang YT, 2001, IEEE T SPEECH AUDI P, V9, P943, DOI 10.1109/89.966097. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Kumar L, 2016, IEEE T SIGNAL PROCES, V64, P3351, DOI 10.1109/TSP.2016.2543201. Kuttruff H., 2009, ROOM ACOUSTICS. Laufer-Goldshtein B, 2016, IEEE-ACM T AUDIO SPE, V24, P1393, DOI 10.1109/TASLP.2016.2555085. Lehmann EA, 2008, J ACOUST SOC AM, V124, P269, DOI 10.1121/1.2936367. Lima Markus V. S., 2015, IEEE Signal Processing Letters, V22, P1098, DOI 10.1109/LSP.2014.2385864. Markovich-Golan S, 2013, IEEE T AUDIO SPEECH, V21, P1513, DOI 10.1109/TASL.2013.2255280. Marti A, 2013, J ACOUST SOC AM, V134, P2627, DOI 10.1121/1.4820885. Nunes L.O., 2012, INT WORKSH AC SIG EN, P1. Nunes LO, 2014, IEEE T SIGNAL PROCES, V62, P5171, DOI 10.1109/TSP.2014.2336636. Omologo M., 1998, SPOKEN DIALOGUE COMP. Pertila P, 2008, EURASIP J AUDIO SPEE, DOI 10.1155/2008/278185. Petrica L, 2016, APPL ACOUST, V113, P162, DOI 10.1016/j.apacoust.2016.06.022. RAO BD, 1989, IEEE T ACOUST SPEECH, V37, P1939, DOI 10.1109/29.45540. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. Salvati Daniele, 2018, IEEE Transactions on Emerging Topics in Computational Intelligence, V2, P103, DOI 10.1109/TETCI.2017.2775237. Salvati D, 2017, J ACOUST SOC AM, V141, P586, DOI 10.1121/1.4974289. Salvati D, 2018, IEEE-ACM T AUDIO SPE, V26, P609, DOI 10.1109/TASLP.2017.2789321. Salvati D, 2016, PATTERN RECOGN LETT, V84, P15, DOI 10.1016/j.patrec.2016.07.003. Salvati D, 2016, IEEE SIGNAL PROC LET, V23, P1459, DOI 10.1109/LSP.2016.2601878. Salvati D, 2014, IEEE SIGNAL PROC LET, V21, P581, DOI 10.1109/LSP.2014.2311164. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. Silverman HF, 2005, IEEE T SPEECH AUDI P, V13, P593, DOI 10.1109/TSA.2005.848875. SMITH JO, 1987, IEEE T ACOUST SPEECH, V35, P1661, DOI 10.1109/TASSP.1987.1165089. Souden M, 2014, IEEE-ACM T AUDIO SPE, V22, P354, DOI 10.1109/TASLP.2013.2292308. Stoica P, 2006, IEEE SIGNAL PROC MAG, V23, P63, DOI 10.1109/SP-M.2006.248717. Taseska M, 2014, IEEE-ACM T AUDIO SPE, V22, P1195, DOI 10.1109/TASLP.2014.2327294. Thiergart O, 2013, IEEE T AUDIO SPEECH, V21, P2583, DOI 10.1109/TASL.2013.2280210. Traa J, 2016, IEEE-ACM T AUDIO SPE, V24, P493, DOI 10.1109/TASLP.2015.2512499. Velasco J, 2016, SIGNAL PROCESS, V119, P209, DOI 10.1016/j.sigpro.2015.08.003. Yook D, 2016, IEEE T CYBERNETICS, V46, P20, DOI 10.1109/TCYB.2015.2391252.},
  da = {2018-10-18},
  doc-delivery-number = {GV3JB},
  eissn = {1879-2677},
  journal-iso = {Signal Process.},
  keywords = {(Acoustic source localization,Geometrically sampled grid),Microphone array,PHAT,Region selection,Sensitivity  map,SRP,SRP-PHAT},
  keywords-plus = {ACOUSTIC SOURCE LOCALIZATION; SOUND SOURCE LOCALIZATION; DISTRIBUTED MICROPHONE ARRAYS; RANGE-DIFFERENCE MEASUREMENTS; IMPULSE RESPONSES; PERFORMANCE; LOCATION; REVERBERANT; MODEL},
  langid = {english},
  number-of-cited-references = {43},
  orcid-numbers = {Salvati, Daniele/0000-0002-8042-0333},
  research-areas = {Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000445989100001},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Engineering, Electrical & Electronic}
}

@article{salvatiSensitivitybasedRegionSelection2018a,
  title = {Sensitivity-Based Region Selection in the Steered Response Power Algorithm},
  author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
  date = {2018-12},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Process.},
  volume = {153},
  pages = {1--10},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2018.07.002},
  abstract = {The steered response power (SRP) algorithm is a well-studied method for acoustic source localization using a microphone array. Recently, different improvements based on the accumulation of all time difference of arrival (TDOA) information have been proposed in order to achieve spatial resolution scalability of the grid search map and reduce the computational cost. However, the TDOA information distribution is not uniform with respect to the search grid, as it depends on the geometry of the array, the sampling frequency, and the spatial resolution. In this paper, we propose a sensitivity-based region selection SRP (R-SRP) algorithm that exploits the nonuniform TDOA information accumulation on the search grid. First, high and low sensitivity regions of the search space are identified using an array sensitivity estimation procedure; then, through the formulation of a peak-to-peak ratio (PPR) measuring the peak energy distribution in the two regions, the source is classified to belong to a high or to a low sensitivity region, and this information is used to design an ad hoc weighting function of the acoustic power map on which the grid search is performed. Simulated and real experiments show that the proposed method improves the localization performance in comparison to the state-of-the-art. (C) 2018 Elsevier B.V. All rights reserved.},
  keywords = {acoustic source localization,Acoustic source localization,distributed   microphone arrays,Geometrically sampled grid,impulse responses,location,Microphone array,model,performance,range-difference measurements,Region selection,reverberant,Sensitivity   map,sound source localization,srp-phat},
  langid = {english}
}

@article{salvatiWeightedMVDRBeamformer2016,
  title = {A Weighted {{MVDR}} Beamformer Based on {{SVM}} Learning for Sound Source Localization},
  author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
  date = {2016-07-01},
  journaltitle = {ResearchGate},
  volume = {84},
  pages = {15--21},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2016.07.003},
  url = {https://www.researchgate.net/publication/305363686_A_weighted_MVDR_beamformer_based_on_SVM_learning_for_sound_source_localization},
  urldate = {2017-01-12},
  abstract = {A weighted MVDR beamformer based on SVM learning for sound source localization on ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FW6HE7F9\\305363686_A_weighted_MVDR_beamformer_based_on_SVM_learning_for_sound_source_localization.html}
}

@article{salvatiWeightedMVDRBeamformer2016a,
  title = {A Weighted {{MVDR}} Beamformer Based on {{SVM}} Learning for Sound Source Localization},
  author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
  date = {2016-12-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {84},
  pages = {15--21},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2016.07.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0167865516301659},
  urldate = {2017-05-22},
  abstract = {A weighted minimum variance distortionless response (WMVDR) algorithm for near-field sound localization in a reverberant environment is presented. The steered response power computation of the WMVDR is based on a machine learning component which improves the incoherent frequency fusion of the narrowband power maps. A support vector machine (SVM) classifier is adopted to select the components of the fusion. The skewness measure of the narrowband power map marginal distribution is showed to be an effective feature for the supervised learning of the power map selection. Experiments with both simulated and real data demonstrate the improvement of the WMVDR beamformer localization accuracy with respect to other state-of-the-art techniques.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QZ878B3M\\Salvati et al. - 2016 - A weighted MVDR beamformer based on SVM learning f.pdf;C\:\\Users\\sauli\\Zotero\\storage\\UB5UNK8D\\S0167865516301659.html},
  keywords = {microphone array,reverberant environment,sound source localization,support vector machine,weighted minimum variance distortionless response}
}

@online{sanderdielemanRecommendingMusicSpotify,
  title = {Recommending Music on {{Spotify}} with Deep Learning},
  author = {Sander Dieleman},
  journaltitle = {Sander Dieleman},
  url = {http://benanne.github.io/2014/08/05/spotify-cnns.html},
  urldate = {2017-02-09},
  abstract = {An overview of what I've been doing as part of my internship at Spotify in NYC this summer: using convolutional neural networks for audio-based music recommendation.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HCCIERFB\\spotify-cnns.html}
}

@article{SanderDielemanRecommendingmusicSpotify,
  title = {Recommending {{Music}} on {{Spotify}} with {{Deep Learning}}},
  author = {Dieleman, Sander},
  journaltitle = {Sander Dieleman},
  url = {http://benanne.github.io/2014/08/05/spotify-cnns.html},
  abstract = {An overview of what I've been doing as part of my internship at Spotify in NYC this summer: using convolutional neural networks for audio-based music recommendation.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9PPKAJ4X\\spotify-cnns.html}
}

@thesis{sarafOutdoorSoundLocalization2018,
  title = {Outdoor Sound Localization Using a Tetrahedral Array},
  author = {Saraf, Ashwin and Démurger, Maxime},
  date = {2018},
  institution = {{Aalborg University}},
  url = {https://projekter.aau.dk/projekter/files/281708108/Master_Thesis_Final.pdf},
  abstract = {The impact of sound on an individual’s health
can be dramatic when exposed to high sound
pressure level (SPL) for an extended period of
time. While total SPL can be monitored using just a single microphone, it is a challenging
problem to find the major sound contributors in
ordinary outdoor environments where multiple
loud sources can be present. A wide range of
solutions already exist for this purpose, which
can compute the sound power map of an environment using microphones arrays. The SRPPHAT algorithm is notable, as it combines the
robustness of beamforming techniques with the
accuracy of time difference of arrival (TDOA)
based methods. However, the method is designed for sources in a reverberant field and has
been optimized for indoor teleconferencing and
sound peak detection. Outdoor source localization is still normally done using traditional
beamforming on a copious number of microphones (sometimes greater than fifty). The purpose of this thesis then is to apply the SRPPHAT for outdoor sound localization using a
minimal number of microphones. For this purpose, a modified SRP-PHAT algorithm is derived and is shown to be more robust than normal SRP-PHAT. The algorithm, called MinSRP-PHAT, is presented and experimentally
evaluated here.},
  langid = {english},
  type = {Master thesis}
}

@article{scardapaneStochasticTrainingNeural2017,
  title = {Stochastic {{Training}} of {{Neural Networks}} via {{Successive Convex Approximations}}},
  author = {Scardapane, Simone and Lorenzo, Paolo Di},
  date = {2017},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  pages = {4947--4956},
  doi = {10.1109/TNNLS.2017.2785361},
  abstract = {This paper proposes a new family of algorithms for training neural networks (NNs). These are based on recent developments in the field of nonconvex optimization, going under the general name of successive convex approximation techniques. The basic idea is to iteratively replace the original (nonconvex, highly dimensional) learning problem with a sequence of (strongly convex) approximations, which are both accurate and simple to optimize. Different from similar ideas (e.g., quasi-Newton algorithms), the approximations can be constructed using only first-order information of the NN function, in a stochastic fashion, while exploiting the overall structure of the learning problem for a faster convergence. We discuss several use cases, based on different choices for the loss function (e.g., squared loss and cross-entropy loss), and for the regularization of the NN’s weights. We experiment on several medium-sized benchmark problems and on a large-scale data set involving simulated physical data. The results show how the algorithm outperforms the state-of-the-art techniques, providing faster convergence to a better minimum. Additionally, we show how the algorithm can be easily parallelized over multiple computational units without hindering its performance. In particular, each computational unit can optimize a tailored surrogate function defined on a randomly assigned subset of the input variables, whose dimension can be selected depending entirely on the available computational power.},
  keywords = {Activation function,Approximation algorithm,Artificial neural network,Benchmark (computing),Computation,Converge,Cross entropy,Extraction,First-order predicate,Iteration,Loss function,Mathematical optimization,Matrix regularization,Neural Network Simulation,Newton,Optimization problem,Parallel computing,PersonNameUse - assigned,Perturbation theory,Quasi-Newton method,Randomness,Subgroup,Test set}
}

@book{scharrer_acoustic_2013,
  title = {Acoustic {{Field Analysis}} in {{Small Microphone Arrays}}},
  author = {Scharrer, Roman},
  date = {2013-08-19},
  publisher = {{Logos Verlag Berlin GmbH}},
  abstract = {In this work, the possibilities of an acoustic field analysis in small microphone arrays are investigated. With the increased use of mobile communication devices, such as smartphones and hearing aids, and the increase in the number of microphones in such devices, multi-channel signal processing has gained popularity. Apart from the definite signal processing, this thesis evaluates what information on the acoustic sound field and environment can be gained from the signal of such small microphone arrays. For this purpose, an innovative sound field classification was developed that determines the energies of the single sound field components. The method is based on spatial coherences of two or more acoustical. The method was successfully verified with a set of simulated and measured input signals. An adaptive automatic sensor mismatch compensation was created, which proved able to fully compensate any slow sensor drift after an initial training. Further, a new method for the blind estimation of the reverberation time based on the dependency of the coherence estimate on the evaluation parameters was proposed. The method determines the reverberation time of a room from the spatial coherence between two or more acoustic sensors.},
  eprint = {lBGDAgAAQBAJ},
  eprinttype = {googlebooks},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UTH9ETII\\4735.html},
  isbn = {978-3-8325-3453-0},
  keywords = {Technology & Engineering / General},
  langid = {english},
  pagetotal = {156}
}

@article{schauPassiveSourceLocalization1987,
  title = {Passive Source Localization Employing Intersecting Spherical Surfaces from Time-of-Arrival Differences},
  author = {Schau, H. and Robinson, A.},
  date = {1987-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {35},
  pages = {1223--1225},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1987.1165266},
  abstract = {Problems associated with the use of intersecting hyperboloids for passive source localization from time-of-arrival difference signals are discussed. A closed-form solution for source location is presented given time-of-arrival difference measurements when the distance from the source to any arbitrary reference is unknown.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QZVCGA89\\Schau and Robinson - 1987 - Passive source localization employing intersecting.pdf;C\:\\Users\\sauli\\Zotero\\storage\\D7CUD6P9\\1165266.html},
  keywords = {Acoustic measurements,Acoustic sensors,array signal processing,Closed-form solution,Equations,Passive radar,Position measurement,Sensor arrays,Surface acoustic waves,Underwater acoustics},
  number = {8}
}

@article{SchauPassivesourcelocalization1987,
  title = {Passive {{Source Localization Employing Intersecting Spherical Surfaces}} from {{Time}}-of-{{Arrival Differences}}},
  author = {Schau, H. and Robinson, A.},
  date = {1987-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {35},
  pages = {1223--1225},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1987.1165266},
  abstract = {Problems associated with the use of intersecting hyperboloids for passive source localization from time-of-arrival difference signals are discussed. A closed-form solution for source location is presented given time-of-arrival difference measurements when the distance from the source to any arbitrary reference is unknown.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HZ57XSYD\\1165266.html},
  keywords = {Acoustic measurements,Acoustic sensors,array signal processing,Closed-form solution,Equations,Passive radar,Position measurement,Sensor arrays,Surface acoustic waves,Underwater acoustics},
  number = {8}
}

@article{schauPassiveSourceLocalization1987a,
  title = {Passive Source Localization Employing Intersecting Spherical Surfaces from Time-of-Arrival Differences},
  author = {Schau, H. and Robinson, A.},
  date = {1987-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {35},
  pages = {1223--1225},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1987.1165266},
  abstract = {Problems associated with the use of intersecting hyperboloids for passive source localization from time-of-arrival difference signals are discussed. A closed-form solution for source location is presented given time-of-arrival difference measurements when the distance from the source to any arbitrary reference is unknown.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q599B2VW\\1165266.html},
  keywords = {Acoustic measurements,Acoustic sensors,array signal processing,Closed-form solution,Equations,Passive radar,Position measurement,Sensor arrays,Surface acoustic waves,Underwater acoustics},
  number = {8}
}

@article{SchauPassivesourcelocalization1987a,
  title = {Passive {{Source Localization Employing Intersecting Spherical Surfaces}} from {{Time}}-of-{{Arrival Differences}}},
  author = {Schau, H. and Robinson, A.},
  date = {1987-08},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {35},
  pages = {1223--1225},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1987.1165266},
  abstract = {Problems associated with the use of intersecting hyperboloids for passive source localization from time-of-arrival difference signals are discussed. A closed-form solution for source location is presented given time-of-arrival difference measurements when the distance from the source to any arbitrary reference is unknown.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DGWGRJ9N\\1165266.html},
  keywords = {Acoustic measurements,Acoustic sensors,array signal processing,Closed-form solution,Equations,Passive radar,Position measurement,Sensor arrays,Surface acoustic waves,Underwater acoustics},
  number = {8}
}

@article{scheiblerr.etal.PyroomacousticsPythonPackage2017,
  title = {Pyroomacoustics: {{A Python}} Package for Audio Room Simulations and Array Processing Algorithms},
  shorttitle = {Pyroomacoustics},
  author = {Scheibler, R. et al.},
  date = {2017-10-11},
  url = {http://arxiv.org/abs/1710.04196},
  urldate = {2019-04-24},
  abstract = {We present pyroomacoustics, a software package aimed at the rapid development and testing of audio array processing algorithms. The content of the package can be divided into three main components: an intuitive Python object-oriented interface to quickly construct different simulation scenarios involving multiple sound sources and microphones in 2D and 3D rooms; a fast C implementation of the image source model for general polyhedral rooms to efficiently generate room impulse responses and simulate the propagation between sources and receivers; and finally, reference implementations of popular algorithms for beamforming, direction finding, and adaptive filtering. Together, they form a package with the potential to speed up the time to market of new algorithms by significantly reducing the implementation overhead in the performance evaluation step.},
  archivePrefix = {arXiv},
  eprint = {1710.04196},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UVASXDZA\\Scheibler et al. - 2017 - Pyroomacoustics A Python package for audio room s.pdf;C\:\\Users\\sauli\\Zotero\\storage\\4TIJGZ9S\\1710.html},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{schmidhuberEvolutionaryComputationReinforcement2000,
  title = {Evolutionary Computation versus Reinforcement Learning},
  booktitle = {2000 26th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}. {{IECON}} 2000. 2000 {{IEEE International Conference}} on {{Industrial Electronics}}, {{Control}} and {{Instrumentation}}. 21st {{Century Technologies}}},
  author = {Schmidhuber, J.},
  date = {2000},
  volume = {4},
  pages = {2992-2997 vol.4},
  doi = {10.1109/IECON.2000.972474},
  abstract = {Many applications of reinforcement learning (RL) and evolutionary computation (EC) are addressing the same problem, namely, to maximize some agent's fitness function in a potentially unknown environment. The most challenging open issues in such applications include partial observability of the agent's environment, hierarchical and other types of abstract credit assignment, and the learning of credit assignment algorithms. I summarize why EC provides a more natural framework for addressing these issues than RL based on value functions and dynamic programming. Then I point out fundamental drawbacks of traditional EC methods in case of stochastic environments, stochastic policies, and unknown temporal delays between actions and observable effects. I discuss a remedy called the success-story algorithm which combines aspects of RL and EC},
  eventtitle = {2000 26th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}. {{IECON}} 2000. 2000 {{IEEE International Conference}} on {{Industrial Electronics}}, {{Control}} and {{Instrumentation}}. 21st {{Century Technologies}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MQF7VMC3\\Schmidhuber - 2000 - Evolutionary computation versus reinforcement lear.pdf;C\:\\Users\\sauli\\Zotero\\storage\\4YVVG9QJ\\972474.html},
  keywords = {abstract credit assignment,agent fitness function,Ambient intelligence,dynamic programming,evolutionary computation,Evolutionary computation,Layout,Learning,learning (artificial intelligence),partial observability,Petroleum,reinforcement learning,stochastic environments,stochastic policies,success-story algorithm,Tiles,unknown environment,unknown temporal delays,value functions}
}

@article{schmidtMultipleEmitterLocation1986,
  title = {Multiple Emitter Location and Signal Parameter Estimation},
  author = {Schmidt, R.},
  date = {1986-03},
  journaltitle = {IEEE Transactions on Antennas and Propagation},
  volume = {34},
  pages = {276--280},
  issn = {0018-926X},
  doi = {10.1109/TAP.1986.1143830},
  abstract = {Processing the signals received on an array of sensors for the location of the emitter is of great enough interest to have been treated under many special case assumptions. The general problem considers sensors with arbitrary locations and arbitrary directional characteristics (gain/phase/polarization) in a noise/interference environment of arbitrary covariance matrix. This report is concerned first with the multiple emitter aspect of this problem and second with the generality of solution. A description is given of the multiple signal classification (MUSIC) algorithm, which provides asymptotically unbiased estimates of 1) number of incident wavefronts present; 2) directions of arrival (DOA) (or emitter locations); 3) strengths and cross correlations among the incident waveforms; 4) noise/interference strength. Examples and comparisons with methods based on maximum likelihood (ML) and maximum entropy (ME), as well as conventional beamforming are included. An example of its use as a multiple frequency estimator operating on time series is included.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\T87BKB35\\Schmidt - 1986 - Multiple emitter location and signal parameter est.pdf;C\:\\Users\\sauli\\Zotero\\storage\\3DIXWB85\\1143830.html},
  keywords = {Adaptive arrays,Direction of arrival estimation,direction-of-arrival estimation,Frequency estimation,Interference,Multiple signal classification,parameter estimation,Polarization,Sensor arrays,Sensor phenomena and characterization,signal processing,Signal processing antennas,Working environment noise},
  number = {3}
}

@article{SchmidtMultipleemitterlocation1986,
  title = {Multiple {{Emitter Location}} and {{Signal Parameter Estimation}}},
  author = {Schmidt, R.},
  date = {1986-03},
  journaltitle = {IEEE Transactions on Antennas and Propagation},
  volume = {34},
  pages = {276--280},
  issn = {0018-926X},
  doi = {10.1109/TAP.1986.1143830},
  abstract = {Processing the signals received on an array of sensors for the location of the emitter is of great enough interest to have been treated under many special case assumptions. The general problem considers sensors with arbitrary locations and arbitrary directional characteristics (gain/phase/polarization) in a noise/interference environment of arbitrary covariance matrix. This report is concerned first with the multiple emitter aspect of this problem and second with the generality of solution. A description is given of the multiple signal classification (MUSIC) algorithm, which provides asymptotically unbiased estimates of 1) number of incident wavefronts present; 2) directions of arrival (DOA) (or emitter locations); 3) strengths and cross correlations among the incident waveforms; 4) noise/interference strength. Examples and comparisons with methods based on maximum likelihood (ML) and maximum entropy (ME), as well as conventional beamforming are included. An example of its use as a multiple frequency estimator operating on time series is included.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZDVNF8KI\\Schmidt - 1986 - Multiple emitter location and signal parameter est.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EBXIX8SW\\1143830.html},
  keywords = {Adaptive arrays,Direction of arrival estimation,direction-of-arrival estimation,Frequency estimation,Interference,Multiple signal classification,parameter estimation,Polarization,Sensor arrays,Sensor phenomena and characterization,signal processing,Signal processing antennas,Working environment noise},
  number = {3}
}

@thesis{schoberSourceLocalizationSmall2017,
  title = {Source {{Localization}} with a {{Small Circular Array}}},
  author = {Schober, Peter},
  date = {2017},
  institution = {{Johannes Kepler University}},
  location = {{Linz}}
}

@online{schoneveldDetectingMusicBPM2016,
  title = {Detecting {{Music BPM}} Using {{Neural Networks}}},
  author = {{schoneveld}, liam},
  date = {2016-08-30T00:00:00+00:00},
  journaltitle = {nlml},
  url = {https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/},
  urldate = {2018-05-07},
  abstract = {I have always wondered whether it would be possible to detect the tempo (or beats per minute, or BPM) of a piece of music using a neural network-based approach. After a small experiment a while back, I decided to make a more serious second attempt. Here’s how it went.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PBHWZEQX\\detecting-bpm-neural-networks.html},
  langid = {english}
}

@article{schroederNewMethodMeasuring1965,
  title = {New {{Method}} of {{Measuring Reverberation Time}}},
  author = {Schroeder, M. R.},
  date = {1965-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {37},
  pages = {409--412},
  issn = {0001-4966},
  doi = {10.1121/1.1909343},
  url = {https://asa.scitation.org/doi/10.1121/1.1909343},
  urldate = {2019-06-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Q4IJN9FF\\1.html},
  number = {3}
}

@inproceedings{sekaninaEvolutionaryCircuitDesign2010,
  title = {Evolutionary Circuit Design: {{Tutorial}}},
  shorttitle = {Evolutionary Circuit Design},
  booktitle = {13th {{IEEE Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} and {{Systems}}},
  author = {Sekanina, L.},
  date = {2010-04},
  pages = {5--5},
  doi = {10.1109/DDECS.2010.5491830},
  abstract = {Summary form only given. Evolutionary algorithms (EAs) are population-based search algorithms that have been successfully applied to solve hard optimization problems in many application domains. Since the early 1990's researchers have begun to apply evolutionary algorithms to synthesize electronic circuits. Nowadays it is evident that the evolutionary design approach can automatically create efficient electronic circuits in many domains. In this tutorial, fundamental concepts of evolutionary design of digital circuits are presented. In particular, the tutorial deals with Cartesian Genetic Programming (CGP)-a method of genetic programming that in many cases outperforms conventional synthesis tools in terms of achievable circuit size reduction. Innovative designs will be presented in domains of small combinational circuits (where the goal is to minimize the number of gates), middle-size circuits (such as image filters intended for FPGAs where the goal is to obtain the quality of filtering of conventional methods for a significantly lower cost on a chip) and large circuits (such as benchmark circuits for comparison of testability analysis methods), covering thus circuit complexity from a few gates to millions of gates. For example, one of evolved image filters is now protected by utility model in the Czech Republic (patent pending). Evolved circuits will be compared with the best-known conventional designs. We will also show how to deal with the so-called scalability problems of evolutionary design which have been identified as the most important problems from the point of view of practical applications. In summary, tutorial participants will become familiar with the state of the art methods in the area of digital circuit evolution. They will learn how to apply CGP, construct the fitness function and run experiments. Lukas Sekanina received all his degrees from Brno University of Technology, Czech Republic (MSc in 1999 and PhD in 2002). He was awarded the Fulbright s- holarship and worked on the evolutionary circuit design with NASA Jet Propulsion Laboratory in Pasadena in 2004. He was a visiting lecturer with Pennsylvania State University and visiting researcher with University of Oslo in 2001. Selected awards: Silver medal (2008), Merit Award (2004) and Honorable Mention (2005) in Human-competitive awards in genetic and evolutionary computation at GECCO, Siemens Award for outstanding PhD thesis in 2003, Siemens Award for outstanding book in 2005. Lukas has served as a program committee member of 10 international conferences and as editorial board member of Int. Journal of Innovative Computing and Applications. He co-authored more than 80 papers mainly on evolvable hardware, with over 400 citations. Currently, he is associate professor with the Faculty of Information Technology, Brno University of Technology. His research interests include evolutionary design and evolvable hardware.},
  eventtitle = {13th {{IEEE Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} and {{Systems}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B9V9CKJ2\\Sekanina - 2010 - Evolutionary circuit design Tutorial.pdf;C\:\\Users\\sauli\\Zotero\\storage\\KZAJVU68\\5491830.html},
  keywords = {cartesian genetic programming,circuit complexity,circuit size reduction,Circuit synthesis,Circuit testing,Combinational circuits,digital circuits,Digital circuits,digital circuits evolutionary design,electronic circuit,Electronic circuits,evolutionary algorithm,evolutionary circuit design,Evolutionary computation,evolutionary design scalability problems,Field programmable gate arrays,Filters,genetic algorithms,genetic programming,Genetic programming,hard optimization problem,image filters,innovative circuit design,network synthesis,optimisation,population based search algorithm,search problems,Tutorial}
}

@online{SelectingAlgorithm,
  title = {Selecting an {{Algorithm}}},
  url = {http://www.tieline.com/manuals/TLR5200D/en/v2_14/selecting_an_algorithm.htm},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3V9TPNV9\\selecting_an_algorithm.html}
}

@article{seltzerMicrophoneArrayProcessing,
  title = {Microphone {{Array Processing}} for {{Robust Speech Recognition}}},
  author = {Seltzer, Michael L},
  pages = {163},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JIWNB5VH\\Seltzer - Microphone Array Processing for Robust Speech Reco.pdf},
  keywords = {*****},
  langid = {english}
}

@inproceedings{sergiiApprobationEvaluationTechniques2018,
  title = {Approbation of {{Evaluation Techniques}} of {{Acoustic Waves Time Delay}} for {{Sound Sources Localization}}},
  booktitle = {2018 {{IEEE}} 38th {{International Conference}} on {{Electronics}} and {{Nanotechnology}} ({{ELNANO}})},
  author = {Sergii, K. and Oleksii, S.},
  date = {2018-04},
  pages = {582--586},
  doi = {10.1109/ELNANO.2018.8477565},
  abstract = {Several approaches to determine the acoustic waves time delay for sound sources localization are considered in this paper. The algorithms of time parameter estimation by means of correlation processing, phase transformation, and pulsed transition functions are presented. A comparison of techniques is performed on an example of processing the audio recordings from small arms.},
  keywords = {acoustic signal processing,acoustic waves,Acoustic waves,acoustic waves time delay,audio recording,cross-correlation function,cross-spectrum,Delay effects,Delays,evaluation techniques,localization,Microphones,Receivers,Shock waves,sound,sound sources localization,time delay,time parameter estimation}
}

@inproceedings{shengEnergyBasedAcoustic2003,
  title = {Energy {{Based Acoustic Source Localization}}},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Information Processing}} in {{Sensor Networks}}},
  author = {Sheng, Xiaohong and Hu, Yu-Hen},
  date = {2003},
  pages = {285--300},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  url = {http://dl.acm.org/citation.cfm?id=1765991.1766011},
  urldate = {2017-05-12},
  abstract = {A novel source localization approach using acoustic energy measurements from the individual sensors in the sensor field is presented. This new approach is based on the acoustic energy decay model that acoustic energy decays inverse of distance square under the conditions that the sound propagates in the free and homogenous space and the targets are pre-detected to be in a certain region of the sensor field. This new approach is power efficient and needs low communication bandwidth and therefore, is suitable for the source localization in the distributed sensor network system. Maximum Likelihood (ML) estimation with Expectation Maximization (EM) solution and projection solution are proposed to solve this energy based source location (EBL) problem. Cramer-Rao Bound (CRB) is derived and used for the sensor deployment analysis. Experiments and simulations are conducted to evaluate ML algorithm with different solutions and to compare it with the Nonlinear Least Square (NLS) algorithm using energy ratio function that we proposed previously. Results show that energy based acoustic source localization algorithms are accurate and robust.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IMW24ISI\\Sheng and Hu - 2003 - Energy Based Acoustic Source Localization.pdf},
  isbn = {978-3-540-02111-7},
  series = {{{IPSN}}'03}
}

@inproceedings{ShengEnergyBasedAcoustic2003,
  title = {Energy {{Based Acoustic Source Localization}}},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Information Processing}} in {{Sensor Networks}}},
  author = {Sheng, Xiaohong and Hu, Yu-Hen},
  date = {2003},
  pages = {285--300},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, Heidelberg}},
  abstract = {A novel source localization approach using acoustic energy measurements from the individual sensors in the sensor field is presented. This new approach is based on the acoustic energy decay model that acoustic energy decays inverse of distance square under the conditions that the sound propagates in the free and homogenous space and the targets are pre-detected to be in a certain region of the sensor field. This new approach is power efficient and needs low communication bandwidth and therefore, is suitable for the source localization in the distributed sensor network system. Maximum Likelihood (ML) estimation with Expectation Maximization (EM) solution and projection solution are proposed to solve this energy based source location (EBL) problem. Cramer-Rao Bound (CRB) is derived and used for the sensor deployment analysis. Experiments and simulations are conducted to evaluate ML algorithm with different solutions and to compare it with the Nonlinear Least Square (NLS) algorithm using energy ratio function that we proposed previously. Results show that energy based acoustic source localization algorithms are accurate and robust.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SJ8MIUVY\\Sheng and Hu - 2003 - Energy Based Acoustic Source Localization.pdf},
  isbn = {978-3-540-02111-7},
  series = {{{IPSN}}'03}
}

@article{shenPeriodicBoundaryBased2018,
  title = {Periodic Boundary Based {{FFT}}-{{FISTA}} for Sound Source Identification},
  author = {Shen, Linbang and Chu, Zhigang and Yang, Yang and Wang, Guangjian},
  date = {2018-01-15},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {130},
  pages = {87--91},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2017.09.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0003682X17307132},
  urldate = {2018-10-19},
  abstract = {Compared with the conventional beamforming, the Fourier-based fast iterative shrinkage thresholding algorithm (FFT-FISTA) can effectively improve the spatial resolution and suppress the sidelobe. To furtherly achieve higher computational efficiency and better sound source identification performance, an alternative periodic boundary is utilized to replace the zero boundary of Fourier transform, a periodic boundary based FFT-FISTA is proposed in this paper. And its superiority is demonstrated by the simulation and validation experiment of equal and unequal intensity sources.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QWKTMY3A\\Shen et al. - 2018 - Periodic boundary based FFT-FISTA for sound source.pdf;C\:\\Users\\sauli\\Zotero\\storage\\VCHW3N5B\\S0003682X17307132.html},
  keywords = {Acoustic source identification,Beamforming,Deconvolution,FFT-FISTA,Periodic boundary,Zero boundary}
}

@article{shenPeriodicBoundaryBased2018a,
  title = {Periodic Boundary Based {{FFT}}-{{FISTA}} for Sound Source Identification},
  author = {Shen, Linbang and Chu, Zhigang and Yang, Yang and Wang, Guangjian},
  date = {2018-01-15},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {130},
  pages = {\{87-91\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2017.09.009\}},
  abstract = {Compared with the conventional beamforming, the Fourier-based fast iterative shrinkage thresholding algorithm (FFT-FISTA) can effectively improve the spatial resolution and suppress the sidelobe. To furtherly achieve higher computational efficiency and better sound source identification performance, an alternative periodic boundary is utilized to replace the zero boundary of Fourier transform, a periodic boundary based FFT-FISTA is proposed in this paper. And its superiority is demonstrated by the simulation and validation experiment of equal and unequal intensity sources. (C) 2017 Elsevier Ltd. All rights reserved.},
  affiliation = {Chu, ZG (Reprint Author), Chongqing Univ, State Key Lab Mech Transmiss, Chongqing 400044, Peoples R China. Shen, Linbang; Chu, Zhigang; Wang, Guangjian, Chongqing Univ, State Key Lab Mech Transmiss, Chongqing 400044, Peoples R China. Shen, Linbang; Chu, Zhigang; Yang, Yang; Wang, Guangjian, Chongqing Univ, Coll Automot Engn, Chongqing 400044, Peoples R China. Shen, Linbang, Chongqing Univ, Coll Mech Engn, Chongqing 400030, Peoples R China. Yang, Yang, Chongqing Ind Polytech Coll, Fac Vehicle Engn, Chongqing 401120, Peoples R China.},
  author-email = {zgchu@cqu.edu.cn},
  cited-references = {Castellini P, 2013, APPL ACOUST, V74, P198, DOI 10.1016/j.apacoust.2012.07.010. Christensen JJ, 2003, P INT, P2652. Chu N, 2014, APPL ACOUST, V76, P197, DOI 10.1016/j.apacoust.2013.08.007. Chu ZG, 2014, MECH SYST SIGNAL PR, V48, P404, DOI 10.1016/j.ymssp.2014.03.012. Faure S, 2015, J SOUND VIB, V346, P165, DOI 10.1016/j.jsv.2015.02.022. Fleury V, 2016, J SOUND VIB, V364, P44, DOI 10.1016/j.jsv.2015.11.027. Fleury V, 2011, J ACOUST SOC AM, V129, P1417, DOI 10.1121/1.3531939. Huang X, 2012, J ACOUST SOC AM, V131, P2152, DOI 10.1121/1.3682041. Lee GS, 2012, APPL ACOUST, V73, P817, DOI 10.1016/j.apacoust.2012.03.001. Lylloff O, 2015, J ACOUST SOC AM, V138, P172, DOI 10.1121/1.4922516. Porteous R, 2015, J SOUND VIB, V355, P117, DOI [10.1016/j.jsv.2015.06.030, 10.1016/j.jsv.2015.00.030]. Sijtsma P, 2010, INT J AEROACOUST, V9, P357, DOI 10.1260/1475-472X.9.3.357. Wang X, 2016, J SOUND VIB, V365, P260, DOI 10.1016/j.jsv.2015.11.036. Yang Y, 2016, J SOUND VIB, V373, P340, DOI 10.1016/j.jsv.2016.03.024. Yardibi T, 2010, J SOUND VIB, V329, P2654, DOI 10.1016/j.jsv.2010.01.014.},
  da = {2018-10-18},
  doc-delivery-number = {FN7HP},
  eissn = {1872-910X},
  funding-acknowledgement = {National Natural Science Foundation of China [11774040, 11404368]; Fundamental Research Funds for the Central Universities [106112017CDJQJ338810]; graduate scientific research and innovation foundation of Chongqing, China [CYB17034]},
  funding-text = {National Natural Science Foundation of China (Grant Nos.: 11774040 and 11404368), Project supported by the Fundamental Research Funds for the Central Universities (Grant No.: 106112017CDJQJ338810), and graduate scientific research and innovation foundation of Chongqing, China (Grant No.: CYB17034).},
  journal-iso = {Appl. Acoust.},
  keywords = {(Beamforming,Acoustic source identification,Deconvolution,FFT-FISTA,Periodic boundary),Zero boundary},
  keywords-plus = {DECONVOLUTION ALGORITHMS; ACOUSTIC SOURCES; NOISE SOURCES; ARRAY; LOCALIZATION; DELAY},
  langid = {english},
  number-of-cited-references = {15},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000416189600008},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Acoustics}
}

@inproceedings{siltanenRaysWavesUnderstanding2010,
  title = {Rays or {{Waves}}? {{Understanding}} the {{Strengths}} and {{Weaknesses}} of {{Computational Room Acoustics Modeling Techniques}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Room Acoustics}}, {{ISRA}} 2010},
  author = {Siltanen, Samuel and Lokki, Tapio and Savioja, Lauri},
  date = {2010},
  pages = {6},
  abstract = {The pros and cons of wave-based and ray-based room acoustics modeling methods are overviewed. Links between imagesource, boundary element and radiance transfer methods are presented. The emphasis is on the main bottlenecks of each method. Accuracy, computational performance and applicability of the output of each method determine for modeling which part of the response they can be used. It is proposed that wave-based methods are used for low frequencies, image source methods for the early part of the room response for mid- and high frequencies, and radiance transfer methods for the rest of the response.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CEXCNDCJ\\Siltanen et al. - 2010 - Rays or Waves Understanding the Strengths and Wea.pdf},
  langid = {english}
}

@article{siltanenRaysWavesUnderstanding2010a,
  title = {Rays or {{Waves}}? {{Understanding}} the {{Strengths}} and {{Weaknesses}} of {{Computational Room Acoustics Modeling Techniques}}},
  author = {Siltanen, Samuel and Lokki, Tapio and Savioja, Lauri},
  date = {2010},
  pages = {6},
  abstract = {The pros and cons of wave-based and ray-based room acoustics modeling methods are overviewed. Links between imagesource, boundary element and radiance transfer methods are presented. The emphasis is on the main bottlenecks of each method. Accuracy, computational performance and applicability of the output of each method determine for modeling which part of the response they can be used. It is proposed that wave-based methods are used for low frequencies, image source methods for the early part of the room response for mid- and high frequencies, and radiance transfer methods for the rest of the response.},
  file = {C\:\\Users\\Saulius\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\L9CT3M4O\\O5a[1].pdf},
  langid = {english}
}

@article{silverman_performance_2005-2,
  title = {Performance of Real-Time Source-Location Estimators for a Large-Aperture Microphone Array},
  author = {Silverman, H. F. and Yu, Ying and Sachar, J. M. and Patterson, W. R.},
  date = {2005-07},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {13},
  pages = {593--606},
  issn = {1063-6676},
  doi = {10.1109/TSA.2005.848875},
  abstract = {A large array of microphones is being studied as a possible means of acquiring data in offices, conference rooms, and auditoria without requiring close-talking microphones. An array that surrounds all possible sources has a large aperture and such arrays have attractive properties for accurate spatial resolution and significant signal-to-noise enhancement. For the first time, this paper presents all the details of a real-time, source-location algorithm (LEMSalg) based on time-of-arrival delays derived from a phase transform applied to the generalized cross-power spectrum. It is being used successfully in a representative environment where microphone SNRs are below 0 dB. We have found that many small features are required to make a useful location estimating algorithm work and work well in real-time. We present an experimental evaluation of the current algorithm's performance using data taken with the Huge Microphone Array (HMA) system, which has 448 microphones in a noisy, reverberant environment. Using off-line computation, we also compared the LEMSalg to two alternative methods. The first of these adds local beamforming to the preprocessing of the base algorithm, increasing performance significantly at modest additional computational cost. The second algorithm maximizes the total steered-response power in the same phase transform. While able to derive good position estimates from shorter data runs, this method is two orders of magnitude more computationally expensive and is not yet suitable for real-time use.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5UIPG66B\\Silverman et al. - 2005 - Performance of real-time source-location estimator.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IUVRVDZE\\1453602.html},
  keywords = {; acoustic location,Acoustic noise,Apertures,Array processing,array signal processing,audio signal processing,Computational efficiency,data acquisition,Delay effects,generalized cross correlation,large-aperture microphone array,microphone arrays,phase transform,Phased arrays,Real time systems,real-time source-location estimators,real-time systems,signal-to-noise enhancement,spatial resolution,steered-response power,time-of-arrival delays,Working environment noise},
  number = {4}
}

@article{silvermanPerformanceRealtimeSourcelocation2005,
  title = {Performance of Real-Time Source-Location Estimators for a Large-Aperture Microphone Array},
  author = {Silverman, H. F. and Yu, Ying and Sachar, J. M. and Patterson, W. R.},
  date = {2005-07},
  volume = {13},
  pages = {593--606},
  issn = {1063-6676},
  doi = {10.1109/TSA.2005.848875},
  abstract = {A large array of microphones is being studied as a possible means of acquiring data in offices, conference rooms, and auditoria without requiring close-talking microphones. An array that surrounds all possible sources has a large aperture and such arrays have attractive properties for accurate spatial resolution and significant signal-to-noise enhancement. For the first time, this paper presents all the details of a real-time, source-location algorithm (LEMSalg) based on time-of-arrival delays derived from a phase transform applied to the generalized cross-power spectrum. It is being used successfully in a representative environment where microphone SNRs are below 0 dB. We have found that many small features are required to make a useful location estimating algorithm work and work well in real-time. We present an experimental evaluation of the current algorithm's performance using data taken with the Huge Microphone Array (HMA) system, which has 448 microphones in a noisy, reverberant environment. Using off-line computation, we also compared the LEMSalg to two alternative methods. The first of these adds local beamforming to the preprocessing of the base algorithm, increasing performance significantly at modest additional computational cost. The second algorithm maximizes the total steered-response power in the same phase transform. While able to derive good position estimates from shorter data runs, this method is two orders of magnitude more computationally expensive and is not yet suitable for real-time use.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9DGU9T67\\1453602.html},
  keywords = {acoustic location,Acoustic noise,Apertures,Array processing,array signal processing,audio signal processing,Computational efficiency,data acquisition,Delay effects,generalized cross correlation,large-aperture microphone array,microphone arrays,phase transform,Phased arrays,Real time systems,real-time source-location estimators,real-time systems,signal-to-noise enhancement,spatial resolution,steered-response power,time-of-arrival delays,Working environment noise},
  number = {4}
}

@article{silvermanPerformanceRealtimeSourcelocation2005a,
  title = {Performance of Real-Time Source-Location Estimators for a Large-Aperture Microphone Array},
  author = {Silverman, H. F. and Yu, Ying and Sachar, J. M. and Patterson, W. R.},
  date = {2005-07},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {13},
  pages = {593--606},
  issn = {1063-6676},
  doi = {10.1109/TSA.2005.848875},
  abstract = {A large array of microphones is being studied as a possible means of acquiring data in offices, conference rooms, and auditoria without requiring close-talking microphones. An array that surrounds all possible sources has a large aperture and such arrays have attractive properties for accurate spatial resolution and significant signal-to-noise enhancement. For the first time, this paper presents all the details of a real-time, source-location algorithm (LEMSalg) based on time-of-arrival delays derived from a phase transform applied to the generalized cross-power spectrum. It is being used successfully in a representative environment where microphone SNRs are below 0 dB. We have found that many small features are required to make a useful location estimating algorithm work and work well in real-time. We present an experimental evaluation of the current algorithm's performance using data taken with the Huge Microphone Array (HMA) system, which has 448 microphones in a noisy, reverberant environment. Using off-line computation, we also compared the LEMSalg to two alternative methods. The first of these adds local beamforming to the preprocessing of the base algorithm, increasing performance significantly at modest additional computational cost. The second algorithm maximizes the total steered-response power in the same phase transform. While able to derive good position estimates from shorter data runs, this method is two orders of magnitude more computationally expensive and is not yet suitable for real-time use.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JWQJBCSA\\1453602.html},
  keywords = {; acoustic location,Acoustic noise,Apertures,Array processing,array signal processing,audio signal processing,Computational efficiency,data acquisition,Delay effects,generalized cross correlation,large-aperture microphone array,microphone arrays,phase transform,Phased arrays,Real time systems,real-time source-location estimators,real-time systems,signal-to-noise enhancement,spatial resolution,steered-response power,time-of-arrival delays,Working environment noise},
  number = {4}
}

@article{silvermanPerformanceRealtimeSourcelocation2005b,
  title = {Performance of Real-Time Source-Location Estimators for a Large-Aperture Microphone Array},
  author = {Silverman, H. F. and {Ying Yu} and Sachar, J. M. and Patterson, W. R.},
  date = {2005-07},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {13},
  pages = {593--606},
  doi = {10.1109/TSA.2005.848875},
  abstract = {A large array of microphones is being studied as a possible means of acquiring data in offices, conference rooms, and auditoria without requiring close-talking microphones. An array that surrounds all possible sources has a large aperture and such arrays have attractive properties for accurate spatial resolution and significant signal-to-noise enhancement. For the first time, this paper presents all the details of a real-time, source-location algorithm (LEMSalg) based on time-of-arrival delays derived from a phase transform applied to the generalized cross-power spectrum. It is being used successfully in a representative environment where microphone SNRs are below 0 dB. We have found that many small features are required to make a useful location estimating algorithm work and work well in real-time. We present an experimental evaluation of the current algorithm's performance using data taken with the Huge Microphone Array (HMA) system, which has 448 microphones in a noisy, reverberant environment. Using off-line computation, we also compared the LEMSalg to two alternative methods. The first of these adds local beamforming to the preprocessing of the base algorithm, increasing performance significantly at modest additional computational cost. The second algorithm maximizes the total steered-response power in the same phase transform. While able to derive good position estimates from shorter data runs, this method is two orders of magnitude more computationally expensive and is not yet suitable for real-time use.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B2T7KJ2Q\\Silverman et al. - 2005 - Performance of real-time source-location estimator.pdf;C\:\\Users\\sauli\\Zotero\\storage\\H54DUSZ9\\1453602.html},
  keywords = {; acoustic location,Acoustic noise,Apertures,array processing,array signal processing,Array signal processing,audio signal processing,Computational efficiency,data acquisition,Delay effects,generalized cross correlation,large-aperture microphone array,microphone arrays,Microphone arrays,phase transform,Phased arrays,Real time systems,real-time source-location estimators,real-time systems,signal-to-noise enhancement,spatial resolution,Spatial resolution,steered-response power,time-of-arrival delays,Working environment noise},
  number = {4}
}

@inproceedings{singhTDOABasedNode2013,
  title = {{{TDOA Based Node Localization}} in {{WSN Using Neural Networks}}},
  booktitle = {2013 {{International Conference}} on {{Communication Systems}} and {{Network Technologies}}},
  author = {Singh, P. and Agrawal, S.},
  date = {2013-04},
  pages = {400--404},
  doi = {10.1109/CSNT.2013.90},
  abstract = {In wireless sensor network, the exact positions of the sensor nodes is necessary for location-aware services. Traditional approaches are not producing satisfactory results. In this paper we propose the use of Time Difference of Arrival (TDOA) information with Neural network for accurate node localization. We use two artificial neural network models-Back Propagation Network (BPN) and Radial Basis Function (RBF) Network model for Wireless Sensor Network's node localization problem. Time Difference of Arrival (TDOA) data is used to calculate the distance information from anchor nodes to sensor nodes. This distance information was used to train the neural networks' models. Simulation results show the superiority of Radial Basis Function Network over Back Propagation Network in terms of root mean square error when training data density is high.},
  eventtitle = {2013 {{International Conference}} on {{Communication Systems}} and {{Network Technologies}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X4T79QDQ\\6524427.html},
  keywords = {Artificial Neural Network,artificial neural network model,back propagation network,backpropagation,BPN,Data models,Localization,location-aware service,Mathematical model,mean square error methods,mobility management (mobile radio),radial basis function networks,radial basis functionnetwork model,RBF,root mean square error,Simulation,TDOA,TDOA based node localization,telecommunication computing,Time difference of arrival,time-of-arrival estimation,Training,training data density,wireless sensor network,wireless sensor networks,WSN}
}

@online{SingleSoundSource,
  title = {Single Sound Source Localization Using Multi-Layer Perceptron - {{IEEE Conference Publication}}},
  url = {https://ieeexplore.ieee.org/document/7950313},
  urldate = {2018-11-14},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RWH37CBE\\7950313.html}
}

@inproceedings{skalevikSchroederFrequencyRevisited2011,
  title = {Schroeder {{Frequency}} Revisited},
  author = {Skålevik, Magne},
  date = {2011},
  pages = {6},
  abstract = {In the middle of the 20th century, Manfred Schroeder explored the transition region of the room acoustical frequency response, namely the cross-over between the low frequency region dominated by separate modes and the high frequency region dominated by dense modal overlap with statistical (Gaussian) properties. The cross-over region is not an abrupt one, and the definition of a limiting frequency is not obvious. Indeed, Schroeder first suggested a theoretical average ten-fold modal overlap as a criterion for the high frequency region, but after years of experience with measurement results, he found it more proper to require only 3-fold modal overlap (Schroeder 1962). This led to the famous Schroeder Frequency, Fs=2000∙(T/V)0.5 However, it can be deduced from Schroder's theory that from measurements one cannot with certainty detect a lower limit for the high frequency region that is higher than approximately 0.5∙Fs. Besides, one can in many small room measurements even below 0.5∙Fs find statistical properties of the frequency response which is indistinguishable from the high frequency region. Since the cross-over frequency has so many important implications in room acoustics, it is worth while having a closer look at the cross-over region itself. This is indeed the objective of this paper. A new limiting frequency will be suggested.},
  eventtitle = {Forum {{Acusticum}} 2011},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2G6YADM4\\Skålevik - 2011 - Schroeder Frequency revisited.pdf},
  langid = {english}
}

@inproceedings{sledevicEvaluationHardwaresoftwareDesign2017,
  title = {An Evaluation of Hardware-Software Design for Sound Source Localization Based on {{SoC}}},
  booktitle = {2017 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  author = {Sledevič, T. and Laptik, R.},
  date = {2017-04},
  pages = {1--4},
  doi = {10.1109/eStream.2017.7950312},
  abstract = {The article presents an evaluation of sound source localization algorithm based on field-programmable gate array (FPGA) and ARM processor. The proposed architecture performs the computation of azimuth and elevation angles of the speaker location in a real-time. Time-difference of arrival (TDOA) features are computed for a grid with four microphones. The speaker position is estimated by matching the features with the acoustic maps. The influence of line and square placement of the microphones is evaluated. The average azimuth errors for square grids with aperture 0.4 m, 0.6 m, 0.8 m are up to ±7°, ±5° and ±4° respectively. Line grid with 0.4 m step and square grids with 0.6 m and 0.8 m aperture give average 45° = ±4° in a range 45° {$<$}; αreel{$<$}; 90° at 2 m distance from grid to speaker.},
  eventtitle = {2017 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QQZVLLPZ\\Sledevič and Laptik - 2017 - An evaluation of hardware-software design for soun.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IE8Q7TU3\\7950312.html},
  keywords = {acoustic map,acoustic maps,Acoustics,Apertures,ARM processor,average azimuth errors,Azimuth,azimuth angle computation,Digital signal processing,elevation angle computation,feature matching,field programmable gate arrays,Field programmable gate arrays,field-programmable gate array,FPGA,FPGA implementation,hardware-software codesign,hardware-software design,line grid,line microphone placement,microphone arrays,Microphones,microprocessor chips,Real-time systems,SoC,sound source localization,sound source localization algorithm,speaker location,speaker position estimation,speaker recognition,square grids,square microphone placement,system-on-chip,TDOA features,time-difference of arrival,time-difference-of-arrival features,time-of-arrival estimation}
}

@incollection{smolaKernelsRegularizationGraphs2003,
  title = {Kernels and {{Regularization}} on {{Graphs}}},
  booktitle = {Learning {{Theory}} and {{Kernel Machines}}},
  author = {Smola, Alexander J. and Kondor, Risi},
  editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
  date = {2003},
  volume = {2777},
  pages = {144--158},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45167-9_12},
  url = {http://link.springer.com/10.1007/978-3-540-45167-9_12},
  urldate = {2019-11-25},
  abstract = {We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\H37JLB28\\Smola and Kondor - 2003 - Kernels and Regularization on Graphs.pdf},
  isbn = {978-3-540-40720-1 978-3-540-45167-9},
  langid = {english},
  options = {useprefix=true}
}

@misc{SoundFieldAnalysis,
  title = {Sound {{Field Analysis}} in {{Room Acoustics}}.Pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CERXVQW8\\Sound Field Analysis in Room Acoustics.pdf}
}

@online{SoundFieldModelling,
  title = {Sound Field Modelling Using {{SimulUS}} ({{May}} 2008)},
  url = {http://www.twi-global.com/technical-knowledge/published-papers/sound-field-modelling-using-simulus-may-2008/},
  urldate = {2017-12-08},
  abstract = {TWI validation evidence of inspection scenarios using SimulUS is discussed},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6TEFRXGN\\sound-field-modelling-using-simulus-may-2008.html}
}

@online{SoundFieldsDefinitions,
  title = {Sound {{Fields}} : {{Definitions}}, {{Terms}}, {{Units}}, {{Measurements}} : {{Acoustic Glossary}}},
  url = {http://www.acoustic-glossary.co.uk/sound-fields.htm},
  urldate = {2017-12-09}
}

@online{SoundFieldsFree2017,
  title = {Sound {{Fields}}: {{Free}} versus {{Diffuse Field}}, {{Near}} versus {{Far Field}}},
  shorttitle = {Sound {{Fields}}},
  date = {2017-01-25T16:14:30.466Z},
  journaltitle = {Siemens PLM Community},
  url = {https://community.plm.automation.siemens.com/t5/Testing-Knowledge-Base/Sound-Fields-Free-versus-Diffuse-Field-Near-versus-Far-Field/ta-p/387463},
  urldate = {2017-12-09},
  abstract = {In the world of acoustics, there are many terms that are used to describe the acoustic field around a sound emitting object.~ Four of the most important are listed below:  Near Field Far Field Free Field Diffuse Field This article explains the differences and usage of these acoustic sound field term...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X45TWS2T\\387463.html}
}

@online{SoundLocalizationCoolarduino2013,
  title = {Sound {{Localization}}. | Coolarduino},
  date = {2013-12-29},
  url = {http://web.archive.org/web/20131229193236/http://coolarduino.wordpress.com/2012/10/01/sound-localization/},
  urldate = {2017-02-18},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CPBQS89Z\\sound-localization.html}
}

@article{SoundLocalizationcoolarduino2013,
  title = {Sound {{Localization}}. | {{Coolarduino}}},
  date = {2013-12},
  url = {http://web.archive.org/web/20131229193236/http://coolarduino.wordpress.com/2012/10/01/sound-localization/},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PI46CIZJ\\sound-localization.html}
}

@online{SoundLocalizationUsing2013,
  title = {Sound {{Localization}} Using {{Arduino}} -},
  date = {2013-05-09T10:29:56+00:00},
  journaltitle = {Use Arduino for Projects},
  url = {http://duino4projects.com/sound-localization-using-arduino/},
  urldate = {2017-02-18},
  abstract = {Well, it’s elementary simple in theory, how to do sound localization based on~phase difference of signals, that received by two spatially distant microphones. The Devil, as always, in details. I’ve not seen any such project created for arduino, and get curious if it’s possible at all. Long story short, here I’d like to present my […]},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GBS6MV2K\\sound-localization-using-arduino.html}
}

@article{SoundLocalizationusing2013,
  title = {Sound {{Localization Using Arduino}} -},
  date = {2013-05},
  journaltitle = {Use Arduino for Projects},
  url = {http://duino4projects.com/sound-localization-using-arduino/},
  abstract = {Well, it's elementary simple in theory, how to do sound localization based on~phase difference of signals, that received by two spatially distant microphones. The Devil, as always, in details. I've not seen any such project created for arduino, and get curious if it's possible at all. Long story short, here I'd like to present my […]},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ESZLYMH6\\sound-localization-using-arduino.html}
}

@online{SoundSourceAnalysis,
  title = {Sound Source Analysis of Moving Rail Vehicles with a Microphone Array},
  journaltitle = {TU Dresden},
  url = {https://tu-dresden.de/ing/maschinenwesen/ifkm/dmt/forschung/projekte/abgeschlossen/schallquellenanalyse?set_language=en},
  urldate = {2018-06-12},
  abstract = {Projektname: 
			Analyzing the sound sources of passing-by rail vehicles with a microphone array 
		 
		 
			Contact: 
			Dipl.-Ing. Johannes Stier 
		 
		 
			Objective target: …},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DWTL5BJY\\schallquellenanalyse.html},
  langid = {english},
  type = {Document}
}

@online{SoundSourceLocalization,
  title = {Sound Source Localization with Data and Model Uncertainties Using the {{EM}} and {{Evidential EM}} Algorithms - Document},
  url = {https://tel.archives-ouvertes.fr/tel-01208211/document},
  urldate = {2017-05-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AUGKXGQJ\\document.html}
}

@article{Soundsourcelocalization,
  title = {Sound {{Source Localization}} with {{Data}} and {{Model Uncertainties Using}} the {{EM}} and {{Evidential EM Algorithms}} - {{Document}}},
  url = {https://tel.archives-ouvertes.fr/tel-01208211/document},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9VN23PMS\\document.html}
}

@online{SoundVR,
  title = {Sound in {{VR}}},
  url = {http://www.ism.univmed.fr/mestre/CDRV-C/Documents/Sites_Enregistres/www.cybertherapy.info/pages/sound.htm},
  urldate = {2017-12-09}
}

@article{spanoudakisEngineeringAmbientIntelligence2015,
  title = {Engineering Ambient Intelligence Systems Using Agent Technology},
  author = {Spanoudakis, Nikolaos and Moraitis, Pavlos},
  date = {2015-05},
  journaltitle = {IEEE Intelligent Systems},
  shortjournal = {IEEE Intell. Syst.},
  volume = {30},
  pages = {60--67},
  issn = {1541-1672},
  doi = {10.1109/MIS.2015.3},
  url = {http://ieeexplore.ieee.org/document/7006373/},
  urldate = {2019-07-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9F5IBW9H\\Spanoudakis and Moraitis - 2015 - Engineering ambient intelligence systems using age.pdf},
  langid = {english},
  number = {3}
}

@article{steckerEffectTemporalAsymmetry2000,
  title = {An Effect of Temporal Asymmetry on Loudness},
  author = {Stecker, G. Christopher and Hafter, Ervin R.},
  date = {2000-05-26},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {107},
  pages = {3358--3368},
  issn = {0001-4966},
  doi = {10.1121/1.429407},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.429407},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3ANHQAP9\\1.html},
  number = {6}
}

@article{stoneComparisonDifferentForms1999,
  title = {Comparison of Different Forms of Compression Using Wearable Digital Hearing Aids},
  author = {Stone, Michael A. and Moore, Brian C. J. and Alcántara, José I. and Glasberg, Brian R.},
  date = {1999-11-23},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {106},
  pages = {3603--3619},
  issn = {0001-4966},
  doi = {10.1121/1.428213},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.428213},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DW3XD7TX\\1.html},
  number = {6}
}

@article{stoneComparisonDifferentForms1999a,
  title = {Comparison of Different Forms of Compression Using Wearable Digital Hearing Aids},
  author = {Stone, Michael A. and Moore, Brian C. J. and Alcántara, José I. and Glasberg, Brian R.},
  date = {1999-11-23},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {106},
  pages = {3603--3619},
  issn = {0001-4966},
  doi = {10.1121/1.428213},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.428213},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KSWA6UZH\\1.html},
  number = {6}
}

@article{stoneComparisonDifferentForms1999b,
  title = {Comparison of Different Forms of Compression Using Wearable Digital Hearing Aids},
  author = {Stone, Michael A. and Moore, Brian C. J. and Alcántara, José I. and Glasberg, Brian R.},
  date = {1999-11-23},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {106},
  pages = {3603--3619},
  issn = {0001-4966},
  doi = {10.1121/1.428213},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.428213},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5BX9B86C\\1.html},
  number = {6}
}

@article{stoneComparisonDifferentForms1999c,
  title = {Comparison of Different Forms of Compression Using Wearable Digital Hearing Aids},
  author = {Stone, Michael A. and Moore, Brian C. J. and Alcántara, José I. and Glasberg, Brian R.},
  date = {1999-11-23},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {106},
  pages = {3603--3619},
  issn = {0001-4966},
  doi = {10.1121/1.428213},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.428213},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ITBQFZBE\\1.html},
  number = {6}
}

@inproceedings{straussDREGONDatasetMethods2018,
  title = {{{DREGON}}: {{Dataset}} and {{Methods}} for {{UAV}}-{{Embedded Sound Source Localization}}},
  shorttitle = {{{DREGON}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Strauss, Martin and Mordel, Pol and Miguet, Victor and Deleforge, Antoine},
  date = {2018-10},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Madrid}},
  doi = {10.1109/IROS.2018.8593581},
  url = {https://ieeexplore.ieee.org/document/8593581/},
  urldate = {2019-06-20},
  abstract = {This paper introduces DREGON, a novel publiclyavailable dataset that aims at pushing research in sound source localization using a microphone array embedded in an unmanned aerial vehicle (UAV). The dataset contains both clean and noisy in-flight audio recordings continuously annotated with the 3D position of the target sound source using an accurate motion capture system. In addition, various signals of interests are available such as the rotational speed of individual rotors and inertial measurements at all time. Besides introducing the dataset, this paper sheds light on the specific properties, challenges and opportunities brought by the emerging task of UAV-embedded sound source localization. Several baseline methods are evaluated and compared on the dataset, with real-time applicability in mind. Very promising results are obtained for the localization of a broad-band source in loud noise conditions, while speech localization remains a challenge under extreme noise levels.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RQQFCN6H\\Strauss et al. - 2018 - DREGON Dataset and Methods for UAV-Embedded Sound.pdf},
  isbn = {978-1-5386-8094-0},
  langid = {english}
}

@online{StructuredApproachUnsupervised,
  title = {A {{Structured Approach}} to {{Unsupervised Depth Learning}} from {{Monocular Videos}}},
  journaltitle = {Google AI Blog},
  url = {http://ai.googleblog.com/2018/11/a-structured-approach-to-unsupervised.html},
  urldate = {2019-11-11},
  abstract = {Posted by Anelia Angelova, Research Scientist, Robotics at Google   Perceiving the depth of a scene is an important task for an autonomous r...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\24K87BWQ\\a-structured-approach-to-unsupervised.html},
  langid = {english}
}

@article{sundarTDOABasedMultipleAcoustic2018,
  title = {{{TDOA}}-{{Based Multiple Acoustic Source Localization Without Association Ambiguity}}},
  author = {Sundar, H. and Sreenivas, T. V. and Seelamantula, C. S.},
  date = {2018-11},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  pages = {1976--1990},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2018.2851147},
  abstract = {Multiple source localization using time-differences of arrival (TDOAs) is challenging because of the ambiguity involved in associating the TDOAs computed across microphone pairs to the sources. We show that the association ambiguity of the TDOAs can be effectively resolved using the concept of an inverse delay interval region (IDIR), which we introduce in this paper. By examining the association between a spatial domain and the TDOAs, we define IDIR as an interhyperboloidal spatial region corresponding to an interval of delays for a given pair of microphones. The proposed scheme for localizing multiple sources involves two stages. In the first stage, the given enclosure is partitioned into nonoverlapping elemental regions and the ones that contain a source are detected using a measure based on the generalized cross-correlation with phase transform and the IDIRs. In the second stage, the sources are finely localized within each of the detected elemental regions by identifying the IDIRs containing a single source and a novel region-constrained localization approach. We evaluate the performance of the proposed approach on real recordings from the AV16.3 corpus and in a simulated reverberation setting with a reverberation time RT60 of up to 500 ms, and show that the DOA estimation error with two active speakers is within 2° and the spatial localization error is less than 30 cm for each speaker.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WGEG84S3\\Sundar et al. - 2018 - TDOA-Based Multiple Acoustic Source Localization W.pdf;C\:\\Users\\sauli\\Zotero\\storage\\KYIB3JFB\\8398408.html},
  keywords = {acoustic signal processing,array signal processing,Delays,direction-of-arrival estimation,Direction-of-arrival estimation,GCC-PHAT,hyperboloid branches,IDIR,interhyperboloidal spatial region,inverse delay interval region,microphone arrays,microphone pairs,microphones,Microphones,multi-source localization,multiple acoustic source localization,multiple source localization,permutation problem,Position measurement,region-constrained localization,reverberation,Reverberation,speaker recognition,Speech processing,TDOA,time-difference of arrival,time-differences of arrival,time-of-arrival estimation},
  number = {11}
}

@article{sundarTDOABasedMultipleAcoustic2018a,
  title = {{{TDOA}}-{{Based Multiple Acoustic Source Localization Without Association Ambiguity}}},
  author = {Sundar, Harshavardhan and Sreenivas, V, Thippur and Seelamantula, Chandra Sekhar},
  date = {2018-11},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{1976-1990\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2851147\}},
  abstract = {Multiple source localization using time-differences of arrival (TDOAs ) is challenging because of the ambiguity involved in associating the TDOAs computed across microphone pairs to the sources. We show that the association ambiguity of the TDOAs can be effectively resolved using the concept of an inverse delay interval region (IDIR), which we introduce in this paper. By examining the association between a spatial domain and the TDOAs, we define IDIR as an interhyperboloidal spatial region corresponding to an interval of delays for a given pair of microphones. The proposed scheme for localizing multiple sources involves two stages. In the first stage, the given enclosure is partitioned into nonoverlapping elemental regions and the ones that contain a source are detected using a measure based on the generalized cross-correlation with phase transform and the IDIRs. In the second stage, the sources are finely localized within each of the detected elemental regions by identifying the IDIRs containing a single source and a novel region-constrained localization approach. We evaluate the performance of the proposed approach on real recordings from the AV16.3 corpus and in a simulated reverberation setting with a reverberation time RT60 of up to 500 ms, and show that the DOA estimation error with two active speakers is within 2 degrees and the spatial localization error is less than 30 cm for each speaker.},
  affiliation = {Sundar, H (Reprint Author), Amazon Com Inc, Cambridge, MA 02142 USA. Sundar, Harshavardhan, Amazon Com Inc, Cambridge, MA 02142 USA. Sreenivas, Thippur, V, Indian Inst Sci, Dept Elect Commun Engn, Bengaluru 560012, India. Seelamantula, Chandra Sekhar, Indian Inst Sci, Dept Elect Engn, Bengaluru 560012, India.},
  author-email = {hsundar@ieee.org tvsree@ece.iisc.ernet.in chandra.sekhar@ieee.org},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Brandstein MS, 1997, COMPUT SPEECH LANG, V11, P91, DOI 10.1006/csla.1996.0024. Bremner D, 1998, DISCRETE COMPUT GEOM, V20, P333, DOI 10.1007/PL00009389. Brutti A., 2010, EURASIP J AUDIO SPEE, V2010. Canclini A, 2015, IEEE-ACM T AUDIO SPE, V23, P1563, DOI 10.1109/TASLP.2015.2439040. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. CHAN YT, 1994, IEEE T SIGNAL PROCES, V42, P1905, DOI 10.1109/78.301830. Chen JD, 2005, EURASIP J APPL SIG P, V2005, P25, DOI 10.1155/ASP.2005.25. Coleman T. F., 2000, SIAM J OPTIMIZ, V6, P418. Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456. Coleman TF, 1994, MATH PROGRAM, V67, P189, DOI DOI 10.1007/BF01582221. De Mori R., 1998, SPOKEN DIALOGUES COM. De Sena E, 2015, IEEE-ACM T AUDIO SPE, V23, P774, DOI 10.1109/TASLP.2015.2405476. DiBiase JH, 2000, THESIS. DICLAUDIO ED, 2000, ACOUST SPEECH SIG PR, P921. Dmochowski J, 2007, IEEE T AUDIO SPEECH, V15, P1327, DOI 10.1109/TASL.2006.889795. Dmochowski JP, 2007, IEEE T AUDIO SPEECH, V15, P2510, DOI 10.1109/TASL.2007.906694. Friedlander B., 1986, SCT5503 DEF TECH INF. Garofolo J. S., 2007, DARPA TIMIT ACOUSTIC. Gillette MD, 2008, IEEE SIGNAL PROC LET, V15, P1, DOI 10.1109/LSP.2007.910324. Godbole S, 2004, LECT NOTES ARTIF INT, V3056, P22. Grunbaum B., 2003, CONVEX POLYTOPES, V221. Habets E., 2002, P 13 ANN WORKSH CIRC, P1. Habets E. A. P., 2010, ROOM IMPULSE RESPONS. Huang Y., 2002, P IEEE WORKSH APPL S, P67. HUANG YT, 2000, ACOUST SPEECH SIG PR, P909. Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5. JACOVITTI G, 1993, IEEE T SIGNAL PROCES, V41, P525, DOI 10.1109/78.193195. Klain D. A., 1997, INTRO GEOMETRIC PROB. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. LATHOUD G, 2005, MACHINE LEARNING MUL, V3361, P182. Lathoud G, 2007, IEEE T AUDIO SPEECH, V15, P1696, DOI 10.1109/TASL.2007.896667. Lee Y, 2010, INT CONF ACOUST SPEE, P2722, DOI 10.1109/ICASSP.2010.5496229. Liang ZW, 2008, APPL ACOUST, V69, P1350, DOI 10.1016/j.apacoust.2007.11.010. Loesch Benedikt, 2009, 2009 IEEE/SP 15th Workshop on Statistical Signal Processing (SSP), P677, DOI 10.1109/SSP.2009.5278486. Lombard A, 2009, INT CONF ACOUST SPEE, P233, DOI 10.1109/ICASSP.2009.4959563. Lui KWK, 2009, SIGNAL PROCESS, V89, P1835, DOI 10.1016/j.sigpro.2009.03.009. Ma WK, 2006, IEEE T SIGNAL PROCES, V54, P3291, DOI 10.1109/TSP.2006.877658. Moore D., 2002, COM0207 IDIAP RES I. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Ribeiro F, 2010, IEEE T AUDIO SPEECH, V18, P1781, DOI 10.1109/TASL.2010.2052250. Sawada H., 2005, 2005 IEEE Antennas and Propagation Society International Symposium (IEEE Cat. No. 05CH37629), P81. Scheuing J, 2008, IEEE T AUDIO SPEECH, V16, P1479, DOI 10.1109/TASL.2008.2004533. SCHMIDT RO, 1972, IEEE T AERO ELEC SYS, VAES8, P821, DOI 10.1109/TAES.1972.309614. SMITH JO, 1987, IEEE T ACOUST SPEECH, V35, P1661, DOI 10.1109/TASSP.1987.1165089. Teng PX, 2010, INT CONF ACOUST SPEE, P145, DOI 10.1109/ICASSP.2010.5496114. Torres AM, 2012, J ACOUST SOC AM, V132, P1511, DOI 10.1121/1.4740503.},
  da = {2018-10-18},
  doc-delivery-number = {GQ1VQ},
  funding-acknowledgement = {XII Plan Grant of the Indian Institute of Science (IISc)},
  funding-text = {The work of C. S. Seelamantula was supported by the XII Plan Grant of the Indian Institute of Science (IISc). This work was done when H. Sundar was with the Department of Electrical and Communication Engineering, Indian Institute of Science, Bengaluru India, and later with the Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD USA.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {GCC-PHAT,hyperboloid branches,multi-source localization,permutation  problem,reverberation,TDOA,time-difference of arrival)},
  keywords-plus = {TIME-DELAY ESTIMATION; SOUND SOURCE LOCALIZATION; REVERBERANT ENVIRONMENTS; MICROPHONE NETWORKS; LOCATION; ROBUST; RANGE; ALGORITHM; TRACKING; SPEAKERS},
  langid = {english},
  number = {11},
  number-of-cited-references = {47},
  orcid-numbers = {Sundar, Harshavardhan/0000-0002-8305-7701},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000441430600003},
  usage-count-last-180-days = {14},
  usage-count-since-2013 = {14},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{sunIndoorSoundSource2018,
  title = {Indoor {{Sound Source Localization With Probabilistic Neural Network}}},
  author = {Sun, Y. and Chen, J. and Yuen, C. and Rahardja, S.},
  date = {2018-08},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  volume = {65},
  pages = {6403--6413},
  issn = {0278-0046},
  doi = {10.1109/TIE.2017.2786219},
  abstract = {It is known that adverse environments such as high reverberation and low signal-to-noise ratio (SNR) pose a great challenge to indoor sound source localization (SSL). To address this challenge, in this paper, we propose an SSL algorithm based on a probabilistic neural network, namely a generalized cross-correlation classification algorithm (GCA). Experimental results for adverse environments with high reverberation time T60up to 600 ms and low SNR such as -10 dB show that the average azimuth angle error and elevation angle error by GCA are only 4.6° and 3.1°, respectively. Compared with three recently published algorithms, GCA has increased the success rate on direction of arrival estimation significantly with good robustness to environmental changes. These results show that the proposed GCA can localize accurately and robustly for diverse indoor applications where the site acoustic features can be studied prior to the localization stage.},
  keywords = {acoustic signal processing,average azimuth angle error,Classification algorithms,Direction of arrival (DOA),direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,diverse indoor applications,elevation angle error,Estimation,GCA,generalized cross correlation (GCC),generalized cross-correlation classification algorithm,high reverberation time,indoor sound source localization,localization stage,low signal-to-noise ratio,low SNR,machine learning,Microphone arrays,neural nets,probabilistic neural network,probabilistic neural network (PNN),probability,reverberation,Signal to noise ratio,site acoustic features,sound source localization (SSL),SSL algorithm,Training},
  number = {8}
}

@article{sunIndoorSoundSource2018a,
  title = {Indoor {{Sound Source Localization With Probabilistic Neural Network}}},
  author = {Sun, Yingxiang and Chen, Jiajia and Yuen, Chau and Rahardja, Susanto},
  date = {2018-08},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  volume = {65},
  pages = {6403--6413},
  issn = {0278-0046, 1557-9948},
  doi = {10.1109/TIE.2017.2786219},
  url = {http://ieeexplore.ieee.org/document/8234649/},
  urldate = {2018-10-19},
  abstract = {It is known that adverse environments such as high reverberation and low signal-to-noise ratio (SNR) pose a great challenge to indoor sound source localization. To address this challenge, in this paper, we propose a sound source localization algorithm based on probabilistic neural network, namely Generalized cross correlation Classification Algorithm (GCA). Experimental results for adverse environments with high reverberation time T60 up to 600ms and low SNR such as ‒10dB show that, the average azimuth angle error and elevation angle error by GCA are only 4.6º and 3.1º respectively. Compared with three recently published algorithms, GCA has increased the success rate on direction of arrival estimation significantly with good robustness to environmental changes. These results show that the proposed GCA can localize accurately and robustly for diverse indoor applications where the site acoustic features can be studied prior to the localization stage.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BGPZMKKQ\\Sun et al. - 2018 - Indoor Sound Source Localization With Probabilisti.pdf},
  langid = {english},
  number = {8}
}

@article{sunIndoorSoundSource2018b,
  title = {Indoor {{Sound Source Localization With Probabilistic Neural Network}}},
  author = {Sun, Yingxiang and Chen, Jiajia and Yuen, Chau and Rahardja, Susanto},
  date = {2018-08},
  journaltitle = {IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS},
  volume = {65},
  pages = {\{6403-6413\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {0278-0046},
  doi = {\{10.1109/TIE.2017.2786219\}},
  abstract = {It is known that adverse environments such as high reverberation and low signal-to-noise ratio (SNR) pose a great challenge to indoor sound source localization (SSL). To address this challenge, in this paper, we propose an SSL algorithm based on a probabilistic neural network, namely a generalized cross-correlation classification algorithm (GCA). Experimental results for adverse environments with high reverberation time T-60 up to 600 ms and low SNR such as -10 dB show that the average azimuth angle error and elevation angle error by GCA are only 4.6 degrees and 3.1 degrees, respectively. Compared with three recently published algorithms, GCA has increased the success rate on direction of arrival estimation significantly with good robustness to environmental changes. These results show that the proposed GCA can localize accurately and robustly for diverse indoor applications where the site acoustic features can be studied prior to the localization stage.},
  affiliation = {Chen, JJ (Reprint Author), Singapore Univ Technol & Design, Pillar Engn Prod Dev, Singapore 487372, Singapore. Sun, Yingxiang; Chen, Jiajia; Yuen, Chau, Singapore Univ Technol & Design, Pillar Engn Prod Dev, Singapore 487372, Singapore. Rahardja, Susanto, Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710065, Shaanxi, Peoples R China. Rahardja, Susanto, Sekolah Tinggi Manajemen & Ilmu Komputer STMIK, Tangerang 15119, Indonesia.},
  author-email = {yingxiang_sun@mymail.sutd.edu.sg jiajia_chen@sutd.edu.sg yuenchau@sutd.edu.sg susantorahardja@ieee.org},
  cited-references = {Alameda-Pineda X., 2014, GTDE MATLAB TOOLBOX. Alameda-Pineda X, 2014, IEEE-ACM T AUDIO SPE, V22, P1082, DOI 10.1109/TASLP.2014.2317989. Argentieri S, 2015, COMPUT SPEECH LANG, V34, P87, DOI 10.1016/j.csl.2015.03.003. Canclini A, 2013, IEEE T AUDIO SPEECH, V21, P439, DOI 10.1109/TASL.2012.2215601. Chen HW, 2009, ISCAS: 2009 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-5, P1863, DOI 10.1109/ISCAS.2009.5118142. Chen JD, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/26503. Deng F, 2017, IEEE T IND ELECTRON, V64, P4894, DOI 10.1109/TIE.2017.2652394. Dmochowski JP, 2007, IEEE T AUDIO SPEECH, V15, P2510, DOI 10.1109/TASL.2007.906694. Guo H, 2011, IEEE T IND ELECTRON, V58, P741, DOI 10.1109/TIE.2009.2022073. He HS, 2013, IEEE T AUDIO SPEECH, V21, P463, DOI 10.1109/TASL.2012.2223674. Hu Y., NOIZEUS DATABASE. Kim J, 2016, IEEE T IND ELECTRON, V63, P3616, DOI 10.1109/TIE.2016.2523460. Laufer-Goldshtein B, 2016, IEEE-ACM T AUDIO SPE, V24, P1393, DOI 10.1109/TASLP.2016.2555085. Lee T., 2015, TLSSC CODE. Lehmann E., 2012, FAST ISM CODE. Lehmann EA, 2010, IEEE T AUDIO SPEECH, V18, P1429, DOI 10.1109/TASL.2009.2035038. Li XF, 2013, IEEE T CYBERNETICS, V43, P1199, DOI 10.1109/TSMCB.2012.2226443. Mungamuru B, 2004, IEEE T SYST MAN CY B, V34, P1526, DOI 10.1109/TSMCB.2004.826398. Pavlidi D, 2013, IEEE T AUDIO SPEECH, V21, P2193, DOI 10.1109/TASL.2013.2272524. Song H, 2016, IEEE T IND ELECTRON, V63, P3725, DOI 10.1109/TIE.2016.2521346. SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q. Velasco J, 2016, SIGNAL PROCESS, V119, P209, DOI 10.1016/j.sigpro.2015.08.003. Wang B, 2015, IEEE T IND ELECTRON, V62, P572, DOI 10.1109/TIE.2014.2327595. Wang J, 2012, IEEE T IND ELECTRON, V59, P1622, DOI 10.1109/TIE.2011.2165462. Wang J, 2015, IEEE T IND ELECTRON, V62, P2397, DOI 10.1109/TIE.2014.2360140. Xiao X., 2015, P IEEE INT C AC SPEE, P76. Yang P, 2014, IEEE T IND ELECTRON, V61, P5641, DOI 10.1109/TIE.2014.2301737. Yook D, 2016, IEEE T CYBERNETICS, V46, P20, DOI 10.1109/TCYB.2015.2391252.},
  da = {2018-10-18},
  doc-delivery-number = {GB2RJ},
  eissn = {1557-9948},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\YJZK4BM8\\Sun et al. - 2018 - Indoor Sound Source Localization With Probabilisti.pdf},
  funding-acknowledgement = {SUTD-MIT International Design Center [IDG31700104]; National Natural Science Foundation of China (NSFC) [61750110529]},
  funding-text = {This work was supported in part by the SUTD-MIT International Design Center under Grant IDG31700104 and in part by the National Natural Science Foundation of China (NSFC) under Grant 61750110529.},
  journal-iso = {IEEE Trans. Ind. Electron.},
  keywords = {(Direction of arrival (DOA),generalized cross correlation (GCC),good intro,machine  learning,probabilistic neural network (PNN),sound source localization  (SSL))},
  keywords-plus = {TIME DIFFERENCE; ARRAY},
  langid = {english},
  number = {8},
  number-of-cited-references = {28},
  orcid-numbers = {Sun, Yingxiang/0000-0001-5456-0757},
  research-areas = {Automation & Control Systems; Engineering; Instruments & Instrumentation},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000428902200038},
  usage-count-last-180-days = {62},
  usage-count-since-2013 = {62},
  web-of-science-categories = {Automation & Control Systems; Engineering, Electrical & Electronic; Instruments & Instrumentation}
}

@inproceedings{suRealtime3DSound2017,
  title = {Towards Real-Time {{3D}} Sound Sources Mapping with Linear Microphone Arrays},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Su, Daobilige and Vidal-Calleja, Teresa and Miro, Jaime Valls},
  date = {2017-05},
  pages = {1662--1668},
  publisher = {{IEEE}},
  location = {{Singapore, Singapore}},
  doi = {10.1109/ICRA.2017.7989196},
  url = {http://ieeexplore.ieee.org/document/7989196/},
  urldate = {2019-08-16},
  abstract = {In this paper, we present a method for realtime 3D sound sources mapping using an off-the-shelf robotic perception sensor equipped with a linear microphone array. Conventional approaches to map sound sources in 3D scenarios use dedicated 3D microphone arrays, as this type of arrays provide two degrees of freedom (DOF) observations. Our method addresses the problem of 3D sound sources mapping using a linear microphone array, which only provides one DOF observations making the estimation of the sound sources location more challenging. In the proposed method, multi hypotheses tracking is combined with a new sound source parametrisation to provide with a good initial guess for an online optimisation strategy. A joint optimisation is carried out to estimate 6 DOF sensor poses and 3 DOF landmarks together with the sound sources locations. Additionally, a dedicated sensor model is proposed to accurately model the noise of the Direction of Arrival (DOA) observation when using a linear microphone array. Comprehensive simulation and experimental results show the effectiveness of the proposed method. In addition, a real-time implementation of our method has been made available as open source software for the benefit of the community.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\LB2LVFZK\\Su et al. - 2017 - Towards real-time 3D sound sources mapping with li.pdf},
  isbn = {978-1-5090-4633-1},
  langid = {english}
}

@article{suRoboticSoundSource,
  title = {Robotic {{Sound Source Mapping}} Using {{Microphone Arrays}}},
  author = {Su, Daobilige},
  pages = {198},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DIMFUM3J\\Su - Robotic Sound Source Mapping using Microphone Arra.pdf},
  keywords = {*****,READ},
  langid = {english}
}

@article{SurveySoundSource,
  title = {A {{Survey}} on {{Sound Source Localization}} in {{Robotics}}: {{From Binaural}} to {{Array Processing Methods}} - {{Document}}},
  url = {https://hal-univ-tlse3.archives-ouvertes.fr/hal-01058575/document},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BZUEYXSW\\document.html}
}

@online{SurveySoundSourcea,
  title = {A {{Survey}} on {{Sound Source Localization}} in {{Robotics}}: From {{Binaural}} to {{Array Processing Methods}} - Document},
  url = {https://hal-univ-tlse3.archives-ouvertes.fr/hal-01058575/document},
  urldate = {2017-05-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PFKP4TDI\\document.html}
}

@inproceedings{suSplitConditionalIndependent2016,
  title = {Split Conditional Independent Mapping for Sound Source Localisation with {{Inverse}}-{{Depth Parametrisation}}},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Su, D. and Vidal-Calleja, T. and Miro, J. V.},
  date = {2016-10},
  pages = {2000--2006},
  doi = {10.1109/IROS.2016.7759315},
  abstract = {In this paper, we propose a framework to map stationary sound sources while simultaneously localise a moving robot. Conventional methods for localisation and sound source mapping rely on a microphone array and either, 1) a proprioceptive sensor only (such as wheel odometry) or 2) an additional exteroceptive sensor (such as cameras or lasers) to get accurately the robot locations. Since odometry drifts over time and sound observations are bearing-only, sparse and extremely noisy, the former can only deal with relatively short trajectories before the whole map drifts. In comparison, the latter can get more accurate trajectory estimation over long distances and a better estimation of the sound source map as a result. However, in most of the work in the literature, trajectory estimation and sound source mapping are treated as uncorrelated, which means an update on the robot trajectory does not propagate properly to the sound source map. In this paper, we proposed an efficient method to correlate robot trajectory with sound source mapping by exploiting the conditional independence property between two maps estimated by two different Simultaneous Localisation and Mapping (SLAM) algorithms running in parallel. In our approach, the first map has the flexibility that can be built with any SLAM algorithm (filtering or optimisation) to estimate robot poses with an exteroceptive sensor. The second map is built by using a filtering-based SLAM algorithm locating all stationary sound sources parametrised with Inverse Depth Parametrisation (IDP). Robot locations used during IDP initialisation are the common features shared between the two SLAM maps, which allow to propagate information accordingly. Comprehensive simulations and experimental results show the effectiveness of the proposed method.},
  eventtitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UCRPXG69\\7759315.html},
  keywords = {conditional independence property,Estimation,estimation theory,exteroceptive sensor,filtering theory,filtering-based SLAM algorithm,IDP initialisation,inverse-depth parametrisation,microphone array,mobile robots,Mobile robots,moving robot localisation,pose estimation,proprioceptive sensor,Robot kinematics,robot pose estimation,robot trajectory estimation,robot vision,simultaneous localisation and mapping,Simultaneous localization and mapping,SLAM (robots),sound source localisation,split conditional independent mapping,stationary sound source mapping,Trajectory}
}

@inproceedings{svaizerAcousticSourceLocation1997,
  title = {Acoustic Source Location in a Three-Dimensional Space Using Crosspower Spectrum Phase},
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Svaizer, P. and Matassoni, M. and Omologo, M.},
  date = {1997-04},
  volume = {1},
  pages = {231-234 vol.1},
  doi = {10.1109/ICASSP.1997.599611},
  abstract = {A microphone array can be used to locate a dominant acoustic source in a given environment. This capability is successfully employed to locate an active talker in teleconferencing or other multi-speaker applications. In this work the source location is obtained in two steps: (1) a time difference of arrival (TDOA) computation between the signals of the array; (2) an “optimal” source location based on the interchannel delay estimates and on a geometrical description of the sensor arrangement. The crosspower spectrum phase technique was used for TDOA estimation, while a maximum likelihood approach was followed to derive the source coordinates. Source location experiments in a three-dimensional space were performed by means of an array of 8 microphones. For this purpose both a loudspeaker and a real talker were used to collect data in a large noisy and reverberant room},
  eventtitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RI55IB48\\Svaizer et al. - 1997 - Acoustic source location in a three-dimensional sp.pdf;C\:\\Users\\sauli\\Zotero\\storage\\XVA8P25M\\599611.html},
  keywords = {acoustic arrays,acoustic signal processing,acoustic source location,acoustic transducer arrays,active talker,architectural acoustics,array signal processing,crosspower spectrum phase,Delay effects,Delay estimation,delays,direction-of-arrival estimation,geometrical description,interchannel delay estimates,large noisy reverberant room,loudspeaker,maximum likelihood approach,Maximum likelihood estimation,microphone array,microphone arrays,Microphones,multi-speaker applications,Phase estimation,Position measurement,sensor arrangement,Sensor arrays,Sensor phenomena and characterization,source coordinates,spectral analysis,TDOA computation,Teleconferencing,three-dimensional space,Time difference of arrival,time difference of arrival computation}
}

@inproceedings{SvaizerAcousticsourcelocation1997,
  title = {Acoustic {{Source Location}} in a {{Three}}-{{Dimensional Space Using Crosspower Spectrum Phase}}},
  booktitle = {1997 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Svaizer, P. and Matassoni, M. and Omologo, M.},
  date = {1997-04},
  volume = {1},
  pages = {231-234 vol.1},
  doi = {10.1109/ICASSP.1997.599611},
  abstract = {A microphone array can be used to locate a dominant acoustic source in a given environment. This capability is successfully employed to locate an active talker in teleconferencing or other multi-speaker applications. In this work the source location is obtained in two steps: (1) a time difference of arrival (TDOA) computation between the signals of the array; (2) an “optimal” source location based on the interchannel delay estimates and on a geometrical description of the sensor arrangement. The crosspower spectrum phase technique was used for TDOA estimation, while a maximum likelihood approach was followed to derive the source coordinates. Source location experiments in a three-dimensional space were performed by means of an array of 8 microphones. For this purpose both a loudspeaker and a real talker were used to collect data in a large noisy and reverberant room},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DJBPXY8V\\Svaizer et al. - 1997 - Acoustic source location in a three-dimensional sp.pdf},
  keywords = {acoustic arrays,acoustic signal processing,acoustic source location,acoustic transducer arrays,active talker,architectural acoustics,array signal processing,crosspower spectrum phase,Delay effects,Delay estimation,delays,direction-of-arrival estimation,geometrical description,interchannel delay estimates,large noisy reverberant room,loudspeaker,maximum likelihood approach,Maximum likelihood estimation,microphone array,microphone arrays,Microphones,multi-speaker applications,Phase estimation,Position measurement,sensor arrangement,Sensor arrays,Sensor phenomena and characterization,source coordinates,spectral analysis,TDOA computation,Teleconferencing,three-dimensional space,Time difference of arrival,time difference of arrival computation}
}

@inproceedings{svenssonModellingAcousticSpaces2002,
  title = {Modelling Acoustic Spaces for Audio Virtual Reality},
  booktitle = {Proceedings of the {{IEEE Benelux Workshop}} on {{Model Based Processing}} and {{Coding}} of {{Audio}}},
  author = {Svensson, U. Peter},
  date = {2002},
  pages = {109--116},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\Z36EZ3HF\\Svensson - 2002 - Modelling acoustic spaces for audio virtual realit.pdf}
}

@online{swarbrickjonesConvolutionalAutoencodersPython2015,
  title = {Convolutional Autoencoders in Python/Theano/Lasagne},
  author = {{swarbrickjones}},
  date = {2015-04-29T20:50:30+00:00},
  journaltitle = {Mike Swarbrick Jones' Blog},
  url = {https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/},
  urldate = {2017-02-19},
  abstract = {If you are just looking for code for a convolutional autoencoder in python, look at this git.~ It needs quite a few python dependencies, the only non-standard~ones are theano, nolearn, and lasagne …},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\574WF2DS\\convolutional-autoencoders-in-pythontheanolasagne.html}
}

@article{swarbrickjonesConvolutionalautoencoderspython2015,
  title = {Convolutional {{Autoencoders}} in {{Python}}/{{Theano}}/{{Lasagne}}},
  author = {{swarbrickjones}},
  date = {2015-04},
  journaltitle = {Mike Swarbrick Jones' Blog},
  abstract = {If you are just looking for code for a convolutional autoencoder in python, look at this git.\textasciitilde{} It needs quite a few python dependencies, the only non-standard~ones are theano, nolearn, and lasagne …},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ATYJ7V2Y\\convolutional-autoencoders-in-pythontheanolasagne.html}
}

@online{T60FileExchange,
  title = {T60.m - {{File Exchange}} - {{MATLAB Central}}},
  url = {https://www.mathworks.com/matlabcentral/fileexchange/1212},
  urldate = {2019-06-19},
  abstract = {Returns an estimate of t60, a measure of reverberation time.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MWGLBY2C\\1212-t60-m.html}
}

@article{tachikawa3DSoundSource2018,
  title = {{{3D}} Sound Source Localization Based on Coherence-Adjusted Monopole Dictionary and Modified Convex Clustering},
  author = {Tachikawa, Tomoya and Yatabe, Kohei and Oikawa, Yasuhiro},
  date = {2018-10},
  journaltitle = {APPLIED ACOUSTICS},
  volume = {139},
  pages = {\{267-281\}},
  publisher = {ELSEVIER SCI LTD},
  location = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
  issn = {0003-682X},
  doi = {\{10.1016/j.apacoust.2018.04.033\}},
  abstract = {In this paper, a sound source localization method for simultaneously estimating both direction-of-arrival (DOA) and distance from the microphone array is proposed. For estimating distance, the off-grid problem must be overcome because the range of distance to be considered is quite broad and even not bounded. The proposed method estimates the positions based on a modified version of the convex clustering method combined with the sparse coefficients estimation. A method for constructing a suitable monopole dictionary based on the coherence is also proposed so that the convex clustering based method can appropriately estimate distance of the sound sources. Numerical and measurement experiments were performed to investigate the performance of the proposed method.},
  affiliation = {Tachikawa, T (Reprint Author), Waseda Univ, Dept Intermedia Art & Sci, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan. Tachikawa, Tomoya; Yatabe, Kohei; Oikawa, Yasuhiro, Waseda Univ, Dept Intermedia Art & Sci, Shinjuku Ku, 3-4-1 Ohkubo, Tokyo 1698555, Japan.},
  author-email = {tachi0611s.w@gmail.com},
  cited-references = {Asaei Afsaneh, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1439, DOI 10.1109/ICASSP.2014.6853835. Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542. Bot RI, 2015, MATH PROGRAM, V150, P251, DOI 10.1007/s10107-014-0766-0. CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278. Chi EC, 2015, J COMPUT GRAPH STAT, V24, P994, DOI 10.1080/10618600.2014.948181. Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9. Dokmanic I, 2012, INT CONF ACOUST SPEE, P2617, DOI 10.1109/ICASSP.2012.6288453. Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430. Ekanadham C, 2011, IEEE T SIGNAL PROCES, V59, P4735, DOI 10.1109/TSP.2011.2160058. Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837. Fairweather G, 1998, ADV COMPUT MATH, V9, P69, DOI 10.1023/A:1018981221740. Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475. Hiptmair R, 2016, SURVEY TREFFTZ METHO, P237, DOI 10.1007/978-3-319-41640-3_8. Hocking T, 2011, P 28 INT C MACH LEAR, P1. Huang QH, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-90. HUANG YD, 1991, IEEE T ANTENN PROPAG, V39, P968, DOI 10.1109/8.86917. Hyder MM, 2010, IEEE T SIGNAL PROCES, V58, P4646, DOI 10.1109/TSP.2010.2050477. Ibrahim Mohamed, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6776, DOI 10.1109/ICASSP.2014.6854912. Jamali-Rad H, 2013, IEEE T SIGNAL PROCES, V61, P4874, DOI 10.1109/TSP.2013.2272288. Jiang JJ, 2013, IEEE SENS J, V13, P3136, DOI 10.1109/JSEN.2013.2257735. Karageorghis A, 2011, INVERSE PROBL SCI EN, V19, P309, DOI 10.1080/17415977.2011.551830. Komodakis N, 2015, IEEE SIGNAL PROC MAG, V32, P31, DOI 10.1109/MSP.2014.2377273. Koyama Shoichi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4443, DOI 10.1109/ICASSP.2014.6854442. Koyama S, 2016, INT CONF ACOUST SPEE, P395, DOI 10.1109/ICASSP.2016.7471704. Koyano Y, 2018, APPL ACOUST, V129, P116, DOI 10.1016/j.apacoust.2017.07.014. Koyano Y, 2017, INT CONF ACOUST SPEE, P176, DOI 10.1109/ICASSP.2017.7952141. Koyano Y, 2016, INT CONF ACOUST SPEE, P370, DOI 10.1109/ICASSP.2016.7471699. Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899. Kumar L, 2016, IEEE T SIGNAL PROCES, V64, P3351, DOI 10.1109/TSP.2016.2543201. Kumar L, 2014, 2014 4TH JOINT WORKSHOP ON HANDS-FREE SPEECH COMMUNICATION AND MICROPHONE ARRAYS (HSCMA), P82, DOI 10.1109/HSCMA.2014.6843256. Kusano T, 2018, ACOUST SCI TECHNOL, V39, P215, DOI 10.1250/ast.39.215. Le Roux J, 2013, INT CONF ACOUST SPEE, P4310, DOI 10.1109/ICASSP.2013.6638473. Liang JL, 2010, IEEE T SIGNAL PROCES, V58, P108, DOI 10.1109/TSP.2009.2029723. Liu Z, 2013, SIGN PROC COMM COMP, P1, DOI [10.1109/ICSPCC.2013.6663980, DOI 10.1109/ICSPCC.2013.6663980]. Lv XL, 2011, IEEE T SIGNAL PROCES, V59, P1371, DOI 10.1109/TSP.2011.2105478. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x. Ortner N, 2015, DISTRIBUTIONS FUNDAM, P1, DOI [10.1007/978-3-319-20140-5_1, DOI 10.1007/978-3-319-20140-5_1]. Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003. ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276. Smyrlis YS, 2009, J APPROX THEORY, V161, P617, DOI 10.1016/j.jat.2008.11.018. Tachikawa T, 2016, P MEET ACOUST, V29, DOI 10.1121/2.0000528. Tachikawa T, 2017, INT CONF ACOUST SPEE, P3191, DOI 10.1109/ICASSP.2017.7952745. Tamura Y, 2017, ACOUST SCI TECHNOL, V38, P128, DOI 10.1250/ast.38.128. Tan KM, 2015, ELECTRON J STAT, V9, P2324, DOI 10.1214/15-EJS1074. Tan Z, 2014, IEEE T SIGNAL PROCES, V62, P4997, DOI 10.1109/TSP.2014.2343940. Wang B, 2012, IEEE SIGNAL PROC LET, V19, P487, DOI 10.1109/LSP.2012.2204248. Xenaki A, 2014, J ACOUST SOC AM, V136, P260, DOI 10.1121/1.4883360. Yaghoobi M, 2009, IEEE T SIGNAL PROCES, V57, P4800, DOI 10.1109/TSP.2009.2026610. Yang Z, 2013, IEEE T SIGNAL PROCES, V61, P38, DOI 10.1109/TSP.2012.2222378. Yatabe K, 2015, ACOUST SCI TECHNOL, V36, P351, DOI 10.1250/ast.36.351. Yatabe K, 2015, INT CONF ACOUST SPEE, P504, DOI 10.1109/ICASSP.2015.7178020. Yin JH, 2011, IEEE T SIGNAL PROCES, V59, P4489, DOI 10.1109/TSP.2011.2158425.},
  da = {2018-10-18},
  doc-delivery-number = {GP7UY},
  eissn = {1872-910X},
  journal-iso = {Appl. Acoust.},
  keywords = {(Sparsity,Convex  optimization,Direction-of-arrival (DOA),Distance estimation,Primal-dual splitting)},
  keywords-plus = {SPARSE SIGNAL RECONSTRUCTION; NEAR-FIELD; INVERSE PROBLEMS; ALGORITHM; RECOVERY; ARRAYS},
  langid = {english},
  number-of-cited-references = {53},
  research-areas = {Acoustics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000441117300031},
  usage-count-last-180-days = {6},
  usage-count-since-2013 = {6},
  web-of-science-categories = {Acoustics}
}

@inproceedings{takeda_sound_2016,
  title = {Sound Source Localization Based on Deep Neural Networks with Directional Activate Function Exploiting Phase Information},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2016-03},
  pages = {405--409},
  doi = {10.1109/ICASSP.2016.7471706},
  abstract = {This paper describes sound source localization (SSL) based on deep neural networks (DNNs) using discriminative training. A naïve DNNs for SSL can be configured as follows. Input is the frequency-domain feature used in other SSL methods, and the structure of DNNs is a fully-connected network using real numbers. The training fails because its network structure loses two important properties, i.e., the orthogonality of sub-bands and the intensity- and time-information saved in complex numbers. We solved these two problems by 1) integrating directional information at each sub-band hierarchically, and 2) designing a directional activator that could treat the complex numbers at each sub-band. Our experiments indicated that our method outperformed the naive DNN-based SSL by 20 points in terms of the block-level accuracy.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\533JH97B\\Takeda and Komatani - 2016 - Sound source localization based on deep neural net.pdf;C\:\\Users\\sauli\\Zotero\\storage\\QH7UUWWT\\7471706.html},
  keywords = {deep neural networks,directional activate function,discriminative training,DNN,frequency domain,frequency-domain analysis,frequency-domain feature,Indexes,intensity-information,Microphones,Multiple signal classification,neural nets,Neural networks,Robots,signal processing,sound source localization,SSL methods,time-information,Training}
}

@inproceedings{takedaDiscriminativeMultipleSound2016,
  title = {Discriminative Multiple Sound Source Localization Based on Deep Neural Networks Using Independent Location Model},
  booktitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2016-12},
  pages = {603--609},
  doi = {10.1109/SLT.2016.7846325},
  abstract = {We propose a training method for multiple sound source localization (SSL) based on deep neural networks (DNNs). Such networks function as posterior probability estimator of sound location in terms of position labels and achieve high localization correctness. Since the previous DNNs' configuration for SSL handles one-sound-source cases, it should be extended to multiple-sound-source cases to apply it to real environments. However, a naïve design causes 1) an increase in the number of labels and training data patterns and 2) a lack of label consistency across different numbers of sound sources, such as one and two-or-more-sound cases. These two problems were solved using our proposed method, which involves an independent location model for the former and an block-wise consistent labeling with ordering for the latter. Our experiments indicated that the SSL based on DNNs trained by our proposed training method out-performed a conventional SSL method by a maximum of 18 points in terms of block-level correctness.},
  eventtitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B47MN4QA\\Takeda and Komatani - 2016 - Discriminative multiple sound source localization .pdf;C\:\\Users\\sauli\\Zotero\\storage\\3PIZ8U37\\7846325.html},
  keywords = {Artificial neural networks,block-level correctness,block-wise consistent labeling,deep neural networks,discriminative multiple sound source localization,DNN,Estimation,independent location model,localization correctness,Microphones,multiple sound sources,neural nets,posterior probability estimator,probability,Robots,signal processing,sound source localization,SSL,Training,Training data}
}

@inproceedings{takedaDiscriminativeMultipleSound2016a,
  title = {Discriminative Multiple Sound Source Localization Based on Deep Neural Networks Using Independent Location Model},
  booktitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2016-12},
  pages = {603--609},
  doi = {10.1109/SLT.2016.7846325},
  abstract = {We propose a training method for multiple sound source localization (SSL) based on deep neural networks (DNNs). Such networks function as posterior probability estimator of sound location in terms of position labels and achieve high localization correctness. Since the previous DNNs' configuration for SSL handles one-sound-source cases, it should be extended to multiple-sound-source cases to apply it to real environments. However, a naïve design causes 1) an increase in the number of labels and training data patterns and 2) a lack of label consistency across different numbers of sound sources, such as one and two-or-more-sound cases. These two problems were solved using our proposed method, which involves an independent location model for the former and an block-wise consistent labeling with ordering for the latter. Our experiments indicated that the SSL based on DNNs trained by our proposed training method out-performed a conventional SSL method by a maximum of 18 points in terms of block-level correctness.},
  eventtitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\T8ELK49D\\Takeda and Komatani - 2016 - Discriminative multiple sound source localization .pdf;C\:\\Users\\sauli\\Zotero\\storage\\4UJRWTZA\\7846325.html},
  keywords = {Artificial neural networks,block-level correctness,block-wise consistent labeling,deep neural networks,discriminative multiple sound source localization,DNN,Estimation,independent location model,localization correctness,Microphones,multiple sound sources,neural nets,posterior probability estimator,probability,Robots,signal processing,sound source localization,SSL,Training,Training data}
}

@inproceedings{takedaSoundSourceLocalization2016,
  title = {Sound Source Localization Based on Deep Neural Networks with Directional Activate Function Exploiting Phase Information},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2016-03},
  pages = {405--409},
  doi = {10.1109/ICASSP.2016.7471706},
  abstract = {This paper describes sound source localization (SSL) based on deep neural networks (DNNs) using discriminative training. A naïve DNNs for SSL can be configured as follows. Input is the frequency-domain feature used in other SSL methods, and the structure of DNNs is a fully-connected network using real numbers. The training fails because its network structure loses two important properties, i.e., the orthogonality of sub-bands and the intensity- and time-information saved in complex numbers. We solved these two problems by 1) integrating directional information at each sub-band hierarchically, and 2) designing a directional activator that could treat the complex numbers at each sub-band. Our experiments indicated that our method outperformed the naive DNN-based SSL by 20 points in terms of the block-level accuracy.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UTHJZPQN\\Takeda and Komatani - 2016 - Sound source localization based on deep neural net.pdf;C\:\\Users\\sauli\\Zotero\\storage\\LFGUAVEW\\7471706.html},
  keywords = {deep neural networks,Deep Neural Networks,directional activate function,discriminative training,Discriminative training,DNN,Frequency domain,frequency-domain analysis,Frequency-domain analysis,frequency-domain feature,Indexes,intensity-information,Microphones,Multiple signal classification,neural nets,Neural networks,Robots,signal processing,sound source localization,Sound source localization,SSL methods,time-information,Training}
}

@inproceedings{takedaUnsupervisedAdaptationDeep2017,
  title = {Unsupervised Adaptation of Deep Neural Networks for Sound Source Localization Using Entropy Minimization},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2017-03},
  pages = {2217--2221},
  doi = {10.1109/ICASSP.2017.7952550},
  abstract = {This paper describes an unsupervised method of adapting deep neural networks (DNNs) for sound source localization (SSL). DNNs-based SSL achieves high localization accuracy for sound data that are similar to training data. However, the accuracy deteriorates if a sound source is at an unknown position in unknown reverberant environments. We solve the problem by using unsupervised adaption of the DNNs' parameters to the observed sound signals. Entropy is used as the objective function and minimized to optimize the parameters on the basis of the gradient method. Adaptation without overfitting is achieved by using 1) a parameter adaptation layer, such as linear transform network, and 2) early stopping of the parameter updates. Experimental results indicated that our method improved localization accuracy by a maximum of 20 points for unknown positions and reverberant data.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WDGB99DQ\\Takeda and Komatani - 2017 - Unsupervised adaptation of deep neural networks fo.pdf;C\:\\Users\\sauli\\Zotero\\storage\\VEFXUVVW\\7952550.html},
  keywords = {acoustic signal processing,deep neural networks,Deep neural networks,DNNs-based SSL,entropy,Entropy,entropy minimization,Estimation,Feature extraction,gradient method,gradient methods,Linear programming,linear transform network,Machine learning,minimisation,neural nets,Neural networks,observed sound signals,parameter adaptation layer,parameter updates,Robots,sound source localization,Sound source localization,Training,transforms,unknown reverberant environments,unsupervised adaptation,Unsupervised adaptation}
}

@inproceedings{takedaUnsupervisedAdaptationDeep2017a,
  title = {Unsupervised Adaptation of Deep Neural Networks for Sound Source Localization Using Entropy Minimization},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Takeda, R. and Komatani, K.},
  date = {2017-03},
  pages = {2217--2221},
  doi = {10.1109/ICASSP.2017.7952550},
  abstract = {This paper describes an unsupervised method of adapting deep neural networks (DNNs) for sound source localization (SSL). DNNs-based SSL achieves high localization accuracy for sound data that are similar to training data. However, the accuracy deteriorates if a sound source is at an unknown position in unknown reverberant environments. We solve the problem by using unsupervised adaption of the DNNs' parameters to the observed sound signals. Entropy is used as the objective function and minimized to optimize the parameters on the basis of the gradient method. Adaptation without overfitting is achieved by using 1) a parameter adaptation layer, such as linear transform network, and 2) early stopping of the parameter updates. Experimental results indicated that our method improved localization accuracy by a maximum of 20 points for unknown positions and reverberant data.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FB9WMNLS\\Takeda and Komatani - 2017 - Unsupervised adaptation of deep neural networks fo.pdf;C\:\\Users\\sauli\\Zotero\\storage\\BWPF7V6J\\7952550.html},
  keywords = {acoustic signal processing,deep neural networks,Deep neural networks,DNNs-based SSL,entropy,Entropy,entropy minimization,Estimation,Feature extraction,gradient method,gradient methods,Linear programming,linear transform network,Machine learning,minimisation,neural nets,Neural networks,observed sound signals,parameter adaptation layer,parameter updates,Robots,sound source localization,Sound source localization,Training,transforms,unknown reverberant environments,unsupervised adaptation,Unsupervised adaptation}
}

@inproceedings{takedaUnsupervisedAdaptationNeural2018,
  title = {Unsupervised {{Adaptation}} of {{Neural Networks}} for {{Discriminative Sound Source Localization}} with {{Eliminative Constraint}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Takeda, R. and Kudo, Y. and Takashima, K. and Kitamura, Y. and Komatani, K.},
  date = {2018-04},
  pages = {3514--3518},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461723},
  abstract = {This paper describes an unsupervised adaptation method of deep neural networks (DNNs) regarding discriminative sound source localization (SSL). DNNs-based SSL and its unsupervised adaptation fail under different conditions from those during training. The estimations sometimes include incoherent unpredictable errors due to the NN's non-linearity. We propose an eliminative posterior probability constraint using a model-based SSL for unsupervised DNNs adaptation. This constraint forces the probability of “less possible candidates” to become zero to eliminate incoherent errors. The candidates are indicated by a model-based SSL method because it can estimate the azimuth of the sound source with moderate accuracy and explicit reasoning. As a result, the localization performance of adapted DNNs improved more than that of model-based SSL. Experimental results showed that our method improved localization correctness of 1D azimuth and 3D regions by a maximum of 13.3 and 5.9 points compared with the model-based SSL.},
  keywords = {Adaptation models,Azimuth,Cost function,deep neural networks,discriminative sound source localization,DNN-based SSL,eliminative constraint,eliminative posterior probability constraint,Entropy,incoherent unpredictable errors,learning (artificial intelligence),localization performance,model-based SSL method,neural nets,neural networks,Neural networks,NN nonlinearity,probability,sound source localization,speech recognition,Three-dimensional displays,Training,unsupervised adaptation,unsupervised adaptation method,unsupervised DNN adaptation}
}

@article{talagalaBinauralSoundSource2014,
  title = {Binaural Sound Source Localization Using the Frequency Diversity of the Head-Related Transfer Function},
  author = {Talagala, Dumidu S. and Zhang, Wen and Abhayapala, Thushara D. and Kamineni, Abhilash},
  date = {2014},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {135},
  pages = {1207--1217},
  url = {http://scitation.aip.org/content/asa/journal/jasa/135/3/10.1121/1.4864304},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WIRI8RKI\\Localization, Tracking, and Separation of Sound Sources for Cognitive Robots - dissertation.pdf},
  number = {3}
}

@article{tanakaEvolutionStrategyBased2016,
  title = {Evolution {{Strategy Based Neural Network Optimization}} and {{LSTM Language Model}} for {{Robust Speech Recognition}}},
  author = {Tanaka, Tomohiro and Shinozaki, Takahiro and Watanabe, Shinji and Hori, Takaaki},
  date = {2016},
  pages = {4},
  abstract = {This paper reports our system for the 1-channel track task in the 4th CHiME challenge (CHiME4). A bottle-neck in developing neural network based systems is the tuning of meta-parameters. We automate it by using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) so that high performance system is obtained without relying on human experts. We run two evolution experiments for the DNN acoustic model used in the official baseline system. One uses development set word error rate (WER) after the cross-entropy (CE) based training as the objective function for the evolution, and the other uses the WER after the sequential discriminative training. Additionally, we run an evolution experiment for a Long Short-Term Memory recurrent neural network based language model (LSTM-LM), replacing the original recurrent neural network language model (RNN-LM) used in the baseline system for N-best rescoring. All of these evolution experiments resulted in reduced WERs. To produce the final results, we augmented training data by pooling speech data from all the 6 channels and imported the optimized meta-parameter settings without modification. For the real test data, reduced WER of 17.40\% and 16.58\% were obtained compared to the baseline WER of 22.75\% when the RNN and LSTM-LMs were used, respectively.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\L4KQDTU4\\Tanaka et al. - Evolution Strategy Based Neural Network Optimizati.pdf},
  langid = {english}
}

@article{taseskaBlindSourceSeparation2017,
  title = {Blind {{Source Separation}} of {{Moving Sources Using Sparsity}}-Based {{Source Detection}} and {{Tracking}}},
  author = {Taseska, M. and Habets, E. A. P.},
  date = {2017},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {PP},
  pages = {1--1},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2780993},
  abstract = {Sparsity-based blind source separation (BSS) algorithms in the short time-frequency (TF) domain have received a lot of attention due to their versatility and noise reduction capabilities. In most of these algorithms, the estimation of the BSS filters relies on accurate association of each TF bin to the dominant source at that bin. The TF bin associations are then used to estimate the statistics of the source signals, and BSS is achieved by optimal spatial filters computed using the estimated statistics. The main objective of this paper is to apply such a framework to scenarios with an unknown number of moving sources. While state-of-the-art approaches employ online clustering algorithms to solve the problem for moving sources, we propose an approximate Bayesian tracker and perform the association of each TF bin to the dominant source using the tracker's measurement-to-source association probabilities. Therefore, the choice of the underlying narrowband models and measurements for the tracker, as well as the resulting tracking algorithm constitute the main contributions of this paper. The TF bin associations obtained from the tracker are then used to estimate the statistics of the source signals. The performance of the resulting BSS filters is compared to the performance of state-of-the-art sparsity-based and independent vector analysisbased BSS algorithms. Note that our proposed approach targets scenarios with at least two spatially separated microphone arrays, with known microphone positions and relative orientations. In addition, the framework allows for efficient management of timevarying number of sources.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\82DED8KC\\Taseska and Habets - 2017 - Blind Source Separation of Moving Sources Using Sp.pdf;C\:\\Users\\sauli\\Zotero\\storage\\979996S5\\8169100.html},
  keywords = {acoustic source tracking,Bayes methods,microphone arrays,Narrowband,Position measurement,PSD matrix estimation,Source separation,spatial filtering,Speech,speech processing},
  number = {99}
}

@book{tashevSoundCaptureProcessing2009,
  title = {Sound {{Capture}} and {{Processing}}: {{Practical Approaches}}},
  shorttitle = {Sound {{Capture}} and {{Processing}}},
  author = {Tashev, Ivan Jelev},
  date = {2009-07-01},
  publisher = {{John Wiley \& Sons}},
  abstract = {Provides state-of-the-art algorithms for sound capture,processing and enhancement Sound Capture and Processing: Practical Approaches coversthe digital signal processing algorithms and devices for capturingsounds, mostly human speech. It explores the devices andtechnologies used to capture, enhance and process sound for theneeds of communication and speech recognition in modern computersand communication devices. This book gives a comprehensiveintroduction to basic acoustics and microphones, with coverage ofalgorithms for noise reduction, acoustic echo cancellation,dereverberation and microphone arrays; charting the progress ofsuch technologies from their evolution to present day standard.Sound Capture and Processing: Practical ApproachesBrings together the state-of-the-art algorithms for soundcapture, processing and enhancement in one easily accessiblevolumeProvides invaluable implementation techniques required toprocess algorithms for real life applications and devicesCovers a number of advanced sound processing techniques, suchas multichannel acoustic echo cancellation, dereverberation andsource separationGenerously illustrated with figures and charts to demonstratehow sound capture and audio processing systems workAn accompanying website containing Matlab code to illustratethe algorithmsThis invaluable guide will provide audio, R\&D and softwareengineers in the industry of building systems or computerperipherals for speech enhancement with a comprehensive overview ofthe technologies, devices and algorithms required for moderncomputers and communication devices. Graduate students studyingelectrical engineering and computer science, and researchers inmultimedia, cell-phones, interactive systems and acousticians willalso benefit from this book.},
  eprint = {plll9smnbOIC},
  eprinttype = {googlebooks},
  isbn = {978-0-470-99443-6},
  keywords = {Technology & Engineering / Electrical,Technology & Engineering / Electronics / General},
  langid = {english},
  pagetotal = {390}
}

@article{TDOAPositioningAlgorithms,
  title = {{{TDOA Positioning Algorithms}}: {{Evaluation}} and {{Implementation}} - {{Tdoa}}-{{Frls}}-030923.{{Pdf}}},
  url = {https://web.wpi.edu/Images/CMS/PPL/tdoa-frls-030923.pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WPE58L2D\\tdoa-frls-030923.html}
}

@online{TDOAPositioningAlgorithmsa,
  title = {{{TDOA Positioning Algorithms}}: {{Evaluation}} and {{Implementation}} - Tdoa-Frls-030923.Pdf},
  url = {https://web.wpi.edu/Images/CMS/PPL/tdoa-frls-030923.pdf},
  urldate = {2017-02-16},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U59ES3M4\\tdoa-frls-030923.html}
}

@online{TecDocT33292008b,
  title = {Tec\_doc\_t3329-2008b.Pdf - Tech3329.Pdf},
  url = {https://tech.ebu.ch/docs/tech/tech3329.pdf},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3DH92HWP\\tech3329.html}
}

@online{TechnologyFAQCobraNet,
  title = {Technology {{FAQ}} | {{CobraNet HOME}}},
  url = {http://www.cobranet.info/technology/faq},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VHIPAR6I\\faq.html}
}

@article{TemporalPatternRecognition,
  title = {Temporal {{Pattern Recognition}} by {{Neural Networks}}},
  url = {http://www.peterasaro.org/gesture/temporal_nn.html},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FSRAVGG3\\temporal_nn.html}
}

@online{TemporalPatternRecognitiona,
  title = {Temporal {{Pattern Recognition}} by {{Neural Networks}}},
  url = {http://www.peterasaro.org/gesture/temporal_nn.html},
  urldate = {2017-02-17},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5AU9TD3B\\temporal_nn.html}
}

@article{tenenbaumGlobalGeometricFramework2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, J. B.},
  date = {2000-12-22},
  journaltitle = {Science},
  volume = {290},
  pages = {2319--2323},
  issn = {00368075, 10959203},
  doi = {10.1126/science.290.5500.2319},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.290.5500.2319},
  urldate = {2020-06-01},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AKWPGFAL\\Tenenbaum - 2000 - A Global Geometric Framework for Nonlinear Dimensi.pdf},
  langid = {english},
  number = {5500}
}

@inproceedings{tervo3DRoomGeometry2012,
  title = {{{3D}} Room Geometry Estimation from Measured Impulse Responses},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tervo, S. and Tossavainen, T.},
  date = {2012-03},
  pages = {513--516},
  doi = {10.1109/ICASSP.2012.6287929},
  abstract = {Estimation of the room geometry from spatial room impulse responses is studied. An algorithm for estimating the geometry is presented. The algorithm does not require any a priori information on the room shape, number of walls, or order of the reflections, but deduces the set of planes that explain the measured source and image-source locations and covariances iteratively. The algorithm is demonstrated with real data experiments.},
  eventtitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\C85E2BXK\\Tervo and Tossavainen - 2012 - 3D room geometry estimation from measured impulse .pdf;C\:\\Users\\sauli\\Zotero\\storage\\FHVQAG3M\\6287929.html},
  keywords = {3D room geometry estimation,acoustic signal processing,Acoustics,Arrays,covariance,covariance analysis,Estimation,Geometry,image-source location,impulse response measurement,Loudspeakers,Microphones,reflection,room acoustic modeling,Room geometry estimation,room impulse response,Shape,spatial room impulse response,transient response}
}

@article{teutschAcousticSourceDetection2006,
  title = {Acoustic Source Detection and Localization Based on Wavefield Decomposition Using Circular Microphone Arrays},
  author = {Teutsch, Heinz and Kellermann, Walter},
  date = {2006-11},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {120},
  pages = {2724--2736},
  issn = {0001-4966},
  doi = {10.1121/1.2346089},
  url = {http://asa.scitation.org/doi/10.1121/1.2346089},
  urldate = {2018-06-12},
  langid = {english},
  number = {5}
}

@online{ThesisDviDownload,
  title = {Thesis.Dvi - Download},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.330.3764&rep=rep1&type=pdf},
  urldate = {2017-05-04},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CMXTCBDP\\download.html}
}

@article{thesisdvidownload,
  title = {Thesis.{{Dvi}} - {{Download}}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.330.3764&rep=rep1&type=pdf},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\WMJW7TI5\\download.html}
}

@inproceedings{thiergart_localization_2009,
  title = {Localization of Sound Sources in Reverberant Environments Based on Directional Audio Coding Parameters},
  booktitle = {Audio {{Engineering Society Convention}} 127},
  author = {Thiergart, Oliver and Schultz-Amling, Richard and Del Galdo, Giovanni and Mahne, Dirk and Kuech, Fabian},
  date = {2009},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?conv=127&papernum=7853},
  urldate = {2017-01-10},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\E6XDBXNH\\Localization of Sound Sources in Reverberant Environments Based on Directional Audio Coding Parameters.pdf}
}

@inproceedings{thiergartLocalizationSoundSources2009,
  title = {Localization of {{Sound Sources}} in {{Reverberant Environments Based}} on {{Directional Audio Coding Parameters}}},
  author = {Thiergart, Oliver and Schultz-Amling, Richard and Del Galdo, Giovanni and Mahne, Dirk and Kuech, Fabian},
  date = {2009-10-01},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15048},
  urldate = {2017-01-10},
  abstract = {Methods for spatial audio processing are becoming more important as the variety of multichannel audio applications is permanently increasing. Directional Audio Coding (DirAC) represents a well proven technique to capture and reproduce spatial sound on the basis of a downmix audio signal and parametric side information, namely direction of arrival and diffuseness of the sound. In addition to spatial audio reproduction, the DirAC parameters can be exploited further. In this paper, we propose a...},
  eventtitle = {Audio {{Engineering Society Convention}} 127},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ITAHCHIC\\Localization of Sound Sources in Reverberant Environments Based on Directional Audio Coding Parameters.pdf;C\:\\Users\\sauli\\Zotero\\storage\\G2F8Z8DB\\browse.html},
  langid = {english}
}

@report{Thump12Thump151000W2014,
  title = {Thump12, {{Thump15 1000W Powered Loudspeakers}}. {{Owner}}'s {{Manual}}},
  date = {2014},
  institution = {{Loud Technologies Inc.}},
  url = {https://mackie.com/sites/default/files/PRODUCT%20RESOURCES/MANUALS/Owners_Manuals/Thump12A_Thump15A_OM.pdf}
}

@online{TielinePOTSAudio,
  title = {Tieline {{POTS}} Audio Sounds as Good as {{ISDN}} Quality at {{64Kbps}} - {{Tieline Technology}}},
  url = {http://www.tieline.com/Announcements/User-Stories/English/Tieline-POTS-audio-sounds-as-good-as-ISDN-quality-at-64Kbps},
  urldate = {2017-06-13},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3GGCJHJ3\\Tieline-POTS-audio-sounds-as-good-as-ISDN-quality-at-64Kbps.html}
}

@article{tieteSoundCompassDistributedMEMS2014,
  title = {{{SoundCompass}}: {{A Distributed MEMS Microphone Array}}-{{Based Sensor}} for {{Sound Source Localization}}},
  shorttitle = {{{SoundCompass}}},
  author = {Tiete, Jelmer and Domínguez, Federico and da Silva, Bruno and Segers, Laurent and Steenhaut, Kris and Touhafi, Abdellah},
  date = {2014-01-23},
  journaltitle = {Sensors (Basel, Switzerland)},
  shortjournal = {Sensors (Basel)},
  volume = {14},
  pages = {1918--1949},
  issn = {1424-8220},
  doi = {10.3390/s140201918},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3958238/},
  urldate = {2020-03-12},
  abstract = {Sound source localization is a well-researched subject with applications ranging from localizing sniper fire in urban battlefields to cataloging wildlife in rural areas. One critical application is the localization of noise pollution sources in urban environments, due to an increasing body of evidence linking noise pollution to adverse effects on human health. Current noise mapping techniques often fail to accurately identify noise pollution sources, because they rely on the interpolation of a limited number of scattered sound sensors. Aiming to produce accurate noise pollution maps, we developed the SoundCompass, a low-cost sound sensor capable of measuring local noise levels and sound field directionality. Our first prototype is composed of a sensor array of 52 Microelectromechanical systems (MEMS) microphones, an inertial measuring unit and a low-power field-programmable gate array (FPGA). This article presents the SoundCompass’s hardware and firmware design together with a data fusion technique that exploits the sensing capabilities of the SoundCompass in a wireless sensor network to localize noise pollution sources. Live tests produced a sound source localization accuracy of a few centimeters in a 25-m2 anechoic chamber, while simulation results accurately located up to five broadband sound sources in a 10,000-m2 open field.},
  eprint = {24463431},
  eprinttype = {pmid},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SJN95ED9\\Tiete et al. - 2014 - SoundCompass A Distributed MEMS Microphone Array-.pdf},
  keywords = {*****,MUST READ},
  number = {2},
  options = {useprefix=true},
  pmcid = {PMC3958238}
}

@inproceedings{tiwariSlidingbandDynamicRange2014,
  title = {A Sliding-Band Dynamic Range Compression for Use in Hearing Aids},
  booktitle = {2014 {{Twentieth National Conference}} on {{Communications}} ({{NCC}})},
  author = {Tiwari, N. and Pandey, P. C.},
  date = {2014-02},
  pages = {1--6},
  doi = {10.1109/NCC.2014.6811300},
  abstract = {Sensorineural hearing loss is associated with elevated hearing thresholds, reduced dynamic range, and loudness recruitment. Dynamic range compression in the hearing aids is provided for restoring normal loudness of low level sounds without making the high level sounds uncomfortably loud. A sliding-band compression is presented for significantly reducing the temporal and spectral distortions generally associated with the currently used single and multiband compression techniques. It uses a frequency-dependent gain function calculated on the basis of critical bandwidth based short-time power spectrum and the specified hearing thresholds, compression ratios, and attack and release times. It is realized using FFT-based analysis-synthesis and can be integrated with other FFT-based signal processing in hearing aids to save computation. The technique is implemented and tested for satisfactory real-time operation, with sampling frequency of 10 kHz, window length of 25.6 ms with 75\% overlap on a 16-bit fixed-point DSP processor with on-chip FFT hardware.},
  eventtitle = {2014 {{Twentieth National Conference}} on {{Communications}} ({{NCC}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\4TWMN8SX\\Tiwari and Pandey - 2014 - A sliding-band dynamic range compression for use i.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ES7PTT2C\\figures.html},
  keywords = {auditory critical bandwidth,Auditory system,Digital signal processing,Dynamic range,dynamic range compression,elevated hearing threshold,fast Fourier transforms,FFT-based analysis-synthesis,FFT-based signal processing,fixed-point DSP processor,frequency 10 kHz,frequency-dependent gain function,Gain,hearing aid,hearing aids,high level sound,loudness recruitment,low level sound,medical signal processing,multiband compression technique,normal loudness,on-chip FFT hardware,real-time operation,real-time systems,reduced dynamic range,sampling frequency,sensorineural hearing loss,short-time power spectrum,signal sampling,single band compression technique,sliding-band dynamic range compression,spectral distortion reduction,Speech,storage capacity 16 bit,temporal distortion reduction,window length}
}

@article{tomarManifoldRegularizedDeep,
  title = {Manifold {{Regularized Deep Neural Networks}}},
  author = {Tomar, Vikrant Singh and Rose, Richard C},
  pages = {5},
  abstract = {Deep neural networks (DNNs) have been successfully applied to a variety of automatic speech recognition (ASR) tasks, both in discriminative feature extraction and hybrid acoustic modeling scenarios. The development of improved loss functions and regularization approaches have resulted in consistent reductions in ASR word error rates (WERs). This paper presents a manifold learning based regularization framework for DNN training. The associated techniques attempt to preserve the underlying low dimensional manifold based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters. This is achieved by imposing manifold based locality preserving constraints on the outputs of the network. The techniques are presented in the context of a bottleneck DNN architecture for feature extraction in a tandem configuration. The ASR WER obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using DNN-bottleneck networks trained without manifold constraints.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZX5JE8IA\\Tomar and Rose - Manifold Regularized Deep Neural Networks.pdf},
  langid = {english}
}

@inproceedings{tomarManifoldRegularizedDeep2014,
  title = {Manifold Regularized Deep Neural Networks},
  booktitle = {{{INTERSPEECH}}},
  author = {Tomar, Vikrant Singh and Rose, Richard C.},
  date = {2014},
  abstract = {Deep neural networks (DNNs) have been successfully applied to a variety of automatic speech recognition (ASR) tasks, both in discriminative feature extraction and hybrid acoustic modeling scenarios. The development of improved loss functions and regularization approaches have resulted in consistent reductions in ASR word error rates (WERs). This paper presents a manifold learning based regularization framework for DNN training. The associated techniques attempt to preserve the underlying low dimensional manifold based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters. This is achieved by imposing manifold based locality preserving constraints on the outputs of the network. The techniques are presented in the context of a bottleneck DNN architecture for feature extraction in a tandem configuration. The ASR WER obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using DNN-bottleneck networks trained without manifold constraints. Index Terms: manifold learning, deep neural networks, speech recognition, tandem feature extraction},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\VYR7KFKJ\\Tomar and Rose - 2014 - Manifold regularized deep neural networks.pdf},
  keywords = {Acoustic cryptanalysis,Acoustic model,Artificial neural network,Deep learning,Feature extraction,Locality of reference,Loss function,Mathematical optimization,Matrix regularization,Nonlinear dimensionality reduction,Speech recognition,Word error rate}
}

@article{torickAutomaticControlLoudness1968,
  title = {Automatic {{Control}} of {{Loudness Level}}},
  author = {Torick, E. L. and Allen, R. G. and Bauer, B. B.},
  date = {1968-12},
  journaltitle = {IEEE Transactions on Broadcasting},
  volume = {BC-14},
  pages = {143--146},
  issn = {0018-9316},
  doi = {10.1109/TBC.1968.265933},
  abstract = {A loudness limiter which reduces disparity in loudness levels, when a program is controlled on a vu basis, has been developed by CBS Laboratories. The device evaluates the loudness level of the program by a "loudness level summation" method described previously by the authors, and automatically reduces the program level when a preset threshold is exceeded.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EME5V5AS\\Torick et al. - 1968 - Automatic Control of Loudness Level.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EXEBUGUV\\4043731.html},
  keywords = {Automatic control,Broadcasting,Compressors,Computerized monitoring,Control systems,Electric variables control,FCC,Gain,signal processing,TV},
  number = {4}
}

@article{torresRobustAcousticSource2012,
  title = {Robust Acoustic Source Localization Based on Modal Beamforming and Time–Frequency Processing Using Circular Microphone Arrays},
  author = {Torres, Ana M. and Cobos, Maximo and Pueo, Basilio and Lopez, Jose J.},
  date = {2012-09-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {132},
  pages = {1511--1520},
  issn = {0001-4966},
  doi = {10.1121/1.4740503},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4740503},
  urldate = {2018-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\J63RYX8I\\1.html},
  number = {3}
}

@inproceedings{touzeneDisparityMapEstimation2010,
  title = {Disparity Map Estimation with Neural Network},
  booktitle = {2010 {{International Conference}} on {{Machine}} and {{Web Intelligence}}},
  author = {Touzene, N. B. and Larabi, S.},
  date = {2010-10},
  pages = {303--306},
  doi = {10.1109/ICMWI.2010.5648182},
  abstract = {This work aims at defining a new approach for a dense disparity map computing based on the neural networks from a pair of stereo images. Our approach has been divided into two main tasks. The first one deals with computing the initial disparity map using a neuronal method (BP). Whereas the second one presents a simple method to refine the initial disparity map using neural refinement so that an accurate result can be acquired. In the literature, the matching score is based only on the pixel intensities. We introduce in this work two additional features: the gradient magnitude and orientation of the gradient vector of pixels which gives a true degree of similarity between pixels. Experimental results on real data sets were conducted for evaluating the proposed method.},
  eventtitle = {2010 {{International Conference}} on {{Machine}} and {{Web Intelligence}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DWI8VE8P\\5648182.html},
  keywords = {Artificial neural networks,Computer architecture,Correlation,disparity,disparity map estimation,neural nets,neural network,Neural network,neuronal method,Neurons,Pixel,stereo image processing,stereo images,Stereo vision,stereovision,Training}
}

@inproceedings{touzeneDisparityMapEstimation2010a,
  title = {Disparity Map Estimation with Neural Network},
  booktitle = {2010 {{International Conference}} on {{Machine}} and {{Web Intelligence}}},
  author = {Touzene, N. B. and Larabi, S.},
  date = {2010-10},
  pages = {303--306},
  doi = {10.1109/ICMWI.2010.5648182},
  abstract = {This work aims at defining a new approach for a dense disparity map computing based on the neural networks from a pair of stereo images. Our approach has been divided into two main tasks. The first one deals with computing the initial disparity map using a neuronal method (BP). Whereas the second one presents a simple method to refine the initial disparity map using neural refinement so that an accurate result can be acquired. In the literature, the matching score is based only on the pixel intensities. We introduce in this work two additional features: the gradient magnitude and orientation of the gradient vector of pixels which gives a true degree of similarity between pixels. Experimental results on real data sets were conducted for evaluating the proposed method.},
  eventtitle = {2010 {{International Conference}} on {{Machine}} and {{Web Intelligence}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UKUEQDMR\\Touzene and Larabi - 2010 - Disparity map estimation with neural network.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZUIM3GYW\\5648182.html},
  keywords = {Artificial neural networks,Computer architecture,Correlation,disparity,disparity map estimation,neural nets,neural network,Neural network,neuronal method,Neurons,Pixel,stereo image processing,stereo images,Stereo vision,stereovision,Training}
}

@incollection{trahiotisInterauralCorrelationBasis2005,
  title = {Interaural {{Correlation}} as the {{Basis}} of a {{Working Model}} of {{Binaural Processing}}: {{An Introduction}}},
  shorttitle = {Interaural {{Correlation}} as the {{Basis}} of a {{Working Model}} of {{Binaural Processing}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Trahiotis, Constantine and Bernstein, Leslie R. and Stern, Richard M. and Buell, Thomas N.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {238--271},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_7},
  url = {http://link.springer.com/chapter/10.1007/0-387-28863-5_7},
  urldate = {2017-04-11},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BDSKMI7F\\0-387-28863-5_7.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@incollection{TrahiotisInterauralCorrelationBasis2005,
  title = {Interaural {{Correlation}} as the {{Basis}} of a {{Working Model}} of {{Binaural Processing}}: {{An Introduction}}},
  shorttitle = {Interaural {{Correlation}} as the {{Basis}} of a {{Working Model}} of {{Binaural Processing}}},
  booktitle = {Sound {{Source Localization}}},
  author = {Trahiotis, Constantine and Bernstein, Leslie R. and Stern, Richard M. and Buell, Thomas N.},
  editor = {Popper, Arthur N. and Fay, Richard R.},
  date = {2005},
  pages = {238--271},
  publisher = {{Springer New York}},
  doi = {10.1007/0-387-28863-5_7},
  copyright = {©2005 Springer Science+Business Media, Inc.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CGK3VUJF\\0-387-28863-5_7.html},
  isbn = {978-0-387-24185-2 978-0-387-28863-5},
  keywords = {Neurobiology,Neurosciences,Zoology},
  langid = {english},
  number = {25},
  series = {Springer {{Handbook}} of {{Auditory Research}}}
}

@inproceedings{traunmullerFrequencyRangeVoice1993,
  title = {The Frequency Range of the Voice Fundamental in the Speech of Male and Female Adults},
  author = {Traunmüller, Hartmut and Eriksson, Anders},
  date = {1993},
  abstract = {Published data on the frequency of the voice fundamental (F0) in speech show its range of variation, often expressed in terms of two standard deviations (SD) of the F0-distribution, to be approximately the same for men and women if expressed in semitones, but the observed SD varies substantially between different investigations. Most of the differences can be attributed to the following factors: SD is increased in tone languages and it varies with the type of discourse. The more ‘lively’ the type of discourse, the larger it is. The dependence of SD on the type of discourse tends to be mom pronounced in the speech of women than of men. Based on an analysis of various production data A is shown that speakers normally achieve an increased SD by increasing the excursions of F0 from a ‘base-value’ that lies about 1.5 SD below their mean F0. This is relevant to applications in speech technology as well as to general theories of speech communication such as the ‘modulation theory’ in which the base-value of F0 is seen as a carrier frequency.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\D94DX3WL\\Traunmüller_Eriksson_1993_The frequency range of the voice fundamental in the speech of male and female.pdf},
  keywords = {Carrier frequency,Frequency band,Languages,Large,Modulation,Speech technology,Technical standard}
}

@article{trinhSelfieSelfsupervisedPretraining2019,
  title = {Selfie: {{Self}}-Supervised {{Pretraining}} for {{Image Embedding}}},
  shorttitle = {Selfie},
  author = {Trinh, Trieu H. and Luong, Minh-Thang and Le, Quoc V.},
  date = {2019-07-27},
  url = {http://arxiv.org/abs/1906.02940},
  urldate = {2019-10-28},
  abstract = {We introduce a pretraining technique called Selfie, which stands for SELFsupervised Image Embedding. Selfie generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other “distractor” patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selfie includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate Selfie on three benchmarks (CIFAR-10, ImageNet 32 × 32, and ImageNet 224 × 224) with varying amounts of labeled data, from 5\% to 100\% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 × 224 with 60 examples per class (5\%), our method improves the mean accuracy of ResNet-50 from 35.6\% to 46.7\%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across different runs.},
  archivePrefix = {arXiv},
  eprint = {1906.02940},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\RI7Y5RAQ\\Trinh et al. - 2019 - Selfie Self-supervised Pretraining for Image Embe.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, eess, stat}
}

@inproceedings{tsingosModelingAcousticsVirtual2001,
  title = {Modeling {{Acoustics}} in {{Virtual Environments Using}} the {{Uniform Theory}} of {{Diffraction}}},
  booktitle = {Proceedings of the 28th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  author = {Tsingos, Nicolas and Funkhouser, Thomas and Ngan, Addy and Carlbom, Ingrid},
  date = {2001},
  pages = {545--552},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/383259.383323},
  url = {http://doi.acm.org/10.1145/383259.383323},
  abstract = {Realistic modeling of reverberant sound in 3D virtual worlds provides users with important cues for localizing sound sources and understanding spatial properties of the environment. Unfortunately, current geometric acoustic modeling systems do not accurately simulate reverberant sound. Instead, they model only direct transmission and specular reflection, while diffraction is either ignored or modeled through statistical approximation. However, diffraction is important for correct interpretation of acoustic environments, especially when the direct path between sound source and receiver is occluded.
The Uniform Theory of Diffraction (UTD) extends geometrical acoustics with diffraction phenomena: illuminated edges become secondary sources of diffracted rays that in turn may propagate through the environment. In this paper, we propose an efficient way for computing the acoustical effect of diffraction paths using the UTD for deriving secondary diffracted rays and associated diffraction coefficients. Our main contributions are: 1) a beam tracing method for enumerating sequences of diffracting edges efficiently and without aliasing in densely occluded polyhedral environments; 2) a practical approximation to the simulated sound field in which diffraction is considered only in shadow regions; and 3) a real-time auralization system demonstrating that diffraction dramatically improves the quality of spatialized sound in virtual environments.},
  isbn = {978-1-58113-374-5},
  series = {{{SIGGRAPH}} '01}
}

@inproceedings{tsingosModelingAcousticsVirtual2001a,
  title = {Modeling {{Acoustics}} in {{Virtual Environments Using}} the {{Uniform Theory}} of {{Diffraction}}},
  booktitle = {Proceedings of the 28th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  author = {Tsingos, Nicolas and Funkhouser, Thomas and Ngan, Addy and Carlbom, Ingrid},
  date = {2001},
  pages = {545--552},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/383259.383323},
  url = {http://doi.acm.org/10.1145/383259.383323},
  abstract = {Realistic modeling of reverberant sound in 3D virtual worlds provides users with important cues for localizing sound sources and understanding spatial properties of the environment. Unfortunately, current geometric acoustic modeling systems do not accurately simulate reverberant sound. Instead, they model only direct transmission and specular reflection, while diffraction is either ignored or modeled through statistical approximation. However, diffraction is important for correct interpretation of acoustic environments, especially when the direct path between sound source and receiver is occluded.
The Uniform Theory of Diffraction (UTD) extends geometrical acoustics with diffraction phenomena: illuminated edges become secondary sources of diffracted rays that in turn may propagate through the environment. In this paper, we propose an efficient way for computing the acoustical effect of diffraction paths using the UTD for deriving secondary diffracted rays and associated diffraction coefficients. Our main contributions are: 1) a beam tracing method for enumerating sequences of diffracting edges efficiently and without aliasing in densely occluded polyhedral environments; 2) a practical approximation to the simulated sound field in which diffraction is considered only in shadow regions; and 3) a real-time auralization system demonstrating that diffraction dramatically improves the quality of spatialized sound in virtual environments.},
  isbn = {978-1-58113-374-5},
  series = {{{SIGGRAPH}} '01}
}

@online{Uncalibrated3DRoom,
  title = {Uncalibrated {{3D}} Room Geometry Estimation from Sound Impulse Responses | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.jfranklin.2017.10.024},
  url = {https://reader.elsevier.com/reader/sd/pii/S0016003217305458?token=538F120186F422633D19925878F5035E9852D2A2A1C8E06ADC54B99B87D73288A173629108D075DE43969312A27B3D02},
  urldate = {2020-02-21},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3YSHRTBY\\S0016003217305458.html},
  langid = {english}
}

@article{uncini_audio_2003,
  title = {Audio {{Signal Processing}} by {{Neural Networks}}},
  author = {Uncini, Aurelio},
  date = {2003-10},
  journaltitle = {Neurocomputing},
  volume = {55},
  pages = {593--625},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(03)00395-3},
  abstract = {In this paper a review of architectures suitable for nonlinear real-time audio signal processing is presented. The computational and structural complexity of neural networks (NNs) represent in fact, the main drawbacks that can hinder many practical NNs multimedia applications. In particular efficient neural architectures and their learning algorithm for real-time on-line audio processing are discussed. Moreover, applications in the fields of (1) audio signal recovery, (2) speech quality enhancement, (3) nonlinear transducer linearization, (4) learning based pseudo-physical sound synthesis, are briefly presented and discussed.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W3YAUS3U\\S0925231203003953.html},
  keywords = {Neural networks for signal processing,Nonlinear audio signal processing,Physical model sound synthesis,Signal predistortion,Signal recovery,Speech enhancement,Spline neural networks,Subband adaptive nonlinear filters},
  number = {3–4},
  series = {Evolving {{Solution}} with {{Neural Networks}}}
}

@article{uncini_audio_2003,
  title = {Audio Signal Processing by Neural Networks},
  author = {Uncini, Aurelio},
  date = {2003-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {55},
  pages = {593--625},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(03)00395-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231203003953},
  urldate = {2017-04-03},
  abstract = {In this paper a review of architectures suitable for nonlinear real-time audio signal processing is presented. The computational and structural complexity of neural networks (NNs) represent in fact, the main drawbacks that can hinder many practical NNs multimedia applications. In particular efficient neural architectures and their learning algorithm for real-time on-line audio processing are discussed. Moreover, applications in the fields of (1) audio signal recovery, (2) speech quality enhancement, (3) nonlinear transducer linearization, (4) learning based pseudo-physical sound synthesis, are briefly presented and discussed.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3GV4GZ8T\\S0925231203003953.html},
  keywords = {Neural networks for signal processing,Nonlinear audio signal processing,Physical model sound synthesis,Signal predistortion,Signal recovery,Speech enhancement,Spline neural networks,Subband adaptive nonlinear filters},
  number = {3–4},
  series = {Evolving {{Solution}} with {{Neural Networks}}}
}

@online{UnderstandingCategoricalCrossEntropy,
  title = {Understanding {{Categorical Cross}}-{{Entropy Loss}}, {{Binary Cross}}-{{Entropy Loss}}, {{Softmax Loss}}, {{Logistic Loss}}, {{Focal Loss}} and All Those Confusing Names},
  url = {https://gombru.github.io/2018/05/23/cross_entropy_loss/},
  urldate = {2019-11-15},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N9URVVK5\\cross_entropy_loss.html},
  keywords = {MUST READ;}
}

@online{UnderstandingLSTMNetworks,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2017-11-28}
}

@online{UnreasonableEffectivenessRecurrent,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
  urldate = {2017-11-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\V9FWZGAE\\rnn-effectiveness.html}
}

@article{urazghildiievPassiveAcousticDetection2018,
  title = {Passive Acoustic Detection and Estimation of the Number of Sources Using Compact Arrays},
  author = {Urazghildiiev, Ildar R. and Hannay, David E.},
  date = {2018-05},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{2825-2833\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5037361\}},
  abstract = {The problem of estimating the number of sound-producing sources detected using a compact array of hydrophones is addressed. Closed form expressions representing the techniques of automatic detection and estimation of the number of callers are given. Their performance is evaluated on a year-long dataset (1 October 2015-6 October 2016) containing humpback whale and killer whale calls collected in the Strait of Georgia, near Vancouver, British Columbia. Manual verification of the automatic detections produced by the approach required similar to 40 h. (C) 2018 Acoustical Society of America.},
  affiliation = {Urazghildiiev, IR (Reprint Author), JASCO Appl Sci USA Inc, Silver Spring, MD 20910 USA. Urazghildiiev, Ildar R., JASCO Appl Sci USA Inc, Silver Spring, MD 20910 USA. Hannay, David E., JASCO Appl Sci Canada Ltd, Victoria, BC, Canada.},
  author-email = {ildar.urazghildiiev@jasco.com},
  cited-references = {Adam O., 2013, DETECTION CLASSIFICA. [Anonymous], 2016, U. S. patent application, Patent No. [2016/0252635, 20160252635]. Arnold TB, 2011, R J, V3, P34. Bortolotto GA, 2016, ZOOLOGIA-CURITIBA, V33, DOI 10.1590/S1984-4689zool-20150133. Brown JC, 2006, J ACOUST SOC AM, V119, pEL34, DOI 10.1121/1.2166949. Buckland S. T., 2001, INTRO DISTANCE SAMPL, P77. Gassmann M, 2015, J ACOUST SOC AM, V138, P2483, DOI 10.1121/1.4927417. Gassmann M, 2013, J ACOUST SOC AM, V134, P3513, DOI 10.1121/1.4824162. Hero A., 1999, DIGITAL SIGNAL PROCE. Hillis C., 2014, OCEANS 2014. Hirotsu R, 2010, J ACOUST SOC AM, V127, P133, DOI 10.1121/1.3268593. Kusel ET, 2011, J ACOUST SOC AM, V129, P3610, DOI 10.1121/1.3583504. Lehman E. L., 1986, TESTING STAT HYPOTHE. Maronna R., 2006, ROBUST STAT THEORY M. Marques TA, 2013, BIOL REV, V88, P287, DOI 10.1111/brv.12001. Marques TA, 2009, J ACOUST SOC AM, V125, P1982, DOI 10.1121/1.3089590. Mathias D, 2013, J ACOUST SOC AM, V134, P2446, DOI 10.1121/1.4816565. Payne K., 1983, COMMUNICATION BEHAV, P9. Shapiro AD, 2009, J ACOUST SOC AM, V126, P451, DOI 10.1121/1.3132525. Shiryayev A. N., 1992, MATH ITS APPL SOVIET, V26. Urazghildiiev IR, 2017, J ACOUST SOC AM, V141, P2548, DOI 10.1121/1.4979792. Urazghildiiev IR, 2014, J ACOUST SOC AM, V136, P2851, DOI 10.1121/1.4898048. Urazghildiiev IR, 2013, J ACOUST SOC AM, V134, P4418, DOI 10.1121/1.4824683. Urazghildiiev IR, 2009, IEEE J OCEANIC ENG, V34, P358, DOI 10.1109/JOE.2009.2014931. Van Trees H. L., 2001, DETECTION ESTIMATI 1. Ward JA, 2012, MAR MAMMAL SCI, V28, pE444, DOI 10.1111/j.1748-7692.2011.00560.x. Wiggins SM, 2013, J ACOUST SOC AM, V133, P3813, DOI 10.1121/1.4802645. Wiggins SM, 2012, J ACOUST SOC AM, V131, P156, DOI 10.1121/1.3662076. Zimmer W. M. X., PASSIVE ACOUSTIC MON. Zimmer WMX, 2008, J ACOUST SOC AM, V124, P2823, DOI 10.1121/1.2988277.},
  da = {2018-10-18},
  doc-delivery-number = {GG9WB},
  eissn = {1520-8524},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {BEAKED-WHALES; SPERM-WHALES; DENSITY-ESTIMATION; HYDROPHONE ARRAYS; TRACKING; VOCALIZATIONS; LOCALIZATION; RECORDER; SENSORS},
  langid = {english},
  number = {5},
  number-of-cited-references = {30},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000433050700045},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@online{US20060227211A1MethodApparatus,
  title = {{{US20060227211A1}} - {{Method}} and Apparatus for Measuring Position and Orientation - {{Google Patents}}},
  url = {https://patents.google.com/patent/US20060227211A1/en?oq=US8698875B2},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\M6S7ZXB2\\en.html}
}

@online{US20120306999A1MotionBasedImage,
  title = {{{US20120306999A1}} - {{Motion}}-{{Based Image Stitching}} - {{Google Patents}}},
  url = {https://patents.google.com/patent/US20120306999},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9863XMJV\\US20120306999.html}
}

@online{US20180225877A1MobileAugmented,
  title = {{{US20180225877A1}} - {{Mobile}} Augmented Reality System - {{Google Patents}}},
  url = {https://patents.google.com/patent/US20180225877A1/en?oq=US8698875B2},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3YR76ILE\\en.html}
}

@online{US20180225877A1MobileAugmenteda,
  title = {{{US20180225877A1}} - {{Mobile}} Augmented Reality System - {{Google Patents}}},
  url = {https://patents.google.com/patent/US20180225877A1/en?oq=US8698875B2},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SJPXCZ83\\en.html}
}

@online{US7187809B2MethodApparatus,
  title = {{{US7187809B2}} - {{Method}} and Apparatus for Aligning Video to Three-Dimensional Point Clouds - {{Google Patents}}},
  url = {https://patents.google.com/patent/US7187809B2/en?q=pointcloud+video+matching&oq=pointcloud+video+matching},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\U6JTUYJS\\en.html}
}

@online{US9041915B2SystemsMethods,
  title = {{{US9041915B2}} - {{Systems}} and Methods of Scene and Action Capture Using Imaging System Incorporating {{3D LIDAR}} - {{Google Patents}}},
  url = {https://patents.google.com/patent/US9041915B2/en?q=pointcloud+video+matching+imu+gps&oq=pointcloud+video+matching+imu+gps+},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\BMWYPPU7\\en.html}
}

@online{US9476730B2RealtimeSystem,
  title = {{{US9476730B2}} - {{Real}}-Time System for Multi-Modal {{3D}} Geospatial Mapping, Object Recognition, Scene Annotation and Analytics - {{Google Patents}}},
  url = {https://patents.google.com/patent/US9476730B2/en?q=pointcloud+video+matching+imu+gps&oq=pointcloud+video+matching+imu+gps+},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\J765Y3L5\\en.html}
}

@online{US9476970B1CameraBased,
  title = {{{US9476970B1}} - {{Camera}} Based Localization - {{Google Patents}}},
  url = {https://patents.google.com/patent/US9476970B1/en?oq=US8698875B2},
  urldate = {2020-02-28},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KS9X3QEU\\en.html}
}

@inproceedings{utamiUtilizingVirtualReality2014,
  title = {Utilizing {{Virtual Reality}} for {{Simulating}} the {{Auditory Perception In Architectural Designed Spaces}}},
  booktitle = {{{ARCC Conference Repository}}},
  author = {Utami, Sentagi S. and Navvab, Mojtaba},
  date = {2014},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\53KAURAE\\Utami and Navvab - 2014 - Utilizing Virtual Reality for Simulating the Audit.pdf}
}

@article{valinLocalizationSimultaneousMoving2004,
  title = {Localization of {{Simultaneous Moving Sound Sources}} for {{Mobile Robot Using}} a {{Frequency}}-{{Domain Steered Beamformer Approach}}},
  author = {Valin, Jean-Marc and Michaud, François and Hadjou, Brahim and Rouat, Jean},
  date = {2004},
  journaltitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
  pages = {1033-1038 Vol.1},
  doi = {10.1109/ROBOT.2004.1307286},
  url = {http://arxiv.org/abs/1602.08629},
  urldate = {2020-04-11},
  abstract = {Mobile robots in real-life settings would benefit from being able to localize sound sources. Such a capability can nicely complement vision to help localize a person or an interesting event in the environment, and also to provide enhanced processing for other capabilities such as speech recognition. In this paper we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on a frequency-domain implementation of a steered beamformer along with a probabilistic post-processor. Results show that a mobile robot can localize in real time multiple moving sources of different types over a range of 5 meters with a response time of 200 ms.},
  archivePrefix = {arXiv},
  eprint = {1602.08629},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PNBMP5NC\\Valin et al. - 2004 - Localization of Simultaneous Moving Sound Sources .pdf;C\:\\Users\\sauli\\Zotero\\storage\\63UII9U4\\1602.html},
  keywords = {Computer Science - Robotics,Computer Science - Sound}
}

@inproceedings{valinRobust3DLocalization2006,
  title = {Robust {{3D Localization}} and {{Tracking}} of {{Sound Sources Using Beamforming}} and {{Particle Filtering}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Acoustics Speech}} and {{Signal Processing Proceedings}}},
  author = {Valin, J.-M. and Michaud, F. and Rouat, J.},
  date = {2006-05},
  volume = {4},
  pages = {IV-IV},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2006.1661100},
  abstract = {In this paper we present a new robust sound source localization and tracking method using an array of eight microphones (US patent pending). The method uses a steered beamformer based on the reliability-weighted phase transform (RWPHAT) along with a particle filter-based tracking algorithm. The proposed system is able to estimate both the direction and the distance of the sources. In a videoconferencing context, the direction was estimated with an accuracy better than one degree while the distance was accurate within 10\% RMS. Tracking of up to three simultaneous moving speakers is demonstrated in a noisy environment},
  eventtitle = {2006 {{IEEE International Conference}} on {{Acoustics Speech}} and {{Signal Processing Proceedings}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PL47CV7Z\\Valin et al. - 2006 - Robust 3D Localization and Tracking of Sound Sourc.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZYT69CZJ\\1661100.html},
  keywords = {3D sound source localization,3D sound source tracking,Array signal processing,audio signal processing,beamforming,Cameras,Delay,Filtering,Frequency domain analysis,microphone array,microphone arrays,Microphone arrays,particle filtering,particle filtering (numerical methods),Particle filters,Particle tracking,reliability-weighted phase transform,Robustness,steered beamformer,teleconferencing,Teleconferencing,transforms,videoconferencing}
}

@article{valinRobustLocalizationTracking2007,
  title = {Robust {{Localization}} and {{Tracking}} of {{Simultaneous Moving Sound Sources Using Beamforming}} and {{Particle Filtering}}},
  author = {Valin, Jean-Marc and Michaud, François and Rouat, Jean},
  date = {2007-03},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {55},
  pages = {216--228},
  issn = {09218890},
  doi = {10.1016/j.robot.2006.08.004},
  url = {http://arxiv.org/abs/1602.08139},
  urldate = {2019-08-16},
  abstract = {Mobile robots in real-life settings would benefit from being able to localize and track sound sources. Such a capability can help localizing a person or an interesting event in the environment, and also provides enhanced processing for other capabilities such as speech recognition. To give this capability to a robot, the challenge is not only to localize simultaneous sound sources, but to track them over time. In this paper we propose a robust sound source localization and tracking method using an array of eight microphones. The method is based on a frequency-domain implementation of a steered beamformer along with a particle filter-based tracking algorithm. Results show that a mobile robot can localize and track in real-time multiple moving sources of different types over a range of 7 meters. These new capabilities allow a mobile robot to interact using more natural means with people in real life settings.},
  archivePrefix = {arXiv},
  eprint = {1602.08139},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\25BXRDBZ\\Valin et al_2007_Robust Localization and Tracking of Simultaneous Moving Sound Sources Using.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NYK5WEAG\\1602.html},
  keywords = {Computer Science - Robotics,Computer Science - Sound},
  number = {3}
}

@article{valinRobustSoundSource2003,
  title = {Robust {{Sound Source Localization Using}} a {{Microphone Array}} on a {{Mobile Robot}}},
  author = {Valin, Jean-Marc and Michaud, François and Rouat, Jean and Létourneau, Dominic},
  date = {2003},
  volume = {2},
  pages = {1228--1233},
  doi = {10.1109/IROS.2003.1248813},
  url = {http://arxiv.org/abs/1602.08213},
  urldate = {2017-04-11},
  abstract = {The hearing sense on a mobile robot is important because it is omnidirectional and it does not require direct line-of-sight with the sound source. Such capabilities can nicely complement vision to help localize a person or an interesting event in the environment. To do so the robot auditory system must be able to work in noisy, unknown and diverse environmental conditions. In this paper we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on time delay of arrival estimation. Results show that a mobile robot can localize in real time different types of sound sources over a range of 3 meters and with a precision of 3 degrees.},
  archivePrefix = {arXiv},
  eprint = {1602.08213},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N6XRMHU2\\Valin et al. - 2003 - Robust Sound Source Localization Using a Microphon.pdf;C\:\\Users\\sauli\\Zotero\\storage\\SKRSKVWW\\1602.html},
  keywords = {Computer Science - Robotics,Computer Science - Sound},
  primaryClass = {cs}
}

@article{ValinRobustSoundSource2003,
  title = {Robust {{Sound Source Localization Using}} a {{Microphone Array}} on a {{Mobile Robot}}},
  author = {Valin, Jean-Marc and Michaud, François and Rouat, Jean and Létourneau, Dominic},
  date = {2003},
  journaltitle = {arXiv:1602.08213 [cs]},
  volume = {2},
  pages = {1228--1233},
  doi = {10.1109/IROS.2003.1248813},
  abstract = {The hearing sense on a mobile robot is important because it is omnidirectional and it does not require direct line-of-sight with the sound source. Such capabilities can nicely complement vision to help localize a person or an interesting event in the environment. To do so the robot auditory system must be able to work in noisy, unknown and diverse environmental conditions. In this paper we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on time delay of arrival estimation. Results show that a mobile robot can localize in real time different types of sound sources over a range of 3 meters and with a precision of 3 degrees.},
  archivePrefix = {arXiv},
  eprint = {1602.08213},
  eprintclass = {cs},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\X4WBQGJP\\Valin et al. - 2003 - Robust Sound Source Localization Using a Microphon.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EFYC5WFR\\1602.html},
  keywords = {Computer Science - Robotics,Computer Science - Sound}
}

@inproceedings{valinRobustSoundSource2003a,
  title = {Robust Sound Source Localization Using a Microphone Array on a Mobile Robot},
  booktitle = {Proceedings 2003 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2003) ({{Cat}}. {{No}}.{{03CH37453}})},
  author = {Valin, J. M. and Michaud, F. and Rouat, J. and Letourneau, D.},
  date = {2003-10},
  volume = {2},
  pages = {1228-1233 vol.2},
  doi = {10.1109/IROS.2003.1248813},
  abstract = {The hearing sense on a mobile robot is important because it is omnidirectional and it does not require direct line-of-sight with the sound source. Such capabilities can nicely complement vision to help localize a person or an interesting event in the environment. To do so the robot auditory system must be able to work in noisy, unknown and diverse environmental conditions. In this paper, we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on time delay of arrival estimation. Results show that a mobile robot can localize in real time different types of sound sources over a range of 3 meters and with a precision of 3°.},
  eventtitle = {Proceedings 2003 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2003) ({{Cat}}. {{No}}.{{03CH37453}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EQJPCC6M\\Valin et al. - 2003 - Robust sound source localization using a microphon.pdf;C\:\\Users\\sauli\\Zotero\\storage\\EP7ASKXE\\1248813.html},
  keywords = {Acoustic noise,acoustic signal processing,array signal processing,Auditory system,Delay effects,Delay estimation,direct line-of-sight,direction-of-arrival estimation,microphone array,Microphone arrays,microphones,mobile robot,mobile robots,Mobile robots,noise,Orbital robotics,robot auditory system,Robot sensing systems,robust sound source localization,robust sound source localization method,Robustness,three-dimensional space,Working environment noise}
}

@inproceedings{valinRobustSoundSource2003b,
  title = {Robust Sound Source Localization Using a Microphone Array on a Mobile Robot},
  booktitle = {Proceedings 2003 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2003) ({{Cat}}. {{No}}.{{03CH37453}})},
  author = {Valin, J.-M. and Michaud, F. and Rouat, J. and Letourneau, D.},
  date = {2003},
  volume = {2},
  pages = {1228--1233},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/IROS.2003.1248813},
  url = {http://ieeexplore.ieee.org/document/1248813/},
  urldate = {2019-08-16},
  abstract = {The hearing sense on a mobile robot is important because it is omnidirectional and it does not require direct line-of-sight with the sound source. Such capabilities can nicely complement vision to help localize a person or an interesting event in the environment. To do so the robot auditory system must be able to work in noisy, unknown and diverse environmental conditions. In this paper we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on time delay of arrival estimation. Results show that a mobile robot can localize in real time different types of sound sources over a range of 3 meters and with a precision of 3◦.},
  eventtitle = {2003 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KKG2BDPB\\Valin et al. - 2003 - Robust sound source localization using a microphon.pdf},
  isbn = {978-0-7803-7860-5},
  langid = {english}
}

@article{velascoSourceLocalizationAcoustic2012,
  title = {Source {{Localization}} with {{Acoustic Sensor Arrays Using Generative Model Based Fitting}} with {{Sparse Constraints}}},
  author = {Velasco, Jose and Pizarro, Daniel and Macias-Guarasa, Javier},
  date = {2012-10},
  journaltitle = {Sensors},
  volume = {12},
  pages = {13781--13812},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/s121013781},
  url = {https://www.mdpi.com/1424-8220/12/10/13781},
  urldate = {2020-05-05},
  abstract = {This paper presents a novel approach for indoor acoustic source localization using sensor arrays. The proposed solution starts by defining a generative model, designed to explain the acoustic power maps obtained by Steered Response Power (SRP) strategies. An optimization approach is then proposed to fit the model to real input SRP data and estimate the position of the acoustic source. Adequately fitting the model to real SRP data, where noise and other unmodelled effects distort the ideal signal, is the core contribution of the paper. Two basic strategies in the optimization are proposed. First, sparse constraints in the parameters of the model are included, enforcing the number of simultaneous active sources to be limited. Second, subspace analysis is used to filter out portions of the input signal that cannot be explained by the model. Experimental results on a realistic speech database show statistically significant localization error reductions of up to 30\% when compared with the SRP-PHAT strategies.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CAKJRACP\\Velasco et al. - 2012 - Source Localization with Acoustic Sensor Arrays Us.pdf;C\:\\Users\\sauli\\Zotero\\storage\\J8F24Q8C\\html.html},
  issue = {10},
  keywords = {acoustic localization,microphone array sensors,optimization techniques,sparse modeling},
  langid = {english},
  number = {10}
}

@article{venkatesanDeepRecurrentNeural2018,
  title = {Deep Recurrent Neural Networks Based Binaural Speech Segregation for the Selection of Closest Target of Interest},
  author = {Venkatesan, R. and Ganesh, A. Balaji},
  date = {2018-08},
  journaltitle = {MULTIMEDIA TOOLS AND APPLICATIONS},
  volume = {77},
  pages = {\{20129-20156\}},
  publisher = {SPRINGER},
  location = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  issn = {1380-7501},
  doi = {\{10.1007/s11042-017-5458-3\}},
  abstract = {An auditory attention model that consists of binaural source segregation and also full localization of a target speech signal in a multi-talker environment is presented. The joint acoustic features, such as monaural, binaural and direct to reverberant ratio (DRR) that are successfully incorporated into deep recurrent neural network (DRNN) based joint discriminative model for the speech source segregation process. The monaural and binaural features are extracted from binaural speech mixtures of two speakers by using mean Hilbert envelope coefficients (MHEC) and interaural time, and level differences, respectively. The performance of deep recurrent network based speech segregation is validated in terms of signal to interference, signal to distortion and signal to artifacts and compared with existing architectures, including deep neural network (DNN). The proposed system is observed and found to be more suitable than monaural speech segregation especially when the desired target and interfering sources are located at different positions. The study also proposes full localization of segregated speech source that created the possibility to select the desired speaker of interest from an input acoustic speech mixture in a reverberant environment. The developed system has the capability to handle binaural segregation problem in multi-source and reverberation conditions. The auditory attention model provides accurate information about speech sources even when the desired targets are located at 2 m and above with higher reverberation time.},
  affiliation = {Ganesh, AB (Reprint Author), Velammal Engn Coll, Dept Elect & Elect Engn, Elect Syst Design Lab, Madras 600066, Tamil Nadu, India. Venkatesan, R.; Ganesh, A. Balaji, Velammal Engn Coll, Dept Elect & Elect Engn, Elect Syst Design Lab, Madras 600066, Tamil Nadu, India.},
  author-email = {abganesh@velammal.edu.in},
  cited-references = {Asano F, 2013, IEEE T AUDIO SPEECH, V21, P1953, DOI 10.1109/TASL.2013.2263140. Cambell DR, 2005, COMPUT INF SYST, V9, P48. Chen JT, 2014, IEEE-ACM T AUDIO SPE, V22, P1993, DOI 10.1109/TASLP.2014.2359159. Ellis D, 2005, PLP RASTA MFCC INVER. Garofolo J., 1993, TIMIT ACOUSTIC PHONE. Geng Y, 2016, ARXIV160908417. Georganti E, 2011, IEEE T AUDIO SPEECH, V19, P1949, DOI 10.1109/TASL.2011.2104953. Han K, 2012, J ACOUST SOC AM, V132, P3475, DOI 10.1121/1.4754541. Hioka Y, 2010, INT CONF ACOUST SPEE, P149, DOI 10.1109/ICASSP.2010.5496103. Hu K, 2011, IEEE T AUDIO SPEECH, V19, P1600, DOI 10.1109/TASL.2010.2093893. Hu Y, 2008, IEEE T AUDIO SPEECH, V16, P229, DOI 10.1109/TASL.2007.911054. Hu Y, 2007, SPEECH COMMUN, V49, P588, DOI 10.1016/j.specom.2006.12.006. Huang PS, 2015, IEEE-ACM T AUDIO SPE, V23, P2136, DOI 10.1109/TASLP.2015.2468583. Hummersone C, 2011, IEEE T AUDIO SPEECH, V19, P2039, DOI 10.1109/TASL.2011.2109380. Hummersone C, 2010, IEEE T AUDIO SPEECH, V18, P1867, DOI 10.1109/TASL.2010.2051354. Jeub M, 2009, P INT C DIG SIGN PRO, P1. Jiang Y, 2014, IEEE-ACM T AUDIO SPE, V22, P2112, DOI 10.1109/TASLP.2014.2361023. Jin ZZ, 2009, IEEE T AUDIO SPEECH, V17, P625, DOI 10.1109/TASL.2008.2010633. Kim G, 2009, J ACOUST SOC AM, V126, P1486, DOI 10.1121/1.3184603. Kohlrausch A, 2013, TECHNOLOGY BINAURAL. Kuster M, 2011, J ACOUST SOC AM, V130, P3781, DOI 10.1121/1.3658446. Leglaive S, 2015, INT CONF ACOUST SPEE, P121, DOI 10.1109/ICASSP.2015.7177944. Liang RZ, 2016, INT C PATT RECOG, P2954, DOI 10.1109/ICPR.2016.7900086. Lu YC, 2011, SPEECH COMMUN, V53, P622, DOI 10.1016/j.specom.2010.06.001. Lu YC, 2010, IEEE T AUDIO SPEECH, V18, P1793, DOI 10.1109/TASL.2010.2050687. Lu YC, 2008, P INT WORKSH AC ECH, P1793. Ma JF, 2009, J ACOUST SOC AM, V125, P3387, DOI 10.1121/1.3097493. Maas AL, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P22. Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P1872, DOI 10.1109/TASL.2010.2052252. May T, 2012, IEEE T AUDIO SPEECH, V20, P2016, DOI 10.1109/TASL.2012.2193391. Narayanan A, 2013, INT CONF ACOUST SPEE, P7092, DOI 10.1109/ICASSP.2013.6639038. Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1562, DOI 10.1109/ICASSP.2014.6853860. Qinfeng Li, 2018, Neural Computing and Applications, V30, P463, DOI 10.1007/s00521-016-2680-2. Raspaud M, 2010, IEEE T AUDIO SPEECH, V18, P68, DOI 10.1109/TASL.2009.2023644. Sadjadi SO, 2015, SPEECH COMMUN, V72, P138, DOI 10.1016/j.specom.2015.04.005. Schadler MR, 2015, J ACOUST SOC AM, V137, P2047, DOI 10.1121/1.4916618. Schadler MR, 2012, J ACOUST SOC AM, V131, P4134, DOI 10.1121/1.3699200. Shao Y, 2008, INT CONF ACOUST SPEE, P1589. Spille C, 2013, INT CONF ACOUST SPEE, P7805, DOI 10.1109/ICASSP.2013.6639183. Vesa S, 2007, P IEEE WORKSH APPL S, P271. Vesa S, 2009, IEEE T AUDIO SPEECH, V17, P1498, DOI 10.1109/TASL.2009.2022001. Weninger Felix, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3709, DOI 10.1109/ICASSP.2014.6854294. Woodruff J, 2013, IEEE T AUDIO SPEECH, V21, DOI 10.1109/TASL.2012.2236316. Woodruff J, 2012, IEEE T AUDIO SPEECH, V20, P1503, DOI 10.1109/TASL.2012.2183869. Woodruff J, 2010, IEEE T AUDIO SPEECH, V18, P1856, DOI 10.1109/TASL.2010.2050087. Wrigley SN, 2008, LECT NOTES COMPUT SC, V4892, P271. Yu Y, 2016, J AUDIO SPEECH MUSIC, DOI [10.1186/s13636-016-0085-x, DOI 10.1186/S13636-016-0085-X]. Yu Y, 2016, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-016-0085-x.},
  da = {2018-10-18},
  doc-delivery-number = {GP2YI},
  eissn = {1573-7721},
  funding-acknowledgement = {Department of Science and Technology (DST) [SR/CSI/09/2011]},
  funding-text = {The authors wish to thank Department of Science and Technology for awarding a project under Cognitive Science Initiative Programme (DST File No.: SR/CSI/09/2011) through which the work has been implemented. The authors would like to thank anonymous reviewers for their valuable and constructive suggestions that improved the quality of the manuscript.},
  journal-iso = {Multimed. Tools Appl.},
  keywords = {(Deep recurrent neural network,binaural speech segregation,Computational Auditory Scene Analysis,direct-to-reverberant ratio(DRR)),distance and  position information},
  keywords-plus = {REVERBERANT ENERGY RATIO; SOUND SOURCE DISTANCE; FILTER BANK FEATURES; SOURCE SEPARATION; CLASSIFICATION; LOCALIZATION; ENVIRONMENTS; INTELLIGIBILITY; ENHANCEMENT; RECOGNITION},
  langid = {english},
  number = {15},
  number-of-cited-references = {48},
  research-areas = {Computer Science; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000440703500055},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic}
}

@article{vera-diazEndtoEndAcousticLocalization2018,
  title = {Towards {{End}}-to-{{End Acoustic Localization}} Using {{Deep Learning}}: From {{Audio Signal}} to {{Source Position Coordinates}}},
  shorttitle = {Towards {{End}}-to-{{End Acoustic Localization}} Using {{Deep Learning}}},
  author = {Vera-Diaz, Juan Manuel and Pizarro, Daniel and Macias-Guarasa, Javier},
  date = {2018-10-12},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {18},
  pages = {3418},
  issn = {1424-8220},
  doi = {10.3390/s18103418},
  url = {http://arxiv.org/abs/1807.11094},
  urldate = {2019-05-16},
  abstract = {This paper presents a novel approach for indoor acoustic source localization using microphone arrays and based on a Convolutional Neural Network (CNN). The proposed solution is, to the best of our knowledge, the first published work in which the CNN is designed to directly estimate the three dimensional position of an acoustic source, using the raw audio signal as the input information avoiding the use of hand crafted audio features. Given the limited amount of available localization data, we propose in this paper a training strategy based on two steps. We first train our network using semi-synthetic data, generated from close talk speech recordings, and where we simulate the time delays and distortion suffered in the signal that propagates from the source to the array of microphones. We then fine tune this network using a small amount of real data. Our experimental results show that this strategy is able to produce networks that significantly improve existing localization methods based on SRP-PHAT strategies. In addition, our experiments show that our CNN method exhibits better resistance against varying gender of the speaker and different window sizes compared with the other methods.},
  archivePrefix = {arXiv},
  eprint = {1807.11094},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\JBSXD4X9\\Vera-Diaz et al. - 2018 - Towards End-to-End Acoustic Localization using Dee.pdf},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  langid = {english},
  number = {10}
}

@article{vesperiniLocalizingSpeakersMultiple2018,
  title = {Localizing Speakers in Multiple Rooms by Using {{Deep Neural Networks}}},
  author = {Vesperini, Fabio and Vecchiotti, Paolo and Principi, Emanuele and Squartini, Stefano and Piazza, Francesco},
  date = {2018-05-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {49},
  pages = {83--106},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2017.12.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230817301377},
  urldate = {2018-10-19},
  abstract = {In the field of human speech capturing systems, a fundamental role is played by the source localization algorithms. In this paper a Speaker Localization algorithm (SLOC) based on Deep Neural Networks (DNN) is evaluated and compared with state-of-the art approaches. The speaker position in the room under analysis is directly determined by the DNN, leading the proposed algorithm to be fully data-driven. Two different neural network architectures are investigated: the Multi Layer Perceptron (MLP) and Convolutional Neural Networks (CNN). GCC-PHAT (Generalized Cross Correlation-PHAse Transform) Patterns, computed from the audio signals captured by the microphone are used as input features for the DNN. In particular, a multi-room case study is dealt with, where the acoustic scene of each room is influenced by sounds emitted in the other rooms. The algorithm is tested by means of the home recorded DIRHA dataset, characterized by multiple wall and ceiling microphone signals for each room. In detail, the focus goes to speaker localization task in two distinct neighboring rooms. As term of comparison, two algorithms proposed in literature for the addressed applicative context are evaluated, the Crosspower Spectrum Phase Speaker Localization (CSP-SLOC) and the Steered Response Power using the Phase Transform speaker localization (SRP-SLOC). Besides providing an extensive analysis of the proposed method, the article shows how DNN-based algorithm significantly outperforms the state-of-the-art approaches evaluated on the DIRHA dataset, providing an average localization error, expressed in terms of Root Mean Square Error (RMSE), equal to 324~mm and 367~mm, respectively, for the Simulated and the Real subsets.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IH7VSYHN\\Vesperini et al. - 2018 - Localizing speakers in multiple rooms by using Dee.pdf;C\:\\Users\\sauli\\Zotero\\storage\\5LBQQ84B\\S0885230817301377.html},
  keywords = {Acoustic source localization,Computational Audio Processing,Convolutional Neural Networks,Deep Neural Networks,GCC-PHAT,Speaker localization}
}

@article{vesperiniLocalizingSpeakersMultiple2018a,
  title = {Localizing Speakers in Multiple Rooms by Using {{Deep Neural Networks}}},
  author = {Vesperini, Fabio and Vecchitti, Paolo and Principi, Emanuele and Squartini, Stefano and Piazza, Francesco},
  date = {2018-05},
  journaltitle = {COMPUTER SPEECH AND LANGUAGE},
  volume = {49},
  pages = {\{83-106\}},
  publisher = {ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD},
  location = {24-28 OVAL RD, LONDON NW1 7DX, ENGLAND},
  issn = {0885-2308},
  doi = {\{10.1016/j.csl.2017.12.002\}},
  abstract = {In the field of human speech capturing systems, a fundamental role is played by the source localization algorithms. In this paper a Speaker Localization algorithm (SLOC) based on Deep Neural Networks (DNN) is evaluated and compared with state of-the art approaches. The speaker position in the room under analysis is directly determined by the DNN, leading the proposed algorithm to be fully data-driven. Two different neural network architectures are investigated: the Multi Layer Perceptron (MLP) and Convolutional Neural Networks (CNN). GCC-PHAT (Generalized Cross Correlation-PHAse Transform) Patterns, computed from the audio signals captured by the microphone are used as input features for the DNN. In particular, a multi-room case study is dealt with, where the acoustic scene of each room is influenced by sounds emitted in the other rooms. The algorithm is tested by means of the home recorded DIRHA dataset, characterized by multiple wall and ceiling microphone signals for each room. In detail, the focus goes to speaker localization task in two distinct neighboring rooms. As term of comparison, two algorithms proposed in literature for the addressed applicative context are evaluated, the Cross power Spectrum Phase Speaker Localization (CSP-SLOC) and the Steered Response Power using the Phase Transform speaker localization (SRP-SLOC). Besides providing an extensive analysis of the proposed method, the article shows how DNN-based algorithm significantly outperforms the state-of-the-art approaches evaluated on the DIRHA dataset, providing an average localization error, expressed in terms of Root Mean Square Error (RMSE), equal to 324 mm and 367 mm, respectively, for the Simulated and the Real subsets. (C) 2017 Elsevier Ltd. All rights reserved.},
  affiliation = {Vesperini, F (Reprint Author), Univ Politecn Marche, Dept Informat Engn, Via Brecce Bianche, I-60131 Ancona, Italy. Vesperini, Fabio; Vecchitti, Paolo; Principi, Emanuele; Squartini, Stefano; Piazza, Francesco, Univ Politecn Marche, Dept Informat Engn, Via Brecce Bianche, I-60131 Ancona, Italy.},
  author-email = {f.vesperini@pm.univpm.it p.vecchiotti@pm.univpm.it e.principi@univpm.it s.squartini@univpm.it f.piazza@univpm.it},
  cited-references = {Al-rfou R., 2016, ARXIV160502688. Asaei A, 2009, SIGNAL PROCESS, V89, P1038, DOI 10.1016/j.sigpro.2008.12.003. Augusto JC, 2013, HUM-CENTRIC COMPUT I, V3, DOI 10.1186/2192-1962-3-12. Brutti A., 2014, P EV. Brutti A, 2007, INT CONF ACOUST SPEE, P493. Champagne B, 1996, IEEE T SPEECH AUDI P, V4, P148, DOI 10.1109/89.486067. Chollet F, 2015, KERAS. Cobos M., 2017, WIREL COMMUN MOB COM, V24. Cristoforetti L, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P2629. Das A., 2016, INT RES J ADV ENG SC, V1, P33. Datum MS, 1996, J ACOUST SOC AM, V100, P372, DOI 10.1121/1.415854. Dehkordi M. B., 2011, 2011 International Symposium on Artificial Intelligence and Signal Processing (AISP), P6, DOI 10.1109/AISP.2011.5960980. DIBIASE JH, 2001, MICROPHONE ARRAYS SI, P157. Do H., 2007, IEEE INT C AC SPEECH, V1, P1, DOI DOI 10.1109/ICASSP.2007.366631. Ferroni G., 2014, P EVALLTA PIS IT DEC, P153. Ferroni G., 2015, P IJCNN KILL IR JUL, P1. GIANNOULIS P, 2015, PROCEEDINGS OF THE 2, P1271. Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735. HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8. Ioffe S, 2015, ICML, V32, P448. Katsamanis A., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5547, DOI 10.1109/ICASSP.2014.6854664. Kingma D. P., 2014, P 3 INT C LEARN REPR. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195. Lee T, 2016, COMPUT SPEECH LANG, V35, P1, DOI 10.1016/j.csl.2015.05.002. Li XF, 2013, IEEE T CYBERNETICS, V43, P1199, DOI 10.1109/TSMCB.2012.2226443. Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882. MCLOUGHLIN I, 2015, PROCEEDINGS OF INTER, P2400. Meng W, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020376. Minotto VP, 2013, INT J HIGH PERFORM C, V27, P291, DOI 10.1177/1094342012452166. Morales-Cordovilla J.A., 2014, P INTERSPEECH, P2450. Mumolo E, 2003, ROBOT AUTON SYST, V42, P69, DOI 10.1016/S0921-8890(02)00325-1. MURRAY JC, 2011, PROCEEDINGS OF THE I, P763. Nair V., 2010, P 27 INT C MACH LEAR, P807, DOI DOI 10.0RG/PAPERS/432.PDF. Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191. Principi E, 2015, EXPERT SYST APPL, V42, P5668, DOI 10.1016/j.eswa.2015.02.036. Rodomagoulakis I, 2017, COMPUT SPEECH LANG, V46, P419, DOI 10.1016/j.csl.2017.02.004. Roman N, 2003, J ACOUST SOC AM, V114, P2236, DOI 10.1121/1.1610463. RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0. Saxena Ashutosh, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P1737, DOI 10.1109/ROBOT.2009.5152861. SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830. Stephenne A, 1997, SIGNAL PROCESS, V59, P253, DOI 10.1016/S0165-1684(97)00051-0. Stern R.M., 2006, BINAURAL SOUND LOCAL, P147. Takeda R, 2016, 2016 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2016), P603, DOI 10.1109/SLT.2016.7846325. Thomas Samuel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2519, DOI 10.1109/ICASSP.2014.6854054. Transfeld P., 2015, P IEEE INT C AC SPEE, P2629. Trifa Vlad M, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P393. Tsiarni A., 2014, P EUSIPCO LISB PORT, P2390. Vecchiotti P, 2018, SMART INNOV SYST TEC, V69, P161, DOI 10.1007/978-3-319-56904-8_16. VERMAAK J, 2001, ACOUST SPEECH SIG PR, P3021. Vesperini F., 2016, P 26 INT WORKSH MACH, P1. Vesperini F, 2016, P INT JOINT C NEUR N, P3391. Willert V, 2006, IEEE T SYST MAN CY B, V36, P982, DOI 10.1109/TSMCB.2006.872263. Wolfel M., 2009, DISTANT SPEECH RECOG. XIAO X, 2015, PROCEEDINGS OF IEEE, P2814. Yook D, 2016, IEEE T CYBERNETICS, V46, P20, DOI 10.1109/TCYB.2015.2391252. Zakarauskas P., 1991, J ACOUST SOC AM, V90, P2366. ZHANG C, 2008, P IEEE INT C AC SPEE, P2565. Zotkin DN, 2004, IEEE T SPEECH AUDI P, V12, P499, DOI 10.1109/TSA.2004.832990.},
  da = {2018-10-18},
  doc-delivery-number = {FS3WX},
  eissn = {1095-8363},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\N782TJ25\\S0885230817301377.html},
  funding-acknowledgement = {Italian University and Research Consortium CINECA; CINECA award under the ISCRA initiative},
  funding-text = {This research has been partly supported by the Italian University and Research Consortium CINECA. We acknowledge the CINECA award under the ISCRA initiative, for the availability of high performance computing resources and support.},
  journal-iso = {Comput. Speech Lang.},
  keywords = {(Acoustic source localization,Computational Audio  Processing),Convolutional Neural Networks,Deep  Neural Networks,GCC-PHAT,Speaker localization},
  keywords-plus = {SOURCE LOCALIZATION; SOUND LOCALIZATION; TIME-DELAY; COMMAND RECOGNITION; ENVIRONMENTS},
  langid = {english},
  number-of-cited-references = {59},
  orcid-numbers = {squartini, stefano/0000-0001-9374-0128},
  research-areas = {Computer Science},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000419716200006},
  usage-count-last-180-days = {30},
  usage-count-since-2013 = {49},
  web-of-science-categories = {Computer Science, Artificial Intelligence}
}

@online{ViewpointSelectionEfficient,
  title = {(2) {{Viewpoint}} Selection for Efficient {{3D}} Scanning with a {{LIDAR}} | {{Request PDF}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/329903122_Viewpoint_selection_for_efficient_3D_scanning_with_a_LIDAR},
  urldate = {2020-01-17},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\SHPFWSV4\\329903122_Viewpoint_selection_for_efficient_3D_scanning_with_a_LIDAR.html},
  langid = {english}
}

@inproceedings{vincentSecondCHiMESpeech2013,
  title = {The Second \&\#x2018;{{CHiME}}\&\#x2019; Speech Separation and Recognition Challenge: {{An}} Overview of Challenge Systems and Outcomes},
  shorttitle = {The Second \&\#x2018;{{CHiME}}\&\#x2019; Speech Separation and Recognition Challenge},
  author = {Vincent, Emmanuel and Barker, Jon and Watanabe, Shinji and Le Roux, Jonathan and Nesta, Francesco and Matassoni, Marco},
  date = {2013-12},
  pages = {162--167},
  publisher = {{IEEE}},
  doi = {10.1109/ASRU.2013.6707723},
  url = {http://ieeexplore.ieee.org/document/6707723/},
  urldate = {2018-07-18},
  abstract = {Distant-microphone automatic speech recognition (ASR) remains a challenging goal in everyday environments involving multiple background sources and reverberation. This paper reports on the results of the 2nd ’CHiME’ Challenge, an initiative designed to analyse and evaluate the performance of ASR systems in a real-world domestic environment. We discuss the rationale for the challenge and provide a summary of the datasets, tasks and baseline systems. The paper overviews the systems that were entered for the two challenge tracks: small-vocabulary with moving talker and medium-vocabulary with stationary talker. We present a summary of the challenge findings including novel results produced by challenge system combination. Possible directions for future challenges are discussed.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\DHPRYPTY\\Vincent et al. - 2013 - The second &#x2018\;CHiME&#x2019\; speech separation.pdf},
  isbn = {978-1-4799-2756-2},
  langid = {english}
}

@online{VoiceDataCabling,
  title = {Voice and {{Data Cabling}} \& {{Wiring Installation Company Miami Fort Lauderdale Palm Beach FL}}},
  url = {http://www.axisnetworkcabling.com/voice-data-cabling-installers-company.html},
  urldate = {2017-06-12},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FT2E8WD3\\voice-data-cabling-installers-company.html}
}

@article{wangAcousticSourceLocalization2018,
  title = {Acoustic Source Localization in Strong Reverberant Environment by Parametric {{Bayesian}} Dictionary Learning},
  author = {Wang, Lu and Liu, Yanshan and Zhao, Lifan and Wang, Qiang and Zeng, Xiangyang and Chen, Kean},
  date = {2018-02-01},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  volume = {143},
  pages = {232--240},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2017.09.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0165168417303249},
  urldate = {2018-10-19},
  abstract = {Sparse representation techniques have become increasingly promising for localizing the sound source in reverberant environment, where the multipath channel effects can be accurately characterized by the image model. In this paper, a dictionary is constructed by discretizing the inner space of the enclosure, which is parameterized by the unknown energy reflective ratio. More specifically, each atom of the dictionary can characterize a specific source-to-microphone multipath channel. Subsequently, source localization can be reformulated as a joint sparse signal recovery and parametric dictionary learning problem. In particular, a sparse Bayesian framework is utilized for modeling, where its solution can be obtained by variational Bayesian expectation maximization technique. Moreover, the joint sparsity in frequency domain is exploited to improve the dictionary learning performances. A remarkably advantage of this approach is that no laborious parameter tuning procedure is required and statistical information can be provided. Numerical simulation results have shown that the proposed algorithm achieves high source localization accuracy, low sidelobes and high robustness for multiple sources with low computational complexity in strong reverberant environments, compared with other state-of-the-art methods.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KWPSQMMT\\Wang et al. - 2018 - Acoustic source localization in strong reverberant.pdf;C\:\\Users\\sauli\\Zotero\\storage\\NVMKYXRB\\S0165168417303249.html},
  keywords = {Parametric dictionary learning,Reverberant environment,Source localization,Sparse Bayesian method}
}

@article{wangAcousticSourceLocalization2018a,
  title = {Acoustic Source Localization in Strong Reverberant Environment by Parametric {{Bayesian}} Dictionary Learning},
  author = {Wang, Lu and Liu, Yanshan and Zhao, Lifan and Wang, Qiang and Zeng, Xiangyang and Chen, Kean},
  date = {2018-02},
  journaltitle = {SIGNAL PROCESSING},
  volume = {143},
  pages = {\{232-240\}},
  publisher = {ELSEVIER SCIENCE BV},
  location = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
  issn = {0165-1684},
  doi = {\{10.1016/j.sigpro.2017.09.005\}},
  abstract = {Sparse representation techniques have become increasingly promising for localizing the sound source in reverberant environment, where the multipath channel effects can be accurately characterized by the image model. In this paper, a dictionary is constructed by discretizing the inner space of the enclosure, which is parameterized by the unknown energy reflective ratio. More specifically, each atom of the dictionary can characterize a specific source-to-microphone multipath channel. Subsequently, source localization can be reformulated as a joint sparse signal recovery and parametric dictionary learning problem. In particular, a sparse Bayesian framework is utilized for modeling, where its solution can be obtained by variational Bayesian expectation maximization technique. Moreover, the joint sparsity in frequency domain is exploited to improve the dictionary learning performances. A remarkably advantage of this approach is that no laborious parameter tuning procedure is required and statistical information can be provided. Numerical simulation results have shown that the proposed algorithm achieves high source localization accuracy, low sidelobes and high robustness for multiple sources with low computational complexity in strong reverberant environments, compared with other state-of-the-art methods. (C) 2017 Elsevier B.V. All rights reserved.},
  affiliation = {Zhao, LF (Reprint Author), Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore. Wang, Lu; Liu, Yanshan; Wang, Qiang; Zeng, Xiangyang; Chen, Kean, Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian 710072, Shaanxi, Peoples R China. Zhao, Lifan, Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.},
  author-email = {wanglu@nwpu.edu.cn liuysnwpu@mail.nwpu.edu.cn zhao0145@e.ntu.edu.sg wqiang0212@mail.nwpu.edu.cn zenggxy@nwpu.edu.cn kachen@nwpu.edu.cn},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Asaei A., 2014, IEEE INT C AC SPEECH, P9. Asaei A, 2014, IEEE-ACM T AUDIO SPE, V22, P620, DOI 10.1109/TASLP.2013.2297012. Asano F, 2011, EUR SIGNAL PR CONF, P2009. Asano F, 2013, IEEE T AUDIO SPEECH, V21, P1953, DOI 10.1109/TASL.2013.2263140. Asano F, 2012, INT CONF ACOUST SPEE, P193, DOI 10.1109/ICASSP.2012.6287850. Babacan SD, 2011, IEEE T IMAGE PROCESS, V20, P984, DOI 10.1109/TIP.2010.2080278. Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894. Beal M., 2004, VARIATIONAL ALGORITH. Bishop C., 2006, PATTERN RECOGN, P462. Blandin C., 2011, SIGNAL PROCESS. Chardon G, 2012, INT CONF ACOUST SPEE, P9, DOI 10.1109/ICASSP.2012.6287804. DiBiase JH, 2001, DIGITAL SIGNAL PROC, P157. Dmochowski J. P., 2007, IEEE WORKSH APPL SIG, P18. Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837. Ji SH, 2008, IEEE T SIGNAL PROCES, V56, P2346, DOI 10.1109/TSP.2007.914345. Kowalski M., 2014, IEEE-ACM T AUDIO SPE, V18, P1818. Koyama Shoichi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4443, DOI 10.1109/ICASSP.2014.6854442. Koyama S, 2015, 2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL SUMMIT AND CONFERENCE (APSIPA), P850, DOI 10.1109/APSIPA.2015.7415391. Le Roux J, 2013, INT CONF ACOUST SPEE, P4310, DOI 10.1109/ICASSP.2013.6638473. Loesch B., 2010, COMP DIFFERENT ALGOR. Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236. Tzikas DG, 2008, IEEE SIGNAL PROC MAG, V25, P131, DOI 10.1109/MSP.2008.929620. Wang L, 2016, IEEE T SIGNAL PROCES, V64, P275, DOI 10.1109/TSP.2015.2481790. Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x.},
  da = {2018-10-18},
  doc-delivery-number = {FK6IT},
  eissn = {1879-2677},
  funding-acknowledgement = {National Natural Science Foundation of China [61501375]; Fundamental Research Funds for the Central Universities [3102016ZY006]},
  funding-text = {This work was supported by the National Natural Science Foundation of China under Grant 61501375 and the Fundamental Research Funds for the Central Universities under Grant 3102016ZY006.},
  journal-iso = {Signal Process.},
  keywords = {(Source localization,Parametric dictionary  learning,Reverberant environment),Sparse Bayesian method},
  langid = {english},
  number-of-cited-references = {25},
  research-areas = {Engineering},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000413608100025},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {21},
  web-of-science-categories = {Engineering, Electrical & Electronic}
}

@article{wangPseudoDeterminedBlindSource2018,
  title = {Pseudo-{{Determined Blind Source Separation}} for {{Ad}}-Hoc {{Microphone Networks}}},
  author = {Wang, Lin and Cavallaro, Andrea},
  date = {2018-05},
  journaltitle = {IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING},
  volume = {26},
  pages = {\{981-994\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2329-9290},
  doi = {\{10.1109/TASLP.2018.2803263\}},
  abstract = {We propose a pseudo-determined blind source separation framework that exploits the information from a large number of microphones in an ad-hoc network to extract and enhance sound sources in a reverberant scenario. After compensating for the time offsets and sampling rate mismatch between (asynchronous) signals, we interpret as a determined M x M mixture the over-determined M x N mixture, where M x N is the number of microphones and N is the number of sources. Next, we propose a pseudodetermined mixture model that can apply an M x M independent component analysis (ICA) directly to the M-channel recordings. Moreover, we propose a reference-based permutation alignment scheme that aligns the permutation of the ICA outputs and classifies them into target channels, which contain the N sources, and nontarget channels, which contain reverberation residuals. Finally, using the signals from nontarget channels, we estimate in each target channel the power spectral density of the noise component that we suppress with a spectral postfilter. Interestingly, we also obtain late-reverberation suppression as by-product. Experiments show that each processing block improves incrementally source separation and that the performance of the proposed pseudodetermined separation improves as the number of microphones increases.},
  affiliation = {Wang, L (Reprint Author), Queen Mary Univ London, Ctr Intelligent Sensing, London E1 4NS, England. Wang, Lin; Cavallaro, Andrea, Queen Mary Univ London, Ctr Intelligent Sensing, London E1 4NS, England.},
  author-email = {lin.wang@qmul.ac.uk a.cavallaro@qmul.ac.uk},
  cited-references = {ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599. Araki S, 2003, EURASIP J APPL SIG P, V2003, P1157, DOI 10.1155/S1110865703305074. Araki S, 2003, IEEE T SPEECH AUDI P, V11, P109, DOI 10.1109/TSA.2003.809193. Asano F, 2003, IEEE T SPEECH AUDI P, V11, P204, DOI 10.1109/TSA.2003.809191. Bertrand A., 2011, P IEEE S COMM VEH TE, P1, DOI DOI 10.1109/SCVT.2011.6101302. Bradley JS, 2003, J ACOUST SOC AM, V113, P3233, DOI 10.1121/1.1570439. Douglas SC, 2007, INT CONF ACOUST SPEE, P637. Douglas SC, 2007, IEEE T AUDIO SPEECH, V15, P1511, DOI 10.1109/TASL.2007.899176. Duong N. Q. K., 2012, 2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin), P241, DOI 10.1109/ICCE-Berlin.2012.6336458. Gerkmann T, 2012, IEEE T AUDIO SPEECH, V20, P1383, DOI 10.1109/TASL.2011.2180896. Hon TK, 2015, IEEE-ACM T AUDIO SPE, V23, P1623, DOI 10.1109/TASLP.2015.2442417. Hu Y, 2008, IEEE T AUDIO SPEECH, V16, P229, DOI 10.1109/TASL.2007.911054. Hyvarinen A., 2004, INDEPENDENT COMPONEN. Joho M., 2000, Second International Workshop on Independent Component Analysis and Blind Signal Separation. Proceedings, P81. Kim J, 2014, P IEEE INT C MULT EX, P1. Kim T, 2007, IEEE T AUDIO SPEECH, V15, P70, DOI 10.1109/TASL.2006.872618. Koutras A., 2001, P EUR 2001 SEP, P1009. LIENHART R, 2003, ACOUST SPEECH SIG PR, P840. Lu ZH, 2013, IEEE T SIGNAL PROCES, V61, P1303, DOI 10.1109/TSP.2012.2234747. Makino S, 2007, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4020-6479-1. Matsuoka K, 2002, SICE 2002: PROCEEDINGS OF THE 41ST SICE ANNUAL CONFERENCE, VOLS 1-5, P2138, DOI 10.1109/SICE.2002.1195729. Miyabe S, 2015, SIGNAL PROCESS, V107, P185, DOI 10.1016/j.sigpro.2014.09.015. Ochi K, 2016, INTERSPEECH, P3369, DOI 10.21437/Interspeech.2016-758. ONO N, 2015, PROC LVA ICA, V9237, P387, DOI DOI 10.1007/978-3-319-22482-4_45. Ono N, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P189, DOI 10.1109/ASPAA.2011.6082320. Osterwise C, 2014, IEEE-ACM T AUDIO SPE, V22, P956, DOI 10.1109/TASLP.2014.2307166. Plinge A, 2016, IEEE SIGNAL PROC MAG, V33, P14, DOI 10.1109/MSP.2016.2555198. Robledo-Arnuncio E, 2007, INT CONF ACOUST SPEE, P949. Saruwatari H, 2006, IEEE T AUDIO SPEECH, V14, P666, DOI 10.1109/TSA.2005.855832. Sawada H, 2004, IEEE T SPEECH AUDI P, V12, P530, DOI 10.1109/TSA.2004.832994. Sawada H, 2002, NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS, P465, DOI 10.1109/NNSP.2002.1030058. Sawada H, 2011, IEEE T AUDIO SPEECH, V19, P516, DOI 10.1109/TASL.2010.2051355. Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005. Wang L, 2016, IEEE-ACM T AUDIO SPE, V24, P1573, DOI 10.1109/TASLP.2016.2573048. Wang L, 2016, IEEE-ACM T AUDIO SPE, V24, P1079, DOI 10.1109/TASLP.2016.2533859. Wang L, 2016, IEEE T SIGNAL PROCES, V64, P1018, DOI 10.1109/TSP.2015.2498130. Wang L, 2015, IEEE-ACM T AUDIO SPE, V23, P1493, DOI 10.1109/TASLP.2015.2438542. Wang L, 2014, DIGIT SIGNAL PROCESS, V31, P79, DOI 10.1016/j.dsp.2014.04.009. Wang L, 2010, EURASIP J AUDIO SPEE, DOI 10.1155/2010/797962. Wang L, 2011, ACOUST AUST, V39, P64. Wang L, 2011, IEEE T AUDIO SPEECH, V19, P549, DOI 10.1109/TASL.2010.2052244. Westner A., 1999, P INT WORKSH IND COM, P11. Winter S, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/71632. Zhang Y., INT J ADAPTIVE CONTR, V25, P88. Zheng YH, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-26.},
  da = {2018-10-18},
  doc-delivery-number = {FZ8NU},
  funding-acknowledgement = {U.K. Engineering and Physical Sciences Research Council [EP/K007491/1]; ARTEMIS-JU; U.K. Technology Strategy Board (Innovate U.K.) through the COPCAMS Project [332913]},
  funding-text = {This work was supported in part by the U.K. Engineering and Physical Sciences Research Council under Grant EP/K007491/1 and in part by the ARTEMIS-JU and the U.K. Technology Strategy Board (Innovate U.K.) through the COPCAMS Project under Grant 332913. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Augusto Sarti.},
  journal-iso = {IEEE-ACM Trans. Audio Speech Lang.},
  keywords = {(Ad-hoc,asynchronous recording,blind source separation,over-determined  mixture)},
  keywords-plus = {PERMUTATION ALIGNMENT; CONVOLUTIVE MIXTURES; SPEECH; LOCALIZATION; ALGORITHMS},
  langid = {english},
  number = {5},
  number-of-cited-references = {45},
  oa = {gold_or_bronze},
  orcid-numbers = {Wang, Lin/0000-0001-8095-9518},
  research-areas = {Acoustics; Engineering},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000427867300010},
  usage-count-last-180-days = {9},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Acoustics; Engineering, Electrical & Electronic}
}

@article{wanImprovedSoundSource2013,
  title = {Improved {{Sound Source Localization Using Classifier}} in {{Reverberant Noisy Environment}}},
  author = {Wan, Xinwang and Liang, Juan},
  date = {2013-12-01},
  journaltitle = {Journal of Applied Sciences},
  volume = {13},
  pages = {4897--4901},
  issn = {18125654},
  doi = {10.3923/jas.2013.4897.4901},
  url = {http://www.scialert.net/abstract/?doi=jas.2013.4897.4901},
  urldate = {2017-05-22},
  number = {21}
}

@article{wanSoundSourceLocalization2013,
  title = {Sound Source Localization Based on Discrimination of Cross-Correlation Functions},
  author = {Wan, Xinwang and Wu, Zhenyang},
  date = {2013-01},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {74},
  pages = {28--37},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2012.06.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0003682X1200165X},
  urldate = {2017-05-22},
  abstract = {Sound source localization plays a crucial role in many microphone arrays application, ranging from speech enhancement to human–computer interface in a reverberant noisy environment. The steered response power (SRP) using the phase transform (SRP-PHAT) method is one of the most popular modern localization algorithms. The SRP-based source localizers have been proved robust, however, the methods may fail to locate the sound source in adverse noise and reverberation conditions, especially when the direct paths to the microphones are unavailable. This paper proposes a localization algorithm based on discrimination of cross-correlation functions. The cross-correlation functions are calculated by the generalized cross-correlation phase transform (GCC-PHAT) method. Using cross-correlation functions, sound source location is estimated by one of the two classifiers: Naive-Bayes classifier and Euclidean distance classifier. Simulation results have demonstrated that the proposed algorithms provide higher localization accuracy than the SRP-PHAT algorithm in reverberant noisy environment.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\B37ZAVTI\\Wan and Wu - 2013 - Sound source localization based on discrimination .pdf;C\:\\Users\\sauli\\Zotero\\storage\\2RDD2EXR\\S0003682X1200165X.html},
  keywords = {Cross-correlation function,Euclidean distance classifier,microphone array,Naive-Bayes classifier,sound source localization},
  number = {1}
}

@incollection{Ward2001,
  title = {Constant {{Directivity Beamforming}}},
  booktitle = {Microphone {{Arrays}}: {{Signal Processing Techniques}} and {{Applications}}},
  author = {Ward, Darren B. and Kennedy, Rodney A. and Williamson, Robert C.},
  editor = {Brandstein, Michael and Ward, Darren},
  date = {2001},
  pages = {3--17},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-04619-7_1},
  url = {https://doi.org/10.1007/978-3-662-04619-7_1},
  abstract = {Beamforming, or spatial filtering, is one of the simplest methods for discriminating between different signals based on the physical location of the sources. Because speech is a very wideband signal, covering some four octaves, traditional narrowband beamforming techniques are inappropriate for hands-free speech acquisition. One class of broadband beamformers, called constant directivity beam-formers, aim to produce a constant spatial response over a broad frequency range. In this chapter we review such beamformers, and discuss implementation issues related to their use in microphone arrays.},
  isbn = {978-3-662-04619-7}
}

@article{wassermanStatisticalAnalysisSemiSupervised,
  title = {Statistical {{Analysis}} of {{Semi}}-{{Supervised Regression}}},
  author = {Wasserman, Larry and Lafferty, John D},
  pages = {8},
  abstract = {Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our first result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GV3P6JVV\\Wasserman and Lafferty - Statistical Analysis of Semi-Supervised Regression.pdf},
  langid = {english}
}

@book{weinsteinSignatureArchitectsSan2006,
  title = {Signature {{Architects}} of the {{San Francisco Bay Area}}},
  author = {Weinstein, Dave},
  date = {2006},
  publisher = {{Gibbs Smith}},
  abstract = {When talk turns to architects who have made their mark in the San Francisco Bay Area, it often stops after two names-Bernard Maybeck and Julia Morgan. Widely admired, they are the stuff of legend. In a new book on the "signature" styles of Bay Area architecture, author and architecture critic Dave Weinstein takes aim at some of the most important yet lesser-known Bay Area architects-in other words, everyone else. What began as an outgrowth of articles he had written for the San Francisco Chronicle about preserving historic buildings is now a definitive work on the architecture of an area that is unlike any other. The fifteen architects profiled here were chosen not because they are the best the area has produced (though several are) but because their stories, taken together, provide a solid history of Bay Area residential architecture.  Fifteen Bay Area architects are profiled: the Newsom brothers, Leola Hall, Ernest Coxhead, Luther Turton, Albert Farr, John Hudson Thomas, Frank Wolfe, Birge Clark, Carr Jones, Gardner Dailey, Roger Lee, Jack Hillmer, Warren Callister, Donald Olsen, and the firm Ace Architects.  Sidebars cover topics such as What's a Queen Anne?, What's a Stick?, What's an Arts and Crafts Home?, and The Eichler Phenomenon A summary on each architect including birth/death dates, style, active projects, famous projects, and a list of houses to visit  Architectural styles covered include English Cottage, Stucco, Prairie, Colonial, Modern, and more},
  eprint = {unqUNeNaoVcC},
  eprinttype = {googlebooks},
  isbn = {978-1-58685-751-6},
  keywords = {Architecture / Buildings / Residential,Architecture / History / General},
  langid = {english},
  pagetotal = {154}
}

@article{wengThreedimensionalSoundLocalization2001,
  title = {Three-Dimensional Sound Localization from a Compact Non-Coplanar Array of Microphones Using Tree-Based Learning},
  author = {Weng, J. and Guentchev, K. Y.},
  date = {2001-07},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {110},
  pages = {310--323},
  issn = {0001-4966},
  doi = {10.1121/1.1377290},
  abstract = {One of the various human sensory capabilities is to identify the direction of perceived sounds. The goal of this work is to study sound source localization in three dimensions using some of the most important cues the human uses. In an attempt to satisfy the requirements of portability and miniaturization in robotics, this approach employs a compact sensor structure that can be placed on a mobile platform. The objective is to estimate the relative sound source position in three-dimensional space without imposing excessive restrictions on its spatio-temporal characteristics and the environment structure. Two types of features are considered, interaural time and level differences. Their relative effectiveness for localization is studied, as well as a practical way of using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknown sounds are processed by the trained system to estimate the 3D location of the sound source. Results from the experiments showed under +/-3 degrees in average angular error and less than +/-20\% in average radial distance error.},
  eprint = {11508957},
  eprinttype = {pmid},
  keywords = {Computer Systems,Equipment Design,Humans,Numerical Analysis; Computer-Assisted,Psychoacoustics,Robotics,Signal Processing; Computer-Assisted,Sound Localization,Speech Perception},
  langid = {english},
  number = {1}
}

@article{wengThreedimensionalSoundLocalization2001a,
  title = {Three-Dimensional Sound Localization from a Compact Non-Coplanar Array of Microphones Using Tree-Based Learning},
  author = {Weng, Juyang and Guentchev, Kamen Y.},
  date = {2001-07-03},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {110},
  pages = {310},
  issn = {0001-4966},
  doi = {10.1121/1.1377290},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.1377290},
  urldate = {2019-09-19},
  abstract = {One of the various human sensory capabilities is to identify the direction of perceived sounds. The goal of this work is to study sound source localization in three dimensions using some of the most important cues the human uses. In an attempt to satisfy the requirements of portability and miniaturization in robotics, this approach employs a compact sensor structure that can be placed on a mobile platform. The objective is to estimate the relative sound source position in three-dimensional space without imposing excessive restrictions on its spatio-temporal characteristics and the environment structure. Two types of features are considered, interaural time and level differences. Their relative effectiveness for localization is studied, as well as a practical way of using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknown sounds are processed by the trained system to estimate the 3D location of the sound source. Results from the experiments showed under ±3° in average angular error and less than ±20\% in average radial distance error.One of the various human sensory capabilities is to identify the direction of perceived sounds. The goal of this work is to study sound source localization in three dimensions using some of the most important cues the human uses. In an attempt to satisfy the requirements of portability and miniaturization in robotics, this approach employs a compact sensor structure that can be placed on a mobile platform. The objective is to estimate the relative sound source position in three-dimensional space without imposing excessive restrictions on its spatio-temporal characteristics and the environment structure. Two types of features are considered, interaural time and level differences. Their relative effectiveness for localization is studied, as well as a practical way of using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknown sounds are processed by the traine...},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\UVZJG834\\1.html},
  langid = {english},
  number = {1}
}

@book{winerAudioExpertEverything2012,
  title = {The {{Audio Expert}}: {{Everything You Need}} to {{Know}} about {{Audio}}},
  shorttitle = {The {{Audio Expert}}},
  author = {Winer, Ethan},
  date = {2012},
  publisher = {{Taylor \& Francis}},
  abstract = {Gain a deep understanding of audio practice and theory with this easy-to-read book, illustrated with more than 400 figures and photographs. Using common sense, plain-English explanations and minimal math, author Ethan Winer helps you understand audio at the deepest, most technical level-no engineering degree necessary. If you're an intermediate to advanced recording engineer or audiophile, you already know the basic mechanics of how audio "works." This book will take you beyond that, weaving together audio concepts, theories of aural perception and acoustics, musical instrument physics, and basic electronics and demonstrating their relationships to one another.  Rather than merely showing you how to use audio devices like equalizers and compressors, Winer explains how they work internally and how they are spec'd and tested.  With The Audio Expert, you get: * Videos and audio examples on the companion website (www.TheAudioExpertBook.com) that help you understand complex topics, such as vibration and resonance * Platform agnostic explanations, applying to Windows and Mac operating systems, and to most software and hardware * Practical tips, tricks, advice, and lots of myth-busting Bonus chapters are currently available on FocalPress.com.  http://www.focalpress.com/books/audio/the\_audio\_expert.aspx?terms=Audio+Expert* Explains clearly the innermost details of audio hardware and software * Contains numerous practical tips, tricks, advice, and lots of myth-busting * Provides videos and audio examples on the companion website for comprehensive understanding},
  eprint = {Hf0YQAUWGfgC},
  eprinttype = {googlebooks},
  isbn = {978-0-240-82100-9},
  keywords = {Music / Instruction & Study / General,Music / Instruction & Study / Techniques,Music / Instruction & Study / Theory,Music / Recording & Reproduction},
  langid = {english},
  pagetotal = {673}
}

@book{wittingModellingDiffuseSound1999,
  title = {Modelling of {{Diffuse Sound Field Excitations}} and {{Dynamic Response Analysis}} of {{Leightweight Structures}}},
  author = {Witting, Michael},
  date = {1999},
  publisher = {{Herbert Utz Verlag}},
  eprint = {sCk_if0wKCQC},
  eprinttype = {googlebooks},
  isbn = {978-3-89675-678-7},
  langid = {english},
  pagetotal = {192}
}

@article{wuComprehensiveSurveyGraph2019,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  date = {2019-01-02},
  url = {http://arxiv.org/abs/1901.00596},
  urldate = {2019-10-16},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this rapidly growing field.},
  archivePrefix = {arXiv},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\6PQPNAMZ\\Wu et al. - 2019 - A Comprehensive Survey on Graph Neural Networks.pdf;C\:\\Users\\sauli\\Zotero\\storage\\AF78GPCX\\1901.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{wuIndoorSoundSource2018,
  title = {An {{Indoor Sound Source Localization Dataset}} for {{Machine Learning}}},
  booktitle = {Proceedings of the 2018 {{2Nd International Conference}} on {{Computer Science}} and {{Artificial Intelligence}}},
  author = {Wu, Tao and Jiang, Yong and Li, Nan and Feng, Tao},
  date = {2018},
  pages = {28--32},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3297156.3297198},
  url = {http://doi.acm.org/10.1145/3297156.3297198},
  urldate = {2019-09-16},
  abstract = {In this paper, we mainly describe a corpus for sound source localization. The audio of the corpus mainly uses the dual microphone array of the simulated human head to collect audio data in four directions. The reason for selecting four directions is it can be used for related research on sound source localization. On the other hand, it can be extended to more directions by means of probability distribution. The data set is divided into original data set and time delay data set (GCC). Acoustic events include reading a speech event of a verse and a non-speech event from a pulse gun. In order to make the corpus more realistic, we use the anechoic chamber and the ordinary room for experiments. The anechoic chamber environment can be used to verify the feasibility of the model, and the normal room environment can test the noise resistance of the model and the actual application effect. In addition, the corpus can also be used to extract other features for sound source localization tasks. On this basis, we have selected several mainstream deep learning methods to verify the dataset, such as convolutional neural network (CNN) and support vector machine (SVM), the proximity algorithm (K-NN) and the random forest algorithm (BF), and give the corresponding experimental results to ensure the accuracy of the dataset. Readers can make improvements based on the baseline system and develop appropriate models to improve the accuracy of recognition.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\QDY8EFWN\\Wu et al. - 2018 - An Indoor Sound Source Localization Dataset for Ma.pdf},
  isbn = {978-1-4503-6606-9},
  keywords = {classification,convolutional neural network,dataset,machine learning,Sound source localization},
  series = {{{CSAI}} '18},
  venue = {Shenzhen, China}
}

@article{wuLearningProbabilisticLatent2016,
  title = {Learning a {{Probabilistic Latent Space}} of {{Object Shapes}} via {{3D Generative}}-{{Adversarial Modeling}}},
  author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
  date = {2016-10-24},
  url = {http://arxiv.org/abs/1610.07584},
  abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
  archivePrefix = {arXiv},
  eprint = {1610.07584},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GMCHVBSG\\Wu et al. - 2016 - Learning a Probabilistic Latent Space of Object Sh.pdf;C\:\\Users\\sauli\\Zotero\\storage\\27XUEG4B\\1610.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  primaryClass = {cs}
}

@article{xenakiSoundSourceLocalization2018,
  title = {Sound Source Localization and Speech Enhancement with Sparse {{Bayesian}} Learning Beamforming},
  author = {Xenaki, Angeliki and Boldt, Jesper Bunsow and Christensen, Mads Graesboll},
  date = {2018-06},
  journaltitle = {JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA},
  volume = {143},
  pages = {\{3912-3921\}},
  publisher = {ACOUSTICAL SOC AMER AMER INST PHYSICS},
  location = {STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA},
  issn = {0001-4966},
  doi = {\{10.1121/1.5042222\}},
  abstract = {Speech localization and enhancement involves sound source mapping and reconstruction from noisy recordings of speech mixtures with microphone arrays. Conventional beamforming methods suffer from low resolution, especially with a limited number of microphones. In practice, there are only a few sources compared to the possible directions-of-arrival (DOA). Hence, DOA estimation is formulated as a sparse signal reconstruction problem and solved with sparse Bayesian learning (SBL). SBL uses a hierarchical two-level Bayesian inference to reconstruct sparse estimates from a small set of observations. The first level derives the posterior probability of the complex source amplitudes from the data likelihood and the prior. The second level tunes the prior towards sparse solutions with hyperparameters which maximize the evidence, i.e., the data probability. The adaptive learning of the hyperparameters from the data auto-regularizes the inference problem towards sparse robust estimates. Simulations and experimental data demonstrate that SBL beamforming provides high-resolution DOA maps outperforming traditional methods especially for correlated or non-stationary signals. Specifically for speech signals, the high-resolution SBL reconstruction offers not only speech enhancement but effectively speech separation. (C) 2018 Acoustical Society of America.},
  affiliation = {Xenaki, A (Reprint Author), GN Hearing AS, DK-2750 Ballerup, Denmark. Xenaki, Angeliki; Boldt, Jesper Bunsow, GN Hearing AS, DK-2750 Ballerup, Denmark. Christensen, Mads Graesboll, Aalborg Univ, AD MT, Audio Anal Lab, DK-9000 Aalborg, Denmark.},
  author-email = {axenaki@gnresound.com},
  cited-references = {Babacan SD, 2010, IEEE T IMAGE PROCESS, V19, P53, DOI 10.1109/TIP.2009.2032894. Boyd S., 2004, CONVEX OPTIMIZATION, P1. CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278. Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406. Chan D, 1995, P 4 EUR C SPEECH COM, P867. Chardon G, 2012, J ACOUST SOC AM, V132, P1521, DOI 10.1121/1.4740476. Christensen MG, 2012, INT CONF ACOUST SPEE, P409, DOI 10.1109/ICASSP.2012.6287903. Edelmann GF, 2011, J ACOUST SOC AM, V130, pEL232, DOI 10.1121/1.3632046. Farmani M, 2017, IEEE-ACM T AUDIO SPE, V25, P611, DOI 10.1109/TASLP.2017.2651373. Fernandez-Grande E, 2017, J ACOUST SOC AM, V141, P532, DOI 10.1121/1.4974047. Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989. Fortunati S, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-120. Gemba KL, 2017, J ACOUST SOC AM, V141, P3411, DOI 10.1121/1.4983467. Gerstoft P, 2016, IEEE SIGNAL PROC LET, V23, P1469, DOI 10.1109/LSP.2016.2598550. Gerstoft P, 2015, J ACOUST SOC AM, V138, P2003, DOI 10.1121/1.4929941. Giri R, 2016, INT CONF ACOUST SPEE, P514, DOI 10.1109/ICASSP.2016.7471728. Giri R, 2016, IEEE T SIGNAL PROCES, V64, P3418, DOI 10.1109/TSP.2016.2546231. He ZS, 2007, IEEE T AUDIO SPEECH, V15, P1551, DOI 10.1109/TASL.2007.898457. Jensen JR, 2013, INT CONF ACOUST SPEE, P3900, DOI 10.1109/ICASSP.2013.6638389. Kayser H, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/298605. Keyrouz F, 2014, IEEE T INSTRUM MEAS, V63, P2098, DOI 10.1109/TIM.2014.2308051. Koochakzadeh A, 2017, INT CONF ACOUST SPEE, P3081, DOI 10.1109/ICASSP.2017.7952723. Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899. Kuklasinski A, 2014, EUR SIGNAL PR CONF, P61. Liu Z., 2012, IEEE T WIREL COMMUN, V11, P1. MacDonald JA, 2008, J ACOUST SOC AM, V123, P4290, DOI 10.1121/1.2909566. Meyer J, 2001, J ACOUST SOC AM, V109, P185, DOI 10.1121/1.1329616. Nadarajah S, 2005, J APPL STAT, V32, P685, DOI 10.1080/02664760500079464. Nannuru S., 2017, ARXIV170400436V2. Pereira A, 2015, APPL ACOUST, V97, P11, DOI 10.1016/j.apacoust.2015.03.008. Quatieri T. F., 2006, SIGNAL PROCESS. Taal CH, 2011, IEEE T AUDIO SPEECH, V19, P2125, DOI 10.1109/TASL.2011.2114881. Tang ZJ, 2011, IEEE T SIGNAL PROCES, V59, P3464, DOI 10.1109/TSP.2011.2140108. Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267. Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236. Van Trees H. L., 2002, OPTIMUM ARRAY PROCES. Wipf D. P., 2007, P 24 INT C MACH LEAR, P1023, DOI DOI 10.1145/1273496.1273625). Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016. Wu XH, 2016, IEEE SENS J, V16, P2004, DOI 10.1109/JSEN.2015.2508059. Xenaki A, 2016, J ACOUST SOC AM, V140, P1828, DOI 10.1121/1.4962325. Xenaki A, 2014, J ACOUST SOC AM, V136, P260, DOI 10.1121/1.4883360. Zhang EL, 2012, J ACOUST SOC AM, V132, P3240, DOI 10.1121/1.4754530. Zotkin DN, 2006, J ACOUST SOC AM, V120, P2202, DOI 10.1121/1.2207578.},
  da = {2018-10-18},
  doc-delivery-number = {GL3LI},
  eissn = {1520-8524},
  funding-acknowledgement = {Innovation Fund Denmark [99-2014-1]},
  funding-text = {This work was supported by the Innovation Fund Denmark, under Grant No. 99-2014-1.},
  journal-iso = {J. Acoust. Soc. Am.},
  keywords-plus = {FIELD ACOUSTIC HOLOGRAPHY; REGULARIZATION; REPRESENTATION; SEPARATION; SELECTION},
  langid = {english},
  number = {6},
  number-of-cited-references = {43},
  oa = {green_published},
  research-areas = {Acoustics; Audiology & Speech-Language Pathology},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000437036000081},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {7},
  web-of-science-categories = {Acoustics; Audiology & Speech-Language Pathology}
}

@article{xiaohongshengMaximumLikelihoodMultiplesource2005,
  title = {Maximum Likelihood Multiple-Source Localization Using Acoustic Energy Measurements with Wireless Sensor Networks},
  author = {{Xiaohong Sheng} and {Yu-Hen Hu}},
  date = {2005-01},
  journaltitle = {IEEE Transactions on Signal Processing},
  shortjournal = {IEEE Trans. Signal Process.},
  volume = {53},
  pages = {44--53},
  issn = {1053-587X},
  doi = {10.1109/TSP.2004.838930},
  url = {http://ieeexplore.ieee.org/document/1369649/},
  urldate = {2019-08-16},
  abstract = {A maximum likelihood (ML) acoustic source location estimation method is presented for the application in a wireless ad hoc sensor network. This method uses acoustic signal energy measurements taken at individual sensors of an ad hoc wireless sensor network to estimate the locations of multiple acoustic sources. Compared to the existing acoustic energy based source localization methods, this proposed ML method delivers more accurate results and offers the enhanced capability of multiple source localization. A multiresolution search algorithm and an expectation–maximization (EM) like iterative algorithm are proposed to expedite the computation of source locations. The Cramér–Rao Bound (CRB) of the ML source location estimate has been derived. The CRB is used to analyze the impacts of sensor placement to the accuracy of location estimates for single target scenario. Extensive simulations have been conducted. It is observed that the proposed ML method consistently outperforms existing acoustic energy based source localization methods. An example applying this method to track military vehicles using real world experiment data also demonstrates the performance advantage of this proposed method over a previously proposed acoustic energy source localization method.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\XZS5F926\\Xiaohong Sheng and Yu-Hen Hu - 2005 - Maximum likelihood multiple-source localization us.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{xiaoLearningbasedApproachDirection2015,
  title = {A Learning-Based Approach to Direction of Arrival Estimation in Noisy and Reverberant Environments},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Xiao, X. and Zhao, S. and Zhong, X. and Jones, D. L. and Chng, E. S. and Li, H.},
  date = {2015-04},
  pages = {2814--2818},
  doi = {10.1109/ICASSP.2015.7178484},
  abstract = {This paper presents a learning-based approach to the task of direction of arrival estimation (DOA) from microphone array input. Traditional signal processing methods such as the classic least square (LS) method rely on strong assumptions on signal models and accurate estimations of time delay of arrival (TDOA) . They only work well in relatively clean conditions, but suffer from noise and reverberation distortions. In this paper, we propose a learning-based approach that can learn from a large amount of simulated noisy and reverberant microphone array inputs for robust DOA estimation. Specifically, we extract features from the generalised cross correlation (GCC) vectors and use a multilayer perceptron neural network to learn the nonlinear mapping from such features to the DOA. One advantage of the learning based method is that as more and more training data becomes available, the DOA estimation will become more and more accurate. Experimental results on simulated data show that the proposed learning based method produces much better results than the state-of-the-art LS method. The testing results on real data recorded in meeting rooms show improved root-mean-square error (RMSE) compared to the LS method.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\9QVV6A96\\Xiao et al. - 2015 - A learning-based approach to direction of arrival .pdf;C\:\\Users\\sauli\\Zotero\\storage\\6WLA72V6\\7178484.html},
  keywords = {acoustic signal processing,Arrays,direction of arrival,direction of arrival estimation,direction-of-arrival estimation,Direction-of-arrival estimation,Estimation,feature extraction,GCC,generalised cross correlation vectors,learning (artificial intelligence),learning-based approach,least squares,machine learning,microphone arrays,multilayer perceptron neural network,multilayer perceptrons,neural networks,noisy environment,nonlinear mapping,reverberant environment,reverberant microphone array,reverberation,robust DOA estimation,Robustness,Speech,Training,Training data,vectors}
}

@article{xiaoStudyLearningBased2016,
  title = {A {{Study}} of {{Learning Based Beamforming Methods}} for {{Speech Recognition}}},
  author = {Xiao, Xiong and Xu, Chenglin and Zhang, Zhaofeng and Zhao, Shengkui and Sun, Sining and Watanabe, Shinji and Wang, Longbiao and Xie, Lei and Jones, Douglas L and Chng, Eng Siong and Li, Haizhou},
  date = {2016},
  pages = {6},
  abstract = {This paper presents a comparative study of three learning based beamforming methods that are specifically designed for robust speech recognition. The three methods are 1) neural network that predicts beamforming weights from generalized cross correlation (GCC) features; 2) neural network that predicts timefrequency (TF) mask which is used to estimate MVDR (minimum variance distortionless response) beamforming weights; 3) maximum likelihood estimation of beamforming weights to fit enhanced features to clean trained Gaussian mixture model. All three methods operate in frequency domain. They are evaluated on the CHiME-4 benchmarking speech recognition task and compared with traditional delay-and-sum and MVDR beamforming methods on the same speech recognition task. Discussions and future research directions are presented.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HRS7Z852\\Xiao et al. - A Study of Learning Based Beamforming Methods for .pdf},
  langid = {english}
}

@article{xingengSupervisedNonlinearDimensionality2005,
  title = {Supervised Nonlinear Dimensionality Reduction for Visualization and Classification},
  author = {Xin Geng and De-Chuan Zhan and Zhi-Hua Zhou},
  date = {2005-12},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {35},
  pages = {1098--1107},
  issn = {1941-0492},
  doi = {10.1109/TSMCB.2005.850151},
  abstract = {When performing visualization and classification, people often confront the problem of dimensionality reduction. Isomap is one of the most promising nonlinear dimensionality reduction techniques. However, when Isomap is applied to real-world data, it shows some limitations, such as being sensitive to noise. In this paper, an improved version of Isomap, namely S-Isomap, is proposed. S-Isomap utilizes class information to guide the procedure of nonlinear dimensionality reduction. Such a kind of procedure is called supervised nonlinear dimensionality reduction. In S-Isomap, the neighborhood graph of the input data is constructed according to a certain kind of dissimilarity between data points, which is specially designed to integrate the class information. The dissimilarity has several good properties which help to discover the true neighborhood of the data and, thus, makes S-Isomap a robust technique for both visualization and classification, especially for real-world problems. In the visualization experiments, S-Isomap is compared with Isomap, LLE, and WeightedIso. The results show that S-Isomap performs the best. In the classification experiments, S-Isomap is used as a preprocess of classification and compared with Isomap, WeightedIso, as well as some other well-established classification methods, including the K-nearest neighbor classifier, BP neural network, J4.8 decision tree, and SVM. The results reveal that S-Isomap excels compared to Isomap and WeightedIso in classification, and it is highly competitive with those well-known classification methods.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\AU4X4QTY\\Xin Geng et al. - 2005 - Supervised nonlinear dimensionality reduction for .pdf;C\:\\Users\\sauli\\Zotero\\storage\\XPSZT55Y\\1542257.html},
  keywords = {Algorithms,Artificial Intelligence,classification,Classification,Classification tree analysis,Cluster Analysis,Computer Simulation,Data Display,Data visualization,Decision trees,dimensionality reduction,Face,graph theory,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Imaging; Three-Dimensional,Independent component analysis,Information Storage and Retrieval,Isomap,learning (artificial intelligence),LLE,manifold learning,Models; Biological,Models; Statistical,neighborhood graph,Neural networks,Nonlinear Dynamics,pattern classification,Pattern Recognition; Automated,Principal component analysis,real-world problems,Robustness,S-Isomap,Space technology,supervised learning,supervised nonlinear dimensionality reduction,Support vector machine classification,Support vector machines,User-Computer Interface,visualization,WeightedIso},
  number = {6}
}

@article{yaltaSoundSourceLocalization2017,
  title = {Sound Source Localization Using Deep Learning Models},
  author = {Yalta, Nelson and Nakadai, Kazuhiro and Ogata, Tetsuya},
  date = {2017-02-01},
  journaltitle = {Journal of Robotics and Mechatronics},
  volume = {29},
  pages = {37--48},
  issn = {0915-3942},
  doi = {10.20965/jrm.2017.p0037},
  url = {https://waseda.pure.elsevier.com/en/publications/sound-source-localization-using-deep-learning-models},
  urldate = {2018-11-14},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\2WFQIKIV\\sound-source-localization-using-deep-learning-models.html},
  langid = {english},
  number = {1}
}

@inproceedings{yang_sound_2016,
  title = {Sound Source Localization Method Based on {{LDA}} Classifier},
  booktitle = {2016 8th {{International Conference}} on {{Wireless Communications Signal Processing}} ({{WCSP}})},
  author = {Yang, Y. and Gu, X. and Wan, X.},
  date = {2016-10},
  pages = {1--5},
  doi = {10.1109/WCSP.2016.7752656},
  abstract = {Methods based on pattern recognition such as support vector machine or neural network, can overcome the deficiency of traditional sound source localization algorithms which have an insufficient robust ability in the harsh environment of small SNR and severe reverberation. Among those methods, Naive-Bayes classifier has a high accuracy to locate sound source with a small amount of calculation and strong robustness. This paper presents a method that use linear discriminant analysis (LDA) classifier to locate sound source with generalized cross correlation phase transform (GCC-PHAT) function as the feature vector. Simulation results have proved that in harsh environment, LDA classifier locating accuracy is higher than Naive-Bayes classifier by 2\%, especially in severe reverberation environment.},
  eventtitle = {2016 8th {{International Conference}} on {{Wireless Communications Signal Processing}} ({{WCSP}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\W7I62IX6\\Yang et al. - 2016 - Sound source localization method based on LDA clas.pdf;C\:\\Users\\sauli\\Zotero\\storage\\N2CVI6AA\\7752656.html},
  keywords = {acoustic radiators,acoustic signal processing,feature vector,GCC-PHAT,GCC-PHAT function,generalized cross correlation phase transform,LDA classifier,linear discriminant analysis classifier,microphone arrays,Naive-Bayes classifier,neural network,pattern recognition,reverberation,Robustness,severe reverberation environment,signal to noise ratio,sound source localization,sound source localization method,support vector machine,Telecommunications,Transforms}
}

@article{yangFastRobustRBF2018,
  title = {Fast and {{Robust RBF Neural Network Based}} on {{Global K}}-{{Means Clustering With Adaptive Selection Radius}} for {{Sound Source Angle Estimation}}},
  author = {Yang, X. and Li, Y. and Sun, Y. and Long, T. and Sarkar, T. K.},
  date = {2018-06},
  journaltitle = {IEEE Transactions on Antennas and Propagation},
  volume = {66},
  pages = {3097--3107},
  issn = {0018-926X},
  doi = {10.1109/TAP.2018.2823713},
  abstract = {The sound source localization technique is widely applied to target detection and localization. However, the application of conventional sound source localization methods is limited in actual environment because of estimation accuracy, computational complexity, and flexibility of the environment. In order to improve the sound source localization performance in actual environment, a fast and robust radial basis function (RBF) neural network based on global K-means clustering with adaptive selection radius is proposed in this paper. In the proposed method, an adaptive selection radius is calculated according to the population density sampling method to remove unnecessary points around cluster centers during the global K-means clustering; thus, compared with the conventional neural network, a fast optimization of hidden layer neuron parameters can be achieved. Afterward, the RBF neural network is trained to locate the sound source by solving nonlinear equations of the time difference of arrival and sound source location. Because of the adoption of adaptive selection radius in global K-means clustering, the proposed method can provide desirable performance with low computational complexity. Based on the simulated and actual experimental data, the proposed method is verified and an improved performance is achieved compared with that of conventional neural network sound source localization methods.},
  keywords = {acoustic signal processing,actual environment,adaptive selection radius,Adaptive selection radius,Adaptive systems,Biological neural networks,cluster centers,computational complexity,Estimation,global K-means clustering,hidden layer neuron parameters,Mathematical model,Microphones,neural network sound source localization methods,Neurons,nonlinear equations,object detection,pattern clustering,population density sampling method,radial basis function,radial basis function (RBF) neural network,radial basis function networks,robust RBF neural network,signal sampling,sound source angle estimation,sound source localization,target detection},
  number = {6}
}

@inproceedings{yangGraphRegularizedDeep2017,
  title = {A {{Graph Regularized Deep Neural Network}} for {{Unsupervised Image Representation Learning}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Shijie and Li, Liang and Wang, Shuhui and Zhang, Weigang and Huang, Qingming},
  date = {2017-07},
  pages = {7053--7061},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.746},
  url = {http://ieeexplore.ieee.org/document/8100229/},
  urldate = {2019-12-11},
  abstract = {Deep Auto-Encoder (DAE) has shown its promising power in high-level representation learning. From the perspective of manifold learning, we propose a graph regularized deep neural network (GR-DNN) to endue traditional DAEs with the ability of retaining local geometric structure. A deep-structured regularizer is formulated upon multi-layer perceptions to capture this structure. The robust and discriminative embedding space is learned to simultaneously preserve the high-level semantics and the geometric structure within local manifold tangent space. Theoretical analysis presents the close relationship between the proposed graph regularizer and the graph Laplacian regularizer in terms of the optimization objective. We also alleviate the growth of the network complexity by introducing the anchor-based bipartite graph, which guarantees the good scalability for large scale data. The experiments on four datasets show the comparable results of the proposed GR-DNN with the state-of-the-art methods.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EHWW6JYB\\Yang et al. - 2017 - A Graph Regularized Deep Neural Network for Unsupe.pdf},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@incollection{yanSoundFieldModelling2015,
  title = {Sound {{Field Modelling}} and {{Multi}}-Objective {{Optimization}} for {{Workshop Layout}}},
  booktitle = {Proceedings of the 5th {{International Asia Conference}} on {{Industrial Engineering}} and {{Management Innovation}} ({{IEMI2014}})},
  author = {Yan, Ji-hong and Lv, Bo-han and Zhao, Li-zhong and Suo, Lai-chun},
  date = {2015},
  pages = {263--268},
  publisher = {{Atlantis Press, Paris}},
  doi = {10.2991/978-94-6239-100-0_49},
  url = {https://link.springer.com/chapter/10.2991/978-94-6239-100-0_49},
  urldate = {2017-12-08},
  abstract = {Manufacturing enterprise mainly concerns makespan or efficiency of logistics in the process of workshop layout planning. However, environmental impact of noise which also has direct influence on staff productivity is not always considered to improve the productivity of enterprise. Guided by experience, the implementation is always based on pre-existing planning and layout principle, but do not provide quantitative analysis of environmental noise through scientific method. This paper proposes an effective approach to analyze environmental noise quantitatively. Mathematical model of noise on the condition of multiple sound sources is built. Then the numerical simulation and quantitative analysis to environmental acoustic field of workshop is presented. Finally, the double objective function of Flexible Assembly Workshop is established to obtain shortest logistic time and minimum noise, which is solved based on Genetic Algorithm. This work provides a new idea for workshop layout planning to reduce environmental impact of noise.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MHH8TQ53\\978-94-6239-100-0_49.html},
  isbn = {978-94-6239-099-7 978-94-6239-100-0},
  langid = {english},
  series = {Proceedings of the {{International Asia Conference}} on {{Industrial Engineering}} and {{Management Innovation}}}
}

@article{yiSemiSupervisedRidgeRegression2018,
  title = {Semi-{{Supervised Ridge Regression}} with {{Adaptive Graph}}-{{Based Label Propagation}}},
  author = {Yi, Yugen and Chen, Yuqi and Dai, Jiangyan and Gui, Xiaolin and Chen, Chunlei and Lei, Gang and Wang, Wenle},
  date = {2018-12-16},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {8},
  pages = {2636},
  doi = {10.3390/app8122636},
  abstract = {In order to overcome the drawbacks of the ridge regression and label propagation algorithms, we propose a new semi-supervised classification method named semi-supervised ridge regression with adaptive graph-based label propagation (SSRR-AGLP). Firstly, we present a new adaptive graph-learning scheme and integrate it into the procedure of label propagation, in which the locality and sparsity of samples are considered simultaneously. Then, we introduce the ridge regression algorithm into label propagation to solve the “out of sample” problem. As a consequence, the proposed SSSRR-AGLP integrates adaptive graph learning, label propagation and ridge regression into a unified framework. Finally, an effective iterative updating algorithm is designed for solving the algorithm, and the convergence analysis is also provided. Extensive experiments are conducted on five databases. Through comparing the results with some well-known algorithms, the effectiveness and superiority of the proposed algorithm are demonstrated.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PKWN8Z7A\\Yi et al. - 2018 - Semi-Supervised Ridge Regression with Adaptive Gra.pdf},
  keywords = {****}
}

@article{yongeAESStandardNetwork2003,
  title = {{{AES}} Standard for Network and File Transfer of Audio-{{Audio}}-File Transfer and Exchange-{{Radio}} Traffic Audio Delivery Extension to the {{Broadcast Wave}} File Format},
  author = {Yonge, M. and Harris, B.},
  date = {2003},
  journaltitle = {JOURNAL OF THE AUDIO ENGINEERING SOCIETY},
  volume = {51},
  pages = {369--383},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\3MWTFH7Q\\aes67-2013-f.pdf},
  number = {5}
}

@article{yookFastSoundSource2016,
  title = {Fast {{Sound Source Localization Using Two}}-{{Level Search Space Clustering}}},
  author = {Yook, D. and Lee, T. and Cho, Y.},
  date = {2016-01},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {46},
  pages = {20--26},
  issn = {2168-2267},
  doi = {10.1109/TCYB.2015.2391252},
  abstract = {Steered response power phase transform (SRP-PHAT) is a method that is widely used for robust sound source localization (SSL). However, since SRP-PHAT searches over a large number of candidate locations, it is too slow to run in real-time for large-scale microphone array systems. In this paper, we propose a robust two-level search space clustering method to speed-up SRP-PHAT-based SSL. The proposed method divides the candidate locations of the sound source into a set of groups and finds a small number of groups that are likely to contain the maximum power location. By searching within the small number of groups, the computational costs are reduced by 61.8\% compared to a previously proposed method without loss of accuracy.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\PFZBXCMM\\Yook et al. - 2016 - Fast Sound Source Localization Using Two-Level Sea.pdf;C\:\\Users\\sauli\\Zotero\\storage\\46ZZQ55E\\7039285.html},
  keywords = {Accuracy,acoustic signal processing,Arrays,Computational efficiency,large-scale microphone array systems,microphone arrays,Microphones,Power generation,Search methods,search problems,search space clustering,Search space clustering (SSC),sound source localization,sound source localization (SSL),steered response power phase transform,steered response power phase transform (SRP-PHAT),Table lookup},
  number = {1}
}

@inproceedings{younAdaptiveRealizationPhase1984,
  title = {Adaptive Realization of the Phase Transform for Time Delay Estimation},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, {{IEEE International Conference}} on {{ICASSP}} '84.},
  author = {Youn, D. and Chiou, S. and Mathews, V.},
  date = {1984-03},
  volume = {9},
  pages = {648--651},
  doi = {10.1109/ICASSP.1984.1172298},
  abstract = {This paper introduces two recursive realizations of the Phase Transform (PHAT) processor for time delay estimation (TDE), using a simple one-pole lowpass filter and the least mean square (LMS) adaptive filter, respectively. It is shown that these adaptive methods are very effective in reducing the effect of interfering tonals. The performances of these methods are compared with those of other existing adaptive TDE algorithms via computer simulations.},
  eventtitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, {{IEEE International Conference}} on {{ICASSP}} '84.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\CRTMIKAJ\\Youn et al. - 1984 - Adaptive realization of the phase transform for ti.pdf;C\:\\Users\\sauli\\Zotero\\storage\\V2U2S22P\\1172298.html},
  keywords = {adaptive filters,Delay effects,Delay estimation,Fourier transforms,Frequency,Least squares approximation,Phase estimation,Signal processing algorithms,Wiener filter,Yield estimation}
}

@inproceedings{YounAdaptiverealizationphase1984,
  title = {Adaptive {{Realization}} of the {{Phase Transform}} for {{Time Delay Estimation}}},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, {{IEEE International Conference}} on {{ICASSP}} '84.},
  author = {Youn, D. and Chiou, S. and Mathews, V.},
  date = {1984-03},
  volume = {9},
  pages = {648--651},
  doi = {10.1109/ICASSP.1984.1172298},
  abstract = {This paper introduces two recursive realizations of the Phase Transform (PHAT) processor for time delay estimation (TDE), using a simple one-pole lowpass filter and the least mean square (LMS) adaptive filter, respectively. It is shown that these adaptive methods are very effective in reducing the effect of interfering tonals. The performances of these methods are compared with those of other existing adaptive TDE algorithms via computer simulations.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZTZFTIJZ\\Youn et al. - 1984 - Adaptive realization of the phase transform for ti.pdf;C\:\\Users\\sauli\\Zotero\\storage\\4KUAKXC3\\1172298.html},
  keywords = {adaptive filters,Delay effects,Delay estimation,Fourier transforms,Frequency,Least squares approximation,Phase estimation,Signal processing algorithms,Wiener filter,Yield estimation}
}

@article{zakarauskasArtificialNeuralNetworks1991,
  title = {Artificial Neural Networks for Simultaneous and Independent Range and Depth Discrimination in Passive Acoustic Localization.},
  author = {Zakarauskas, Pierre and Ozard, John M. and Brouwer, Peter},
  date = {1991-10-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {90},
  pages = {2366--2366},
  issn = {0001-4966},
  doi = {10.1121/1.402117},
  url = {https://asa.scitation.org/doi/10.1121/1.402117},
  urldate = {2018-10-19},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\MZ6G6XVJ\\Zakarauskas et al. - 1991 - Artificial neural networks for simultaneous and in.pdf;C\:\\Users\\sauli\\Zotero\\storage\\AV2Y4GV3\\1.html},
  number = {4}
}

@article{zhangBiologicallyInspiredCoupled2018,
  title = {A {{Biologically Inspired Coupled Microphone Array}} for {{Sound Source Bearing Estimation}}},
  author = {Zhang, Yaqiong and Yang, Ming and Zhu, Xinlei and Ta, Na and Rao, Zhushi},
  date = {2018-02},
  journaltitle = {JOURNAL OF VIBRATION AND ACOUSTICS-TRANSACTIONS OF THE ASME},
  volume = {140},
  publisher = {ASME},
  location = {TWO PARK AVE, NEW YORK, NY 10016-5990 USA},
  issn = {1048-9002},
  doi = {\{10.1115/1.4037852\}},
  abstract = {The Ormia ochracea, a species of parasitic fly, has a remarkable localization ability despite the tiny interaural distance compared with the incoming wavelength. The mechanical coupling between its ears enhances the differences of the two received signals, the main cues to locate the source. Inspired by the coupling mechanism, we present a miniature coupled two-microphone array for estimating sound source horizontal bearing. The coupled array consists of a standard two-microphone array and a two-input, two-output filter which implements the coupling. The relationship between filter parameters and time delay magnification is investigated to provide theoretical support for array design. With appropriate parameters, the time delay of received signals can be linearly magnified. Based on the linear magnification, we present a method for estimating source direction using the coupled array. The influence of time delay magnification on time delay estimation accuracy is explored through the general cross-correlation (GCC) method. Experiments are conducted to verify the coupled array and demonstrate its advantages on improving the resolution of estimation of time delay and accuracy of bearing estimation compared with the standard array with the same element spacing.},
  affiliation = {Rao, ZS (Reprint Author), Shanghai Jiao Tong Univ, Sch Mech Engn, Inst Vibrat Shock & Noise, State Key Lab Mech Syst & Vibrat, Shanghai 200240, Peoples R China. Zhang, Yaqiong; Yang, Ming; Zhu, Xinlei; Ta, Na; Rao, Zhushi, Shanghai Jiao Tong Univ, Sch Mech Engn, Inst Vibrat Shock & Noise, State Key Lab Mech Syst & Vibrat, Shanghai 200240, Peoples R China.},
  article-number = {011019},
  author-email = {yaoyaosjtu@sjtu.edu.cn Young1@sjtu.edu.cn xl.zhu@sjtu.edu.cn wutana@sjtu.edu.cn zsrao@sjtu.edu.cn},
  cited-references = {An P, 2009, SENSORS-BASEL, V9, P5637, DOI 10.3390/s90705637. Behdad N, 2010, ELECTRON LETT, V46, P1650, DOI 10.1049/el.2010.8034. Behdad N, 2011, IEEE ANTENN WIREL PR, V10, P361, DOI 10.1109/LAWP.2011.2146223. Chen CC, 2012, IEEE SENS J, V12, P1504, DOI 10.1109/JSEN.2011.2173931. Lee J. H., 2015, ASME J VIB ACOUST, V137. Liu HJ, 2008, APPL PHYS LETT, V93, DOI 10.1063/1.3043724. Liu HJ, 2013, SCI REP-UK, V3, DOI 10.1038/srep02489. Miles R., 1997, AUDITORY MECH, P18. Miles RN, 2009, J ACOUST SOC AM, V125, P2013, DOI 10.1121/1.3082118. Miles R. N., 2016, ASME J VIB ACOUST, V138. Miles RN, 1995, J ACOUST SOC AM, V98, P3059, DOI 10.1121/1.413830. Miles RN, 2006, AUDIOL NEURO-OTOL, V11, P86, DOI 10.1159/000090681. Robert D, 1996, J COMP PHYSIOL A, V179, P29. Robert D, 1998, J COMP PHYSIOL A, V183, P443, DOI 10.1007/s003590050270. Xu HP, 2015, J ACOUST SOC AM, V138, pEL270, DOI 10.1121/1.4929735.},
  da = {2018-10-18},
  doc-delivery-number = {FX1GJ},
  eissn = {1528-8927},
  journal-iso = {J. Vib. Acoust.-Trans. ASME},
  keywords-plus = {FLY ORMIA-OCHRACEA; DIRECTIONAL HEARING; SOURCE LOCALIZATION; EARS},
  langid = {english},
  number = {1},
  number-of-cited-references = {15},
  research-areas = {Acoustics; Engineering; Mechanics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000425796700019},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Engineering, Mechanical; Mechanics}
}

@article{zhangLearningMetricsDiscriminant,
  title = {Learning {{Metrics}} via {{Discriminant Kernels}} and {{Multidimensional Scaling}}: {{Toward Expected Euclidean Representation}}},
  author = {Zhang, Zhihua},
  pages = {8},
  abstract = {Distance-based methods in machine learning and pattern recognition have to rely on a metric distance between points in the input space. Instead of specifying a metric a priori, we seek to learn the metric from data via kernel methods and multidimensional scaling (MDS) techniques. Under the classification setting, we define discriminant kernels on the joint space of input and output spaces and present a specific family of discriminant kernels. This family of discriminant kernels is attractive because the induced metrics are Euclidean and Fisher separable, and MDS techniques can be used to find the lowdimensional Euclidean representations (also called feature vectors) of the induced metrics. Since the feature vectors incorporate information from both input points and their corresponding labels and they enjoy Fisher separability, they are appropriate to be used in distance-based classifiers.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ZK24GBU4\\Zhang - Learning Metrics via Discriminant Kernels and Mult.pdf},
  langid = {english}
}

@inproceedings{zhangMaximumLikelihoodSound2007,
  title = {Maximum {{Likelihood Sound Source Localization}} for {{Multiple Directional Microphones}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Zhang, C. and Zhang, Z. and Florencio, D.},
  date = {2007-04},
  volume = {1},
  pages = {I-125-I-128},
  doi = {10.1109/ICASSP.2007.366632},
  abstract = {This paper presents a maximum likelihood (ML) framework for multi-microphone sound source localization (SSL). Besides deriving the framework, we focus on making the connection and contrast between the ML-based algorithm and popular steered response power (SRP) SSL algorithms such as phase transform (SRP-PHAT). We also show under our ML framework how challenging conditions such as directional microphone arrays and reverberations can be handled. The computational cost of our method is low-similar to SRP-PHAT. The effectiveness of the proposed method is shown on a large dataset with 99 real-world audio sequences recorded by directional circular microphone arrays in over 50 different meeting rooms.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\F9MQJ6ZA\\Zhang et al. - 2007 - Maximum Likelihood Sound Source Localization for M.pdf;C\:\\Users\\sauli\\Zotero\\storage\\XMA9KWPM\\4217032.html},
  keywords = {Acoustic noise,acoustic signal processing,audio sequences,Computational efficiency,Computational intelligence,Delay effects,Delay estimation,directional circular microphone arrays,Frequency domain analysis,maximum likelihood,Maximum likelihood estimation,maximum likelihood sound source localization,microphone arrays,multimicrophone sound source localization,multiple directional circular microphone arrays,phase transform,reverberation,sound source localization,Steered response power,Working environment noise}
}

@inproceedings{zhangMaximumLikelihoodSound2007a,
  title = {Maximum {{Likelihood Sound Source Localization}} for {{Multiple Directional Microphones}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  author = {Zhang, C. and Zhang, Z. and Florencio, D.},
  date = {2007-04},
  volume = {1},
  pages = {I-125-I-128},
  doi = {10.1109/ICASSP.2007.366632},
  abstract = {This paper presents a maximum likelihood (ML) framework for multi-microphone sound source localization (SSL). Besides deriving the framework, we focus on making the connection and contrast between the ML-based algorithm and popular steered response power (SRP) SSL algorithms such as phase transform (SRP-PHAT). We also show under our ML framework how challenging conditions such as directional microphone arrays and reverberations can be handled. The computational cost of our method is low-similar to SRP-PHAT. The effectiveness of the proposed method is shown on a large dataset with 99 real-world audio sequences recorded by directional circular microphone arrays in over 50 different meeting rooms.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{ICASSP}} '07},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\5NQDB2ZZ\\Zhang et al_2007_Maximum Likelihood Sound Source Localization for Multiple Directional.pdf;C\:\\Users\\sauli\\Zotero\\storage\\KCA245IE\\4217032.html},
  keywords = {Acoustic noise,acoustic signal processing,audio sequences,Computational efficiency,Computational intelligence,Delay effects,Delay estimation,directional circular microphone arrays,Frequency domain analysis,maximum likelihood,maximum likelihood estimation,Maximum likelihood estimation,maximum likelihood sound source localization,microphone arrays,Microphone arrays,multimicrophone sound source localization,multiple directional circular microphone arrays,phase transform,Reverberation,Sound source localization,steered response power,Working environment noise}
}

@article{zhangMaximumLikelihoodSound2008,
  title = {Maximum {{Likelihood Sound Source Localization}} and {{Beamforming}} for {{Directional Microphone Arrays}} in {{Distributed Meetings}}},
  author = {Zhang, C. and Florencio, D. and Ba, D. E. and Zhang, Z.},
  date = {2008-04},
  journaltitle = {IEEE Transactions on Multimedia},
  volume = {10},
  pages = {538--548},
  issn = {1520-9210},
  doi = {10.1109/TMM.2008.917406},
  abstract = {In distributed meeting applications, microphone arrays have been widely used to capture superior speech sound and perform speaker localization through sound source localization (SSL) and beamforming. This paper presents a unified maximum likelihood framework of these two techniques, and demonstrates how such a framework can be adapted to create efficient SSL and beamforming algorithms for reverberant rooms and unknown directional patterns of microphones. The proposed method is closely related to steered response power-based algorithms, which are known to work extremely well in real-world environments. We demonstrate the effectiveness of the proposed method on challenging synthetic and real-world datasets, including over six hours of recorded meetings.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\ARJN9W4H\\Zhang et al. - 2008 - Maximum Likelihood Sound Source Localization and B.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZCWSDFV7\\4459286.html},
  keywords = {Beamforming,beamforming algorithms,directional microphone arrays,directional mics,distributed meetings,maximum likelihood detection,maximum likelihood sound source localization,microphone array,microphone arrays,reverberant rooms,reverberation,sound source localization,steered response power based algorithms},
  number = {3}
}

@article{zhangUnderwaterSourceLocalization2018,
  title = {Underwater {{Source Localization Using TDOA}} and {{FDOA Measurements With Unknown Propagation Speed}} and {{Sensor Parameter Errors}}},
  author = {Zhang, Bingbing and Hu, Yongchang and Wang, Hongyi and Zhuang, Zhaowen},
  date = {2018},
  journaltitle = {IEEE ACCESS},
  volume = {6},
  pages = {\{36645-36661\}},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  location = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  issn = {2169-3536},
  doi = {\{10.1109/ACCESS.2018.2852636\}},
  abstract = {Underwater source localization problems are complicated and challenging: 1) the sound propagation speed is often unknown and the unpredictable ocean current might lead to the uncertainties of sensor parameters (i.e., position and velocity); 2) the underwater acoustic signal travels much slower than the radio one in terrestrial environments, thus resulting into a significantly severe Doppler effect; and 3) energy-efficient techniques are urgently required and hence in favor of the design with a low computational complexity. Considering these issues, we propose a simple and efficient underwater source localization approach based on the time difference of arrival and frequency difference of arrival measurements, which copes with unknown propagation speed and sensor parameter errors. The proposed method mitigates the impact of the Doppler effect for accurately inferring the source parameters (i.e., position and velocity). The Cramer-Rao lower bounds (CRLBs) for this kind of localization are derived and, moreover, the analytical study shows that our method can yield the performance that is very close to the CRLB, particularly under small noise. The numerical results not only confirm the above conclusions but also show that our method outperforms other competing approaches.},
  affiliation = {Zhang, BB (Reprint Author), Natl Univ Def Technol, Sch Elect Sci, Changsha 410073, Hunan, Peoples R China. Zhang, Bingbing; Wang, Hongyi; Zhuang, Zhaowen, Natl Univ Def Technol, Sch Elect Sci, Changsha 410073, Hunan, Peoples R China. Hu, Yongchang, Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2624 AL Delft, Netherlands.},
  author-email = {zbbzb@nudt.edu.cn},
  cited-references = {Amiri R, 2017, IEEE COMMUN LETT, V21, P2222, DOI 10.1109/LCOMM.2017.2724541. Boyd S., 2004, CONVEX OPTIMIZATION. CHAN YT, 1994, IEEE T SIGNAL PROCES, V42, P1905, DOI 10.1109/78.301830. CHEN CT, 1977, J ACOUST SOC AM, V62, P1129, DOI 10.1121/1.381646. Cheung KW, 2004, IEEE T SIGNAL PROCES, V52, P1121, DOI 10.1109/TSP.2004.823465. Diamant R, 2013, IEEE T MOBILE COMPUT, V12, P1257, DOI 10.1109/TMC.2012.100. FOY WH, 1976, IEEE T AERO ELEC SYS, V12, P187, DOI 10.1109/TAES.1976.308294. Fujisawa K, 1997, MATH PROGRAM, V79, P235, DOI 10.1007/BF02614319. Han GJ, 2015, IEEE T IND ELECTRON, V62, P1725, DOI 10.1109/TIE.2014.2362731. Heidemann J, 2012, PHILOS T R SOC A, V370, P158, DOI 10.1098/rsta.2011.0214. Ho KC, 2007, IEEE T SIGNAL PROCES, V55, P684, DOI 10.1109/TSP.2006.885744. Ho KC, 2004, IEEE T SIGNAL PROCES, V52, P2453, DOI 10.1109/TSP.2004.831921. Hu YC, 2017, IEEE T SIGNAL PROCES, V65, P3261, DOI 10.1109/TSP.2017.2684741. Kay S. M., 1993, ESTIMATION THEORY. Liang QL, 2013, IEEE T PARALL DISTR, V24, P2100, DOI 10.1109/TPDS.2012.310. Lui KWK, 2009, IEEE T SIGNAL PROCES, V57, P752, DOI 10.1109/TSP.2008.2007916. MACKENZIE KV, 1981, J ACOUST SOC AM, V70, P807, DOI 10.1121/1.386920. Mason SF, 2008, IEEE J SEL AREA COMM, V26, P1638, DOI 10.1109/JSAC.2008.081204. Messer H, 2006, PR IEEE SEN ARRAY, P304. Noam Y., 2008, P IEEE WORKSH SENS A, P395. Noroozi A., 2017, IEEE COMMUN LETT, V7, P352. Ramezani H, 2015, IEEE T WIREL COMMUN, V14, P2584, DOI 10.1109/TWC.2015.2389220. Ramezani H, 2013, IEEE T SIGNAL PROCES, V61, P1434, DOI 10.1109/TSP.2012.2235432. Rui LY, 2015, IEEE T AERO ELEC SYS, V51, P600, DOI 10.1109/TAES.2014.140482. Sandys-Wunsch M, 2002, IEEE J OCEANIC ENG, V27, P328, DOI 10.1109/JOE.2002.1002488. Stojanovic M, 2009, IEEE COMMUN MAG, V47, P84, DOI 10.1109/MCOM.2009.4752682. Sun M, 2011, IEEE T SIGNAL PROCES, V59, P3434, DOI 10.1109/TSP.2011.2131135. Trees H. L. V., 2007, BAYESIAN BOUNDS PARA. Wang G, 2013, IEEE T VEH TECHNOL, V62, P853, DOI 10.1109/TVT.2012.2225074. Wang Y, 2018, IEEE T WIREL COMMUN, V17, P1242, DOI 10.1109/TWC.2017.2777457. Wang YL, 2017, IEEE COMMUN LETT, V21, P80, DOI 10.1109/LCOMM.2016.2614936. Wen JM, 2017, IEEE T SIGNAL PROCES, V65, P1370, DOI 10.1109/TSP.2016.2634550. Wen JM, 2017, IEEE T INFORM THEORY, V63, P631, DOI 10.1109/TIT.2016.2627082. Xu T, 2016, INT CONF ACOUST SPEE, P3906, DOI 10.1109/ICASSP.2016.7472409. Yu HG, 2012, IEEE T WIREL COMMUN, V11, P44, DOI 10.1109/TWC.2011.102611.110728. Zhang BB, 2017, IEEE ACCESS, V5, P26932, DOI 10.1109/ACCESS.2017.2778425. Zhang BB, 2017, IEEE ACCESS, V5, P18016, DOI 10.1109/ACCESS.2017.2750322. Zheng J, 2007, SIGNAL PROCESS, V87, P3096, DOI 10.1016/j.sigpro.2007.06.014.},
  da = {2018-10-18},
  doc-delivery-number = {GO1GT},
  funding-acknowledgement = {Hunan Science and Technology Project [2016JC2008]; China Postdoctoral Science Foundation [2016T90978]; National Nature Science Foundation of China [61604176]},
  funding-text = {This work was supported in part by the Hunan Science and Technology Project under Grant 2016JC2008, in part by the China Postdoctoral Science Foundation under Grant 2016T90978, and in part by the National Nature Science Foundation of China under Grant 61604176.},
  journal-iso = {IEEE Access},
  keywords = {(Underwater localization,algebraic solution,frequency difference of arrival (FDOA)),sensor node uncertainty,sound propagation speed  uncertainty,time difference of arrival (TDOA)},
  keywords-plus = {ACOUSTIC COMMUNICATION; EFFICIENT ESTIMATOR; SOUND SPEED; LOCATION; NETWORKS; UNCERTAINTIES; ALGORITHMS; TRACKING; POSITION; MODELS},
  langid = {english},
  number-of-cited-references = {38},
  oa = {gold},
  orcid-numbers = {Hu, Yongchang/0000-0002-9623-9555},
  research-areas = {Computer Science; Engineering; Telecommunications},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000439698700015},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications}
}

@article{zhangViewAdaptiveRecurrent2017,
  title = {View {{Adaptive Recurrent Neural Networks}} for {{High Performance Human Action Recognition}} from {{Skeleton Data}}},
  author = {Zhang, Pengfei and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Xue, Jianru and Zheng, Nanning},
  date = {2017-03-23},
  url = {http://arxiv.org/abs/1703.08274},
  urldate = {2018-04-12},
  abstract = {Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves significant improvement over the state-of-the-art approaches on three benchmark datasets.},
  archivePrefix = {arXiv},
  eprint = {1703.08274},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\GZP3PM6G\\Zhang et al. - 2017 - View Adaptive Recurrent Neural Networks for High P.pdf;C\:\\Users\\sauli\\Zotero\\storage\\IPBL2P55\\1703.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{zhengchengLIDARwoYongita3CiYuanDiTuShengChengniokeruWeiGuanCeLingYuXueJiannotamenoShiDianXuanZe2018,
  title = {LIDARを用いた3次元地図生成における未観測領域削減のための視点選択},
  author = {正誠, 井内 and 修士, 大石 and 純, 三浦},
  date = {2018},
  pages = {1A1-I01},
  publisher = {{一般社団法人 日本機械学会}},
  doi = {10.1299/jsmermd.2018.1A1-I01},
  url = {https://www.jstage.jst.go.jp/article/jsmermd/2018/0/2018_1A1-I01/_article/-char/ja/},
  urldate = {2020-01-17},
  abstract = {J-STAGE},
  eventtitle = {ロボティクス・メカトロニクス講演会講演概要集 2018},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\HY37FHAQ\\ja.html},
  langid = {japanese}
}

@article{zhengCNNsbasedAcousticScene2018,
  title = {{{CNNs}}-Based {{Acoustic Scene Classification}} Using {{Multi}}-{{Spectrogram Fusion}} and {{Label Expansions}}},
  author = {Zheng, Weiping and Mo, Zhenyao and Xing, Xiaotao and Zhao, Gansen},
  date = {2018-09-05},
  url = {http://arxiv.org/abs/1809.01543},
  urldate = {2019-05-07},
  abstract = {Spectrograms have been widely used in Convolutional Neural Networks based schemes for acoustic scene classification, such as the STFT spectrogram and the MFCC spectrogram, etc. They have different time-frequency characteristics, contributing to their own advantages and disadvantages in recognizing acoustic scenes. In this letter, a novel multi-spectrogram fusion framework is proposed, making the spectrograms complement each other. In the framework, a single CNN architecture is applied onto multiple spectrograms for feature extraction. The deep features extracted from multiple spectrograms are then fused to discriminate the acoustic scenes. Moreover, motivated by the inter-class similarities in acoustic scene datasets, a label expansion method is further proposed in which super-class labels are constructed upon the original classes. On the help of the expanded labels, the CNN models are transformed into the multitask learning form to improve the acoustic scene classification by appending the auxiliary task of super-class classification. To verify the effectiveness of the proposed methods, intensive experiments have been performed on the DCASE2017 and the LITIS Rouen datasets. Experimental results show that the proposed method can achieve promising accuracies on both datasets. Specifically, accuracies of 0.9744, 0.8865 and 0.7778 are obtained for the LITIS Rouen dataset, the DCASE Development set and Evaluation set respectively.},
  archivePrefix = {arXiv},
  eprint = {1809.01543},
  eprinttype = {arxiv},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KXPPIHQA\\Zheng et al. - 2018 - CNNs-based Acoustic Scene Classification using Mul.pdf;C\:\\Users\\sauli\\Zotero\\storage\\S3QGPUBY\\1809.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{zhou_binaural_2011,
  title = {Binaural {{Moving Sound Source Localization}} by {{Joint Estimation}} of {{ITD}} and {{ILD}}},
  booktitle = {Audio {{Engineering Society Convention}} 130},
  author = {Zhou, Cheng and Hu, Ruimin and Tu, Weiping and Wang, Xiaochen and Gao, Li},
  date = {2011-05-13},
  publisher = {{Audio Engineering Society}},
  url = {http://www.aes.org/e-lib/browse.cfm?elib=15797},
  urldate = {2017-01-10},
  abstract = {Spatial cues ITD and ILD which provide sound localization information play a very important role in binaural localization system. Efficient improvement of binaural moving sound source localization method by joint estimation of ITD and ILD based on Doppler effect is investigated. By removing Doppler effect influence, results show that the proposed binaural moving sound source localization method achieves 0.3\%(velocity = 1m/s), 5.7\%(velocity = 5m/s) and 10.5\%(velocity = 10m/s) accuracy...},
  eventtitle = {Audio {{Engineering Society Convention}} 130},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EUP72DCD\\Binaural Moving Sound Source Localization by Joint Estimation of ITD and ILD.pdf;C\:\\Users\\sauli\\Zotero\\storage\\ZWQQJ763\\browse.html},
  langid = {english}
}

@inproceedings{zhouLearningLocalGlobal2004,
  title = {Learning with Local and Global Consistency},
  booktitle = {Advances in {{Neural Information Processing Systems}} 16},
  author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Schölkopf, Bernhard},
  date = {2004},
  pages = {321--328},
  publisher = {{MIT Press}},
  abstract = {We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data. 1},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\FF7AILD3\\Zhou et al. - 2004 - Learning with local and global consistency.pdf;C\:\\Users\\sauli\\Zotero\\storage\\H7XM2KGP\\summary.html}
}

@article{zhouSemiSupervisedRegressionCoTraining,
  title = {Semi-{{Supervised Regression}} with {{Co}}-{{Training}}},
  author = {Zhou, Zhi-Hua and Li, Ming},
  pages = {6},
  abstract = {In many practical machine learning and data mining applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. Previous research mainly focuses on semi-supervised classification. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two k-nearest neighbor regressors with different distance metrics, each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\KIUM24H7\\Zhou and Li - Semi-Supervised Regression with Co-Training.pdf},
  langid = {english}
}

@article{zhuGaussianFilterTDOA2018,
  title = {Gaussian Filter for {{TDOA}} Based Sound Source Localization in Multimedia Surveillance},
  author = {Zhu, Mengyao and Yao, Huan and Wu, Xiukun and Lu, Zhihua and Zhu, Xiaoqiang and Huang, Qinghua},
  date = {2018-02},
  journaltitle = {MULTIMEDIA TOOLS AND APPLICATIONS},
  volume = {77},
  pages = {\{3369-3385\}},
  publisher = {SPRINGER},
  location = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  issn = {1380-7501},
  doi = {\{10.1007/s11042-017-5129-4\}},
  abstract = {Although multimedia surveillance systems are becoming increasingly ubiquitous in our living environment, automated multimedia surveillance systems based on video camera lacks the robustness and reliability most of the time in several real applications. To overcome this drawback, audio sensory devices have been taken into account in a considerable amount of research. For example, Sound Source Localization (SSL) may indicate potential security risks and could point the camera in that direction. In this paper, a reliable sound source localization based on Time-Difference-Of-Arrival (TDOA) is explored. The novel aspect of our approach includes a TDOA based Gaussian filter to improve the accuracy and stability of sound source localization. The advantage of our proposed algorithm is its extensive integration with various TDOA-based methods in all kinds of microphone array. The Experimental comparison shows significant improvement over the state of the art TDOA-based algorithm.},
  affiliation = {Zhu, MY (Reprint Author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China. Zhu, Mengyao; Yao, Huan; Wu, Xiukun; Zhu, Xiaoqiang; Huang, Qinghua, Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China. Lu, Zhihua, Ningbo Univ, Coll Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.},
  author-email = {zhumengyao@shu.edu.cn luzhihua@nbu.edu.cn},
  cited-references = {Benesty J, 2008, SPRINGER TOP SIGN PR, V1, P1. Bian XH, 2005, LECT NOTES COMPUT SC, V3468, P19. Brandstein M., 2013, MICROPHONE ARRAYS SI. Brandstein MS, 1997, IEEE T SPEECH AUDI P, V5, P45, DOI 10.1109/89.554268. BUCKLEY KM, 1988, IEEE T ACOUST SPEECH, V36, P953, DOI 10.1109/29.1617. CARTER GC, 1973, P IEEE, V61, P1497, DOI 10.1109/PROC.1973.9300. CARTER GC, 1977, J ACOUST SOC AM, V62, P922, DOI 10.1121/1.381623. Champagne B, 1996, IEEE T SPEECH AUDI P, V4, P148, DOI 10.1109/89.486067. Chang XJ, 2017, IEEE T IMAGE PROCESS, V26, P3911, DOI 10.1109/TIP.2017.2708506. Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901. Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546. Chang XJ, 2016, IEEE T NEUR NET LEAR, V27, P1502, DOI 10.1109/TNNLS.2015.2441735. Crocco M, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2871183. DESILVA GC, 2008, ADV MULT MOD INT, V4903, P466. EPHRAIM Y, 1985, IEEE T ACOUST SPEECH, V33, P443, DOI 10.1109/TASSP.1985.1164550. Garofolo J., 1993, TIMIT ACOUSTIC PHONE. Guo Y, 2010, P 12 ACM INT C ADJ P, P411. HAHN WR, 1973, IEEE T INFORM THEORY, V19, P608, DOI 10.1109/TIT.1973.1055077. Haykin S., 2002, PRENTICE HALL, V2, P478. IANNIELLO JP, 1982, IEEE T ACOUST SPEECH, V30, P998, DOI 10.1109/TASSP.1982.1163992. Johnson D. H., 1992, ARRAY SIGNAL PROCESS. KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830. KOTUS J, 2013, MULT COMM SERV SEC, V368, P107. Kotus J, 2014, MULTIMED TOOLS APPL, V68, P5, DOI 10.1007/s11042-012-1183-0. Lee NT, 2010, INT CONF ACOUST SPEE, P4466, DOI 10.1109/ICASSP.2010.5495611. Ma ZG, 2017, IEEE T MULTIMEDIA, V19, P1558, DOI 10.1109/TMM.2017.2659221. Pham QC, 2010, INT C IM PROC THEOR, P47. Schmidt R., 1981, THESIS. SCHMIDT RO, 1972, IEEE T AERO ELEC SYS, VAES8, P821, DOI 10.1109/TAES.1972.309614. SMITH JO, 1987, IEEE T ACOUST SPEECH, V35, P1661, DOI 10.1109/TASSP.1987.1165089. Stachurski J, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P93, DOI 10.1109/AVSS.2013.6636622. Svaizer P, 1997, INT CONF ACOUST SPEE, P231, DOI 10.1109/ICASSP.1997.599611. Teck Wee Chua, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P44, DOI 10.1007/978-3-319-04117-9_5. WANG H, 1985, IEEE T ACOUST SPEECH, V33, P823, DOI 10.1109/TASSP.1985.1164667. WAX M, 1983, IEEE T ACOUST SPEECH, V31, P1210, DOI 10.1109/TASSP.1983.1164183. Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938. Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023. Zhu L, 2016, P 25 INT JOINT C ART, P3959. Zhu L., 2016, IEEE T CYBERNETICS, P1. Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624. Zieger C, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P314, DOI 10.1109/AVSS.2009.49.},
  da = {2018-10-18},
  doc-delivery-number = {FW2KT},
  eissn = {1573-7721},
  funding-acknowledgement = {key support Projects of Shanghai Science and Technology Committee [16010500100]; National Natural Science Foundation of China [61402277, 61571279]; Innovation Program of Shanghai Municipal Education Commission [15ZZ044]},
  funding-text = {This work was supported by the key support Projects of Shanghai Science and Technology Committee (16010500100), the National Natural Science Foundation of China (61402277, 61571279), and Innovation Program of Shanghai Municipal Education Commission (15ZZ044).},
  journal-iso = {Multimed. Tools Appl.},
  keywords = {(Microphone array,Gaussian filter,sound source location,Surveillance),TDOA},
  keywords-plus = {TIME-DELAY ESTIMATION; ACOUSTIC SOURCE; SOURCE LOCATION; ARRAYS; INFORMATION; ESTIMATOR},
  langid = {english},
  number = {3},
  number-of-cited-references = {41},
  research-areas = {Computer Science; Engineering},
  times-cited = {1},
  type = {Article},
  unique-id = {ISI:000425132600026},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic}
}

@article{zhukovLabelPropagationGraphs,
  title = {Label Propagation on Graphs},
  author = {Zhukov, Leonid E},
  pages = {24},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\H96X6LI7\\Zhukov - Label propagation on graphs.pdf},
  keywords = {*****,MUST READ},
  langid = {english}
}

@article{zhukovLabelPropagationGraphsa,
  title = {Label Propagation on Graphs},
  author = {Zhukov, Leonid E},
  pages = {26},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\IMPDDW3K\\Zhukov - Label propagation on graphs.pdf},
  langid = {english}
}

@article{zhuSemiSupervisedLearningUsing,
  title = {Semi-{{Supervised Learning Using Gaussian Fields}} and {{Harmonic Functions}}},
  author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
  pages = {8},
  abstract = {An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.},
  file = {C\:\\Users\\sauli\\Zotero\\storage\\EQDADRNV\\Zhu et al. - Semi-Supervised Learning Using Gaussian Fields and.pdf},
  langid = {english}
}

@inproceedings{zhuSoundSourceLocalization2018,
  title = {Sound {{Source Localization Based}} on {{Robust Least Squares}} in {{Reverberant Environments}}},
  booktitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Zhu, H. and Dang, X. and Li, Z. and Ge, Q.},
  date = {2018-07},
  pages = {2029--2035},
  doi = {10.23919/ICIF.2018.8455305},
  abstract = {In this paper, we address the problem of sound source localization in reverberant environments. Time-delay estimation (TDE) methods are widely employed to locate sound sources based on the time differences of arrival (TDOAs) of signals received at different microphone pairs. In strong reverberations, the highest peak of the localization function is not necessarily from the true source resulting from the multi-path effect. Our previously proposed method based on the optimal peak association (OPA) aims to extract multiple peaks from the localization function for each microphone pair and find out the optimal association of TDOAs corresponding to the same sound source. However, due to the limitation of geometric configuration of microphones and possible missed detections, some microphone pairs fail to provide high-quality TDOA measurements. An improved OPA method is developed in this work based on the robust least squares which can determine the weights adaptively in terms of their respective observation accuracy. Experimental results demonstrate the superiority of the proposed method compared with the original OPA method in reverberant environments.},
  keywords = {acoustic signal processing,delay estimation,direction-of-arrival estimation,Frequency locked loops,geometric configuration,high-quality TDOA measurements,improved OPA method,least squares approximations,localization,localization function,microphone arrays,microphone pair,multipath effect,optimal peak association,reverberant,reverberant environments,reverberation,robust least squares,sound source localization,time difference of arrival,time difference of arrival estimation,time-delay estimation methods,time-of-arrival estimation}
}

@article{zhuStudyResponseDifference2018,
  title = {Study of {{Response Difference Amplification}} and {{Bionic Coupled Circuit}} in {{Small Acoustic Array}} for {{Spatial Localization}}},
  author = {Zhu, Xinlei and Yang, Ming and Zhang, Yaqiong and Ta, Na and Rao, Zhushi},
  date = {2018-08},
  journaltitle = {JOURNAL OF VIBRATION AND ACOUSTICS-TRANSACTIONS OF THE ASME},
  volume = {140},
  publisher = {ASME},
  location = {TWO PARK AVE, NEW YORK, NY 10016-5990 USA},
  issn = {1048-9002},
  doi = {\{10.1115/1.4039401\}},
  abstract = {Precisions of localization are a function of the size of an array. A kind of parasitoid fly, Ormia ochracea, performs an extraordinary directional hearing ability despite its tiny-scaled auditory organ. In this paper, vibration modes and transfer functions of the Ormia ochracea's ear model were calculated, and the phase difference amplification in responses are analyzed to investigate the directional hearing mechanism. A novel three-element bionic model is proposed for spatial sound source localization for small distance-wavelength ratios. The amplification of the phase difference of this model is verified. In order to realize the bionic localization model, based on electric-mechanic analogy method, a system that consists of a triangular acoustic array and a bionic coupling circuit is designed and tested. Frequency responses of the circuit output, as a means of transfer function of the system, are taken into estimation of the source directions. The result has shown that this circuit design has better performance in estimating the direction of sound sources compared to the uncoupled array with same size.},
  affiliation = {Rao, ZS (Reprint Author), Shanghai Jiao Tong Univ, State Key Lab Mech Syst & Vibrat, Sch Mech Engn, Inst Vibrat Shock & Noise, Shanghai 200240, Peoples R China. Zhu, Xinlei; Yang, Ming; Zhang, Yaqiong; Ta, Na; Rao, Zhushi, Shanghai Jiao Tong Univ, State Key Lab Mech Syst & Vibrat, Sch Mech Engn, Inst Vibrat Shock & Noise, Shanghai 200240, Peoples R China.},
  article-number = {041013},
  author-email = {xl.zhu@sjtu.edu.cn Young1@sjtu.edu.cn yaoyaosjtu@sjtu.edu.cn wutana@sjtu.edu.cn zsrao@sjtu.edu.cn},
  cited-references = {Aljanaideh K. F., 2017, ASME J VIB ACOUST, V140. Begault D. R., 2004, ANTHOLOGY ARTICLES S. Benesty J, 2008, SPRINGER TOP SIGN PR, V1, P1. Blauert J., 1997, SPATIAL HEARING PSYC. Carlile S., 1996, VIRTUAL AUDITORY SPA, P27. Chen JC, 2002, IEEE SIGNAL PROC MAG, V19, P30, DOI 10.1109/79.985676. Ho KC, 2008, IEEE T SIGNAL PROCES, V56, P464, DOI 10.1109/TSP.2007.906728. Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899. Kuntzman ML, 2013, APPL PHYS LETT, V102, DOI 10.1063/1.4776687. Lee J. H., 2015, ASME J VIB ACOUST, V137. Lisiewski AP, 2011, J ACOUST SOC AM, V129, pE166, DOI 10.1121/1.3565473. Liu HJ, 2008, APPL PHYS LETT, V93, DOI 10.1063/1.3043724. Liu HJ, 2008, PROC SPIE, V6932, DOI 10.1117/12.776519. Mason AC, 2001, NATURE, V410, P686, DOI 10.1038/35070564. Miles RN, 1995, J ACOUST SOC AM, V98, P3059, DOI 10.1121/1.413830. Miles RN, 2006, AUDIOL NEURO-OTOL, V11, P86, DOI 10.1159/000090681. Ono N, 2003, BOSTON TRANSDUCERS'03: DIGEST OF TECHNICAL PAPERS, VOLS 1 AND 2, P935. Popper AN, 2005, SOUND SOURCE LOCALIZ. Qingsheng Wang, 2009, WSEAS Transactions on Systems, V8, P855. ROBERT D, 1994, CELL TISSUE RES, V275, P63, DOI 10.1007/BF00305376. Robert D, 1996, J COMP PHYSIOL A, V179, P29. Robert D, 2000, CELL TISSUE RES, V301, P447, DOI 10.1007/s004410000257. Robert D, 1998, J COMP PHYSIOL A, V183, P443, DOI 10.1007/s003590050270. Robert D., 1994, J ACOUST SOC AM, V96, P3296. Robert D., 1992, J ACOUST SOC AM, V92, P2422. Touse M, 2010, APPL PHYS LETT, V96, DOI 10.1063/1.3418640. Vedurmudi AP, 2016, BIOL CYBERN, V110, P359, DOI 10.1007/s00422-016-0696-4. Wang W., 2010, MACHINE AUDITION PRI. Yang M, 2016, J ACOUST SOC AM, V140, P3854, DOI 10.1121/1.4965967. Zhang Y., 2017, ASME J VIB ACOUST, V140. Zhu X., 2013, 20 INT C SOUND VIBR.},
  da = {2018-10-18},
  doc-delivery-number = {GJ6ZE},
  eissn = {1528-8927},
  funding-acknowledgement = {Research Project of State Key Laboratory of Mechanical System and Vibration [MSVZD201103]},
  funding-text = {This research was supported by Research Project of State Key Laboratory of Mechanical System and Vibration MSVZD201103.},
  journal-iso = {J. Vib. Acoust.-Trans. ASME},
  keywords-plus = {FLY ORMIA-OCHRACEA; DIRECTIONAL HEARING; EARS},
  langid = {english},
  number = {4},
  number-of-cited-references = {31},
  research-areas = {Acoustics; Engineering; Mechanics},
  times-cited = {0},
  type = {Article},
  unique-id = {ISI:000435532800013},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Acoustics; Engineering, Mechanical; Mechanics}
}

@online{zotero-1607,
  type = {online}
}

@thesis{zotero-168,
  type = {thesis}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

